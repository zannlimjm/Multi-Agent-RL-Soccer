[2022-11-05 04:09:02.307633] Process 0. Episode 50, average_reward -0.080000
Episode 50: Total Loss of tensor([[2.9012]], grad_fn=<SubBackward0>)
[2022-11-05 04:09:36.540894] Process 3. Episode 50, average_reward -0.100000
Episode 50: Total Loss of tensor([[-9.1894]], grad_fn=<SubBackward0>)
[2022-11-05 04:09:39.443415] Process 1. Episode 50, average_reward -0.080000
Episode 50: Total Loss of tensor([[-105.2982]], grad_fn=<SubBackward0>)
[2022-11-05 04:09:42.745306] Process 4. Episode 50, average_reward -0.100000
Episode 50: Total Loss of tensor([[14.1629]], grad_fn=<SubBackward0>)
[2022-11-05 04:09:45.338556] Process 2. Episode 50, average_reward -0.080000
Episode 50: Total Loss of tensor([[-2.0798]], grad_fn=<SubBackward0>)
[2022-11-05 04:09:48.715054] Process 5. Episode 50, average_reward -0.100000
Episode 50: Total Loss of tensor([[4.5581]], grad_fn=<SubBackward0>)
[2022-11-05 04:12:20.534861] Process 0. Episode 100, average_reward -0.090000
Episode 100: Total Loss of tensor([[3.5746]], grad_fn=<SubBackward0>)
[2022-11-05 04:12:42.343156] Process 3. Episode 100, average_reward -0.130000
Episode 100: Total Loss of tensor([[-12.9354]], grad_fn=<SubBackward0>)
[2022-11-05 04:12:47.132682] Process 4. Episode 100, average_reward -0.070000
Episode 100: Total Loss of tensor([[-4.1213]], grad_fn=<SubBackward0>)
[2022-11-05 04:12:48.598404] Process 1. Episode 100, average_reward -0.090000
Episode 100: Total Loss of tensor([[-2.2938]], grad_fn=<SubBackward0>)
[2022-11-05 04:12:52.002872] Process 2. Episode 100, average_reward -0.060000
Episode 100: Total Loss of tensor([[-69.9177]], grad_fn=<SubBackward0>)
[2022-11-05 04:12:56.389693] Process 5. Episode 100, average_reward -0.070000
Episode 100: Total Loss of tensor([[7.0446]], grad_fn=<SubBackward0>)
[2022-11-05 04:15:13.387732] Process 0. Episode 150, average_reward -0.086667
Episode 150: Total Loss of tensor([[-0.4890]], grad_fn=<SubBackward0>)
[2022-11-05 04:15:29.211291] Process 4. Episode 150, average_reward -0.053333
Episode 150: Total Loss of tensor([[3.8104]], grad_fn=<SubBackward0>)
[2022-11-05 04:15:35.024496] Process 2. Episode 150, average_reward -0.080000
Episode 150: Total Loss of tensor([[12.4041]], grad_fn=<SubBackward0>)
[2022-11-05 04:15:36.595798] Process 3. Episode 150, average_reward -0.120000
Episode 150: Total Loss of tensor([[17.1288]], grad_fn=<SubBackward0>)
[2022-11-05 04:15:42.548369] Process 1. Episode 150, average_reward -0.080000
Episode 150: Total Loss of tensor([[6.1284]], grad_fn=<SubBackward0>)
[2022-11-05 04:15:57.434305] Process 5. Episode 150, average_reward -0.080000
Episode 150: Total Loss of tensor([[12.8084]], grad_fn=<SubBackward0>)
[2022-11-05 04:18:14.807746] Process 4. Episode 200, average_reward -0.060000
Episode 200: Total Loss of tensor([[11.1416]], grad_fn=<SubBackward0>)
[2022-11-05 04:18:16.243097] Process 0. Episode 200, average_reward -0.090000
Episode 200: Total Loss of tensor([[1.5150]], grad_fn=<SubBackward0>)
[2022-11-05 04:18:21.185654] Process 2. Episode 200, average_reward -0.070000
Episode 200: Total Loss of tensor([[-120.3147]], grad_fn=<SubBackward0>)
[2022-11-05 04:18:24.872532] Process 3. Episode 200, average_reward -0.100000
Episode 200: Total Loss of tensor([[15.5872]], grad_fn=<SubBackward0>)
[2022-11-05 04:18:34.764254] Process 1. Episode 200, average_reward -0.080000
Episode 200: Total Loss of tensor([[-126.3297]], grad_fn=<SubBackward0>)
[2022-11-05 04:18:54.654096] Process 5. Episode 200, average_reward -0.075000
Episode 200: Total Loss of tensor([[-4.1469]], grad_fn=<SubBackward0>)
[2022-11-05 04:20:57.601294] Process 4. Episode 250, average_reward -0.068000
Episode 250: Total Loss of tensor([[5.6755]], grad_fn=<SubBackward0>)
[2022-11-05 04:21:01.710721] Process 2. Episode 250, average_reward -0.064000
Episode 250: Total Loss of tensor([[1.3713]], grad_fn=<SubBackward0>)
[2022-11-05 04:21:05.008446] Process 0. Episode 250, average_reward -0.084000
Episode 250: Total Loss of tensor([[1.0444]], grad_fn=<SubBackward0>)
[2022-11-05 04:21:09.403914] Process 3. Episode 250, average_reward -0.088000
Episode 250: Total Loss of tensor([[6.8510]], grad_fn=<SubBackward0>)
[2022-11-05 04:21:15.716065] Process 1. Episode 250, average_reward -0.072000
Episode 250: Total Loss of tensor([[6.8306]], grad_fn=<SubBackward0>)
[2022-11-05 04:21:47.381797] Process 5. Episode 250, average_reward -0.068000
Episode 250: Total Loss of tensor([[-19.1662]], grad_fn=<SubBackward0>)
[2022-11-05 04:23:36.224130] Process 4. Episode 300, average_reward -0.073333
Episode 300: Total Loss of tensor([[5.0866]], grad_fn=<SubBackward0>)
[2022-11-05 04:23:50.083340] Process 0. Episode 300, average_reward -0.070000
Episode 300: Total Loss of tensor([[9.8894]], grad_fn=<SubBackward0>)
[2022-11-05 04:23:54.056366] Process 2. Episode 300, average_reward -0.070000
Episode 300: Total Loss of tensor([[1.8249]], grad_fn=<SubBackward0>)
[2022-11-05 04:23:58.082873] Process 3. Episode 300, average_reward -0.083333
Episode 300: Total Loss of tensor([[-3.4462]], grad_fn=<SubBackward0>)
[2022-11-05 04:24:00.761969] Process 1. Episode 300, average_reward -0.070000
Episode 300: Total Loss of tensor([[6.1963]], grad_fn=<SubBackward0>)
[2022-11-05 04:24:35.580609] Process 5. Episode 300, average_reward -0.080000
Episode 300: Total Loss of tensor([[37.4058]], grad_fn=<SubBackward0>)
[2022-11-05 04:26:16.602649] Process 4. Episode 350, average_reward -0.082857
Episode 350: Total Loss of tensor([[16.9740]], grad_fn=<SubBackward0>)
[2022-11-05 04:26:35.711079] Process 0. Episode 350, average_reward -0.074286
Episode 350: Total Loss of tensor([[-8.8511]], grad_fn=<SubBackward0>)
[2022-11-05 04:26:44.230499] Process 3. Episode 350, average_reward -0.074286
Episode 350: Total Loss of tensor([[10.0900]], grad_fn=<SubBackward0>)
[2022-11-05 04:26:48.076353] Process 1. Episode 350, average_reward -0.074286
Episode 350: Total Loss of tensor([[-1.8544]], grad_fn=<SubBackward0>)
[2022-11-05 04:26:51.707138] Process 2. Episode 350, average_reward -0.071429
Episode 350: Total Loss of tensor([[13.0597]], grad_fn=<SubBackward0>)
[2022-11-05 04:27:18.244825] Process 5. Episode 350, average_reward -0.080000
Episode 350: Total Loss of tensor([[28.1355]], grad_fn=<SubBackward0>)
[2022-11-05 04:28:54.433668] Process 4. Episode 400, average_reward -0.085000
Episode 400: Total Loss of tensor([[11.3837]], grad_fn=<SubBackward0>)
[2022-11-05 04:29:18.712386] Process 0. Episode 400, average_reward -0.070000
Episode 400: Total Loss of tensor([[-4.8867]], grad_fn=<SubBackward0>)
[2022-11-05 04:29:25.332172] Process 3. Episode 400, average_reward -0.080000
Episode 400: Total Loss of tensor([[2.8824]], grad_fn=<SubBackward0>)
[2022-11-05 04:29:31.131754] Process 1. Episode 400, average_reward -0.080000
Episode 400: Total Loss of tensor([[11.9146]], grad_fn=<SubBackward0>)
[2022-11-05 04:29:38.587643] Process 2. Episode 400, average_reward -0.080000
Episode 400: Total Loss of tensor([[2.9705]], grad_fn=<SubBackward0>)
[2022-11-05 04:30:12.754522] Process 5. Episode 400, average_reward -0.075000
Episode 400: Total Loss of tensor([[15.5223]], grad_fn=<SubBackward0>)
[2022-11-05 04:31:33.709356] Process 4. Episode 450, average_reward -0.086667
Episode 450: Total Loss of tensor([[-0.9731]], grad_fn=<SubBackward0>)
[2022-11-05 04:32:12.667107] Process 1. Episode 450, average_reward -0.075556
Episode 450: Total Loss of tensor([[-116.3055]], grad_fn=<SubBackward0>)
[2022-11-05 04:32:13.552817] Process 3. Episode 450, average_reward -0.077778
Episode 450: Total Loss of tensor([[-1.2415]], grad_fn=<SubBackward0>)
[2022-11-05 04:32:17.997691] Process 0. Episode 450, average_reward -0.068889
Episode 450: Total Loss of tensor([[-82.4274]], grad_fn=<SubBackward0>)
[2022-11-05 04:32:20.371212] Process 2. Episode 450, average_reward -0.082222
Episode 450: Total Loss of tensor([[3.1219]], grad_fn=<SubBackward0>)
[2022-11-05 04:33:08.564158] Process 5. Episode 450, average_reward -0.073333
Episode 450: Total Loss of tensor([[7.6369]], grad_fn=<SubBackward0>)
[2022-11-05 04:34:09.645131] Process 4. Episode 500, average_reward -0.080000
Episode 500: Total Loss of tensor([[-1.4461]], grad_fn=<SubBackward0>)
[2022-11-05 04:34:59.599273] Process 0. Episode 500, average_reward -0.074000
Episode 500: Total Loss of tensor([[12.5061]], grad_fn=<SubBackward0>)
[2022-11-05 04:35:02.799111] Process 3. Episode 500, average_reward -0.076000
Episode 500: Total Loss of tensor([[-5.7461]], grad_fn=<SubBackward0>)
[2022-11-05 04:35:04.056380] Process 1. Episode 500, average_reward -0.074000
Episode 500: Total Loss of tensor([[-6.4080]], grad_fn=<SubBackward0>)
[2022-11-05 04:35:06.728428] Process 2. Episode 500, average_reward -0.082000
Episode 500: Total Loss of tensor([[24.8049]], grad_fn=<SubBackward0>)
[2022-11-05 04:35:55.089521] Process 5. Episode 500, average_reward -0.076000
Episode 500: Total Loss of tensor([[13.6316]], grad_fn=<SubBackward0>)
[2022-11-05 04:36:46.962594] Process 4. Episode 550, average_reward -0.080000
Episode 550: Total Loss of tensor([[12.2843]], grad_fn=<SubBackward0>)
[2022-11-05 04:37:42.281793] Process 0. Episode 550, average_reward -0.074545
Episode 550: Total Loss of tensor([[3.3878]], grad_fn=<SubBackward0>)
[2022-11-05 04:37:50.518696] Process 1. Episode 550, average_reward -0.074545
Episode 550: Total Loss of tensor([[-6.4048]], grad_fn=<SubBackward0>)
[2022-11-05 04:38:00.072719] Process 2. Episode 550, average_reward -0.081818
Episode 550: Total Loss of tensor([[16.7580]], grad_fn=<SubBackward0>)
[2022-11-05 04:38:00.128198] Process 3. Episode 550, average_reward -0.076364
Episode 550: Total Loss of tensor([[20.6122]], grad_fn=<SubBackward0>)
[2022-11-05 04:38:35.411723] Process 5. Episode 550, average_reward -0.076364
Episode 550: Total Loss of tensor([[3.7891]], grad_fn=<SubBackward0>)
[2022-11-05 04:39:21.336700] Process 4. Episode 600, average_reward -0.075000
Episode 600: Total Loss of tensor([[0.1818]], grad_fn=<SubBackward0>)
[2022-11-05 04:40:26.926010] Process 0. Episode 600, average_reward -0.073333
Episode 600: Total Loss of tensor([[9.6790]], grad_fn=<SubBackward0>)
[2022-11-05 04:40:27.462046] Process 1. Episode 600, average_reward -0.073333
Episode 600: Total Loss of tensor([[10.8388]], grad_fn=<SubBackward0>)
[2022-11-05 04:40:37.210206] Process 2. Episode 600, average_reward -0.081667
Episode 600: Total Loss of tensor([[8.9762]], grad_fn=<SubBackward0>)
[2022-11-05 04:40:38.026363] Process 3. Episode 600, average_reward -0.073333
Episode 600: Total Loss of tensor([[6.9144]], grad_fn=<SubBackward0>)
[2022-11-05 04:41:10.591115] Process 5. Episode 600, average_reward -0.073333
Episode 600: Total Loss of tensor([[8.4500]], grad_fn=<SubBackward0>)
[2022-11-05 04:41:55.065459] Process 4. Episode 650, average_reward -0.078462
Episode 650: Total Loss of tensor([[23.4181]], grad_fn=<SubBackward0>)
[2022-11-05 04:43:00.971329] Process 0. Episode 650, average_reward -0.069231
Episode 650: Total Loss of tensor([[-7.7590]], grad_fn=<SubBackward0>)
[2022-11-05 04:43:14.264851] Process 2. Episode 650, average_reward -0.081538
Episode 650: Total Loss of tensor([[-0.2542]], grad_fn=<SubBackward0>)
[2022-11-05 04:43:18.551100] Process 1. Episode 650, average_reward -0.072308
Episode 650: Total Loss of tensor([[4.8950]], grad_fn=<SubBackward0>)
[2022-11-05 04:43:29.931554] Process 3. Episode 650, average_reward -0.070769
Episode 650: Total Loss of tensor([[4.0513]], grad_fn=<SubBackward0>)
[2022-11-05 04:43:56.963142] Process 5. Episode 650, average_reward -0.070769
Episode 650: Total Loss of tensor([[8.5085]], grad_fn=<SubBackward0>)
[2022-11-05 04:44:28.865342] Process 4. Episode 700, average_reward -0.081429
Episode 700: Total Loss of tensor([[4.6702]], grad_fn=<SubBackward0>)
[2022-11-05 04:45:39.989909] Process 0. Episode 700, average_reward -0.067143
Episode 700: Total Loss of tensor([[-138.8691]], grad_fn=<SubBackward0>)
[2022-11-05 04:45:54.438942] Process 2. Episode 700, average_reward -0.081429
Episode 700: Total Loss of tensor([[-4.6717]], grad_fn=<SubBackward0>)
[2022-11-05 04:46:05.499267] Process 1. Episode 700, average_reward -0.070000
Episode 700: Total Loss of tensor([[11.9685]], grad_fn=<SubBackward0>)
[2022-11-05 04:46:13.346795] Process 3. Episode 700, average_reward -0.068571
Episode 700: Total Loss of tensor([[-4.1184]], grad_fn=<SubBackward0>)
[2022-11-05 04:46:46.288268] Process 5. Episode 700, average_reward -0.065714
Episode 700: Total Loss of tensor([[1.9396]], grad_fn=<SubBackward0>)
[2022-11-05 04:47:04.047461] Process 4. Episode 750, average_reward -0.078667
Episode 750: Total Loss of tensor([[8.9828]], grad_fn=<SubBackward0>)
[2022-11-05 04:48:20.564958] Process 0. Episode 750, average_reward -0.065333
Episode 750: Total Loss of tensor([[-58.9648]], grad_fn=<SubBackward0>)
[2022-11-05 04:48:47.601311] Process 1. Episode 750, average_reward -0.066667
Episode 750: Total Loss of tensor([[8.5853]], grad_fn=<SubBackward0>)
[2022-11-05 04:48:50.816533] Process 2. Episode 750, average_reward -0.084000
Episode 750: Total Loss of tensor([[6.8842]], grad_fn=<SubBackward0>)
[2022-11-05 04:48:54.882364] Process 3. Episode 750, average_reward -0.066667
Episode 750: Total Loss of tensor([[21.4806]], grad_fn=<SubBackward0>)
[2022-11-05 04:49:30.764130] Process 5. Episode 750, average_reward -0.069333
Episode 750: Total Loss of tensor([[-134.6707]], grad_fn=<SubBackward0>)
[2022-11-05 04:49:39.431351] Process 4. Episode 800, average_reward -0.076250
Episode 800: Total Loss of tensor([[1.2474]], grad_fn=<SubBackward0>)
[2022-11-05 04:51:04.201115] Process 0. Episode 800, average_reward -0.065000
Episode 800: Total Loss of tensor([[-5.3935]], grad_fn=<SubBackward0>)
[2022-11-05 04:51:28.002063] Process 1. Episode 800, average_reward -0.067500
Episode 800: Total Loss of tensor([[-5.1669]], grad_fn=<SubBackward0>)
[2022-11-05 04:51:35.731460] Process 2. Episode 800, average_reward -0.082500
Episode 800: Total Loss of tensor([[1.1691]], grad_fn=<SubBackward0>)
[2022-11-05 04:51:50.707788] Process 3. Episode 800, average_reward -0.063750
Episode 800: Total Loss of tensor([[6.7015]], grad_fn=<SubBackward0>)
[2022-11-05 04:52:11.509116] Process 5. Episode 800, average_reward -0.066250
Episode 800: Total Loss of tensor([[15.1452]], grad_fn=<SubBackward0>)
[2022-11-05 04:52:16.266476] Process 4. Episode 850, average_reward -0.072941
Episode 850: Total Loss of tensor([[3.1515]], grad_fn=<SubBackward0>)
[2022-11-05 04:53:42.802385] Process 0. Episode 850, average_reward -0.068235
Episode 850: Total Loss of tensor([[-14.8182]], grad_fn=<SubBackward0>)
[2022-11-05 04:54:06.859333] Process 1. Episode 850, average_reward -0.067059
Episode 850: Total Loss of tensor([[0.1130]], grad_fn=<SubBackward0>)
[2022-11-05 04:54:33.212953] Process 2. Episode 850, average_reward -0.078824
Episode 850: Total Loss of tensor([[-1.0798]], grad_fn=<SubBackward0>)
[2022-11-05 04:54:43.469720] Process 3. Episode 850, average_reward -0.063529
Episode 850: Total Loss of tensor([[6.4448]], grad_fn=<SubBackward0>)
[2022-11-05 04:54:48.563090] Process 4. Episode 900, average_reward -0.070000
Episode 900: Total Loss of tensor([[5.2375]], grad_fn=<SubBackward0>)
[2022-11-05 04:54:55.652765] Process 5. Episode 850, average_reward -0.065882
Episode 850: Total Loss of tensor([[4.1766]], grad_fn=<SubBackward0>)
[2022-11-05 04:56:23.564356] Process 0. Episode 900, average_reward -0.067778
Episode 900: Total Loss of tensor([[24.1481]], grad_fn=<SubBackward0>)
[2022-11-05 04:56:45.375666] Process 1. Episode 900, average_reward -0.066667
Episode 900: Total Loss of tensor([[13.0858]], grad_fn=<SubBackward0>)
[2022-11-05 04:57:22.359005] Process 4. Episode 950, average_reward -0.067368
Episode 950: Total Loss of tensor([[11.7054]], grad_fn=<SubBackward0>)
[2022-11-05 04:57:33.262131] Process 2. Episode 900, average_reward -0.080000
Episode 900: Total Loss of tensor([[16.0942]], grad_fn=<SubBackward0>)
[2022-11-05 04:57:33.278943] Process 5. Episode 900, average_reward -0.067778
Episode 900: Total Loss of tensor([[11.3313]], grad_fn=<SubBackward0>)
[2022-11-05 04:57:36.643258] Process 3. Episode 900, average_reward -0.065556
Episode 900: Total Loss of tensor([[7.2155]], grad_fn=<SubBackward0>)
[2022-11-05 04:59:12.147267] Process 0. Episode 950, average_reward -0.065263
Episode 950: Total Loss of tensor([[13.6865]], grad_fn=<SubBackward0>)
[2022-11-05 04:59:41.916550] Process 1. Episode 950, average_reward -0.065263
Episode 950: Total Loss of tensor([[-10.6118]], grad_fn=<SubBackward0>)
[2022-11-05 04:59:52.414642] Process 4. Episode 1000, average_reward -0.068000
Episode 1000: Total Loss of tensor([[-3.5765]], grad_fn=<SubBackward0>)
[2022-11-05 05:00:08.497289] Process 5. Episode 950, average_reward -0.068421
Episode 950: Total Loss of tensor([[7.1699]], grad_fn=<SubBackward0>)
[2022-11-05 05:00:18.347111] Process 3. Episode 950, average_reward -0.065263
Episode 950: Total Loss of tensor([[2.4884]], grad_fn=<SubBackward0>)
[2022-11-05 05:00:29.679624] Process 2. Episode 950, average_reward -0.081053
Episode 950: Total Loss of tensor([[-0.7994]], grad_fn=<SubBackward0>)
[2022-11-05 05:01:54.544882] Process 0. Episode 1000, average_reward -0.063000
Episode 1000: Total Loss of tensor([[6.0227]], grad_fn=<SubBackward0>)
[2022-11-05 05:02:17.677990] Process 1. Episode 1000, average_reward -0.064000
Episode 1000: Total Loss of tensor([[-1.6724]], grad_fn=<SubBackward0>)
[2022-11-05 05:02:24.946026] Process 4. Episode 1050, average_reward -0.068571
Episode 1050: Total Loss of tensor([[-1.9362]], grad_fn=<SubBackward0>)
[2022-11-05 05:02:48.993768] Process 5. Episode 1000, average_reward -0.067000
Episode 1000: Total Loss of tensor([[0.3321]], grad_fn=<SubBackward0>)
[2022-11-05 05:02:58.225554] Process 3. Episode 1000, average_reward -0.063000
Episode 1000: Total Loss of tensor([[2.0698]], grad_fn=<SubBackward0>)
[2022-11-05 05:03:25.878588] Process 2. Episode 1000, average_reward -0.077000
Episode 1000: Total Loss of tensor([[25.2082]], grad_fn=<SubBackward0>)
[2022-11-05 05:04:28.959642] Process 0. Episode 1050, average_reward -0.067619
Episode 1050: Total Loss of tensor([[13.9043]], grad_fn=<SubBackward0>)
[2022-11-05 05:04:45.344303] Process 1. Episode 1050, average_reward -0.067619
Episode 1050: Total Loss of tensor([[14.6967]], grad_fn=<SubBackward0>)
[2022-11-05 05:04:51.289434] Process 4. Episode 1100, average_reward -0.073636
Episode 1100: Total Loss of tensor([[12.9133]], grad_fn=<SubBackward0>)
[2022-11-05 05:05:25.008975] Process 5. Episode 1050, average_reward -0.065714
Episode 1050: Total Loss of tensor([[5.5856]], grad_fn=<SubBackward0>)
[2022-11-05 05:05:35.037266] Process 3. Episode 1050, average_reward -0.062857
Episode 1050: Total Loss of tensor([[5.9974]], grad_fn=<SubBackward0>)
[2022-11-05 05:06:06.757212] Process 2. Episode 1050, average_reward -0.080952
Episode 1050: Total Loss of tensor([[16.7462]], grad_fn=<SubBackward0>)
[2022-11-05 05:07:02.722521] Process 0. Episode 1100, average_reward -0.067273
Episode 1100: Total Loss of tensor([[-0.5017]], grad_fn=<SubBackward0>)
[2022-11-05 05:07:12.001294] Process 1. Episode 1100, average_reward -0.066364
Episode 1100: Total Loss of tensor([[10.6614]], grad_fn=<SubBackward0>)
[2022-11-05 05:07:14.195568] Process 4. Episode 1150, average_reward -0.073043
Episode 1150: Total Loss of tensor([[20.9922]], grad_fn=<SubBackward0>)
[2022-11-05 05:07:48.443295] Process 5. Episode 1100, average_reward -0.066364
Episode 1100: Total Loss of tensor([[-55.8565]], grad_fn=<SubBackward0>)
[2022-11-05 05:08:09.348542] Process 3. Episode 1100, average_reward -0.065455
Episode 1100: Total Loss of tensor([[0.5211]], grad_fn=<SubBackward0>)
[2022-11-05 05:08:52.734416] Process 2. Episode 1100, average_reward -0.081818
Episode 1100: Total Loss of tensor([[-37.0304]], grad_fn=<SubBackward0>)
[2022-11-05 05:09:32.390388] Process 0. Episode 1150, average_reward -0.065217
Episode 1150: Total Loss of tensor([[2.5717]], grad_fn=<SubBackward0>)
[2022-11-05 05:09:32.612476] Process 4. Episode 1200, average_reward -0.071667
Episode 1200: Total Loss of tensor([[10.6735]], grad_fn=<SubBackward0>)
[2022-11-05 05:09:40.324368] Process 1. Episode 1150, average_reward -0.068696
Episode 1150: Total Loss of tensor([[-3.0497]], grad_fn=<SubBackward0>)
[2022-11-05 05:10:11.831321] Process 5. Episode 1150, average_reward -0.068696
Episode 1150: Total Loss of tensor([[10.5466]], grad_fn=<SubBackward0>)
[2022-11-05 05:10:38.204418] Process 3. Episode 1150, average_reward -0.065217
Episode 1150: Total Loss of tensor([[8.0119]], grad_fn=<SubBackward0>)
[2022-11-05 05:11:34.349442] Process 2. Episode 1150, average_reward -0.081739
Episode 1150: Total Loss of tensor([[-0.3258]], grad_fn=<SubBackward0>)
[2022-11-05 05:11:56.023256] Process 4. Episode 1250, average_reward -0.070400
Episode 1250: Total Loss of tensor([[11.0971]], grad_fn=<SubBackward0>)
[2022-11-05 05:12:03.522018] Process 0. Episode 1200, average_reward -0.066667
Episode 1200: Total Loss of tensor([[9.9401]], grad_fn=<SubBackward0>)
[2022-11-05 05:12:08.862540] Process 1. Episode 1200, average_reward -0.072500
Episode 1200: Total Loss of tensor([[-8.3624]], grad_fn=<SubBackward0>)
[2022-11-05 05:12:33.125030] Process 5. Episode 1200, average_reward -0.069167
Episode 1200: Total Loss of tensor([[8.3129]], grad_fn=<SubBackward0>)
[2022-11-05 05:13:20.703636] Process 3. Episode 1200, average_reward -0.065833
Episode 1200: Total Loss of tensor([[13.6635]], grad_fn=<SubBackward0>)
[2022-11-05 05:14:16.122145] Process 2. Episode 1200, average_reward -0.081667
Episode 1200: Total Loss of tensor([[3.0546]], grad_fn=<SubBackward0>)
[2022-11-05 05:14:17.339037] Process 4. Episode 1300, average_reward -0.072308
Episode 1300: Total Loss of tensor([[3.2343]], grad_fn=<SubBackward0>)
[2022-11-05 05:14:33.432448] Process 0. Episode 1250, average_reward -0.066400
Episode 1250: Total Loss of tensor([[3.3536]], grad_fn=<SubBackward0>)
[2022-11-05 05:14:46.914081] Process 1. Episode 1250, average_reward -0.074400
Episode 1250: Total Loss of tensor([[-8.1622]], grad_fn=<SubBackward0>)
[2022-11-05 05:14:53.617955] Process 5. Episode 1250, average_reward -0.068000
Episode 1250: Total Loss of tensor([[-3.8589]], grad_fn=<SubBackward0>)
[2022-11-05 05:15:55.948365] Process 3. Episode 1250, average_reward -0.068800
Episode 1250: Total Loss of tensor([[-0.1738]], grad_fn=<SubBackward0>)
[2022-11-05 05:16:38.204708] Process 4. Episode 1350, average_reward -0.073333
Episode 1350: Total Loss of tensor([[1.2190]], grad_fn=<SubBackward0>)
[2022-11-05 05:17:01.157946] Process 2. Episode 1250, average_reward -0.081600
Episode 1250: Total Loss of tensor([[7.4493]], grad_fn=<SubBackward0>)
[2022-11-05 05:17:05.532194] Process 0. Episode 1300, average_reward -0.066154
Episode 1300: Total Loss of tensor([[-0.6369]], grad_fn=<SubBackward0>)
[2022-11-05 05:17:15.925552] Process 5. Episode 1300, average_reward -0.070769
Episode 1300: Total Loss of tensor([[3.2791]], grad_fn=<SubBackward0>)
[2022-11-05 05:17:19.696659] Process 1. Episode 1300, average_reward -0.075385
Episode 1300: Total Loss of tensor([[20.8328]], grad_fn=<SubBackward0>)
[2022-11-05 05:18:28.789065] Process 3. Episode 1300, average_reward -0.068462
Episode 1300: Total Loss of tensor([[5.7083]], grad_fn=<SubBackward0>)
[2022-11-05 05:19:01.705922] Process 4. Episode 1400, average_reward -0.072857
Episode 1400: Total Loss of tensor([[8.7303]], grad_fn=<SubBackward0>)
[2022-11-05 05:19:34.186722] Process 5. Episode 1350, average_reward -0.071852
Episode 1350: Total Loss of tensor([[2.6409]], grad_fn=<SubBackward0>)
[2022-11-05 05:19:35.634163] Process 2. Episode 1300, average_reward -0.080000
Episode 1300: Total Loss of tensor([[12.1281]], grad_fn=<SubBackward0>)
[2022-11-05 05:19:42.701698] Process 0. Episode 1350, average_reward -0.068889
Episode 1350: Total Loss of tensor([[-81.5147]], grad_fn=<SubBackward0>)
[2022-11-05 05:20:01.030820] Process 1. Episode 1350, average_reward -0.074074
Episode 1350: Total Loss of tensor([[0.2040]], grad_fn=<SubBackward0>)
[2022-11-05 05:20:59.200362] Process 3. Episode 1350, average_reward -0.068889
Episode 1350: Total Loss of tensor([[4.0807]], grad_fn=<SubBackward0>)
[2022-11-05 05:21:27.136269] Process 4. Episode 1450, average_reward -0.073793
Episode 1450: Total Loss of tensor([[13.8147]], grad_fn=<SubBackward0>)
[2022-11-05 05:21:54.724038] Process 5. Episode 1400, average_reward -0.072143
Episode 1400: Total Loss of tensor([[8.4598]], grad_fn=<SubBackward0>)
[2022-11-05 05:22:04.854827] Process 2. Episode 1350, average_reward -0.080000
Episode 1350: Total Loss of tensor([[23.9980]], grad_fn=<SubBackward0>)
[2022-11-05 05:22:27.449663] Process 0. Episode 1400, average_reward -0.068571
Episode 1400: Total Loss of tensor([[-4.0515]], grad_fn=<SubBackward0>)
[2022-11-05 05:22:36.507171] Process 1. Episode 1400, average_reward -0.075714
Episode 1400: Total Loss of tensor([[0.6142]], grad_fn=<SubBackward0>)
[2022-11-05 05:23:33.409430] Process 3. Episode 1400, average_reward -0.068571
Episode 1400: Total Loss of tensor([[2.5373]], grad_fn=<SubBackward0>)
[2022-11-05 05:23:46.983859] Process 4. Episode 1500, average_reward -0.074000
Episode 1500: Total Loss of tensor([[7.3197]], grad_fn=<SubBackward0>)
[2022-11-05 05:24:22.851858] Process 5. Episode 1450, average_reward -0.071724
Episode 1450: Total Loss of tensor([[0.3586]], grad_fn=<SubBackward0>)
[2022-11-05 05:24:30.040119] Process 2. Episode 1400, average_reward -0.077857
Episode 1400: Total Loss of tensor([[-140.6222]], grad_fn=<SubBackward0>)
[2022-11-05 05:25:01.841298] Process 0. Episode 1450, average_reward -0.068966
Episode 1450: Total Loss of tensor([[-135.5982]], grad_fn=<SubBackward0>)
[2022-11-05 05:25:09.075728] Process 1. Episode 1450, average_reward -0.074483
Episode 1450: Total Loss of tensor([[20.5452]], grad_fn=<SubBackward0>)
[2022-11-05 05:26:07.630179] Process 3. Episode 1450, average_reward -0.068966
Episode 1450: Total Loss of tensor([[6.5353]], grad_fn=<SubBackward0>)
[2022-11-05 05:26:11.198003] Process 4. Episode 1550, average_reward -0.074194
Episode 1550: Total Loss of tensor([[19.6754]], grad_fn=<SubBackward0>)
[2022-11-05 05:26:51.659360] Process 5. Episode 1500, average_reward -0.070000
Episode 1500: Total Loss of tensor([[1.8878]], grad_fn=<SubBackward0>)
[2022-11-05 05:26:56.812226] Process 2. Episode 1450, average_reward -0.077931
Episode 1450: Total Loss of tensor([[4.9651]], grad_fn=<SubBackward0>)
[2022-11-05 05:27:30.785048] Process 0. Episode 1500, average_reward -0.069333
Episode 1500: Total Loss of tensor([[-2.2500]], grad_fn=<SubBackward0>)
[2022-11-05 05:27:40.280235] Process 1. Episode 1500, average_reward -0.073333
Episode 1500: Total Loss of tensor([[-25.1098]], grad_fn=<SubBackward0>)
[2022-11-05 05:28:37.253286] Process 3. Episode 1500, average_reward -0.068667
Episode 1500: Total Loss of tensor([[-0.9866]], grad_fn=<SubBackward0>)
[2022-11-05 05:28:37.764009] Process 4. Episode 1600, average_reward -0.073125
Episode 1600: Total Loss of tensor([[-3.2925]], grad_fn=<SubBackward0>)
[2022-11-05 05:29:19.830028] Process 2. Episode 1500, average_reward -0.077333
Episode 1500: Total Loss of tensor([[8.7009]], grad_fn=<SubBackward0>)
[2022-11-05 05:29:30.654730] Process 5. Episode 1550, average_reward -0.069677
Episode 1550: Total Loss of tensor([[6.3930]], grad_fn=<SubBackward0>)
[2022-11-05 05:30:02.933996] Process 0. Episode 1550, average_reward -0.068387
Episode 1550: Total Loss of tensor([[9.2090]], grad_fn=<SubBackward0>)
[2022-11-05 05:30:20.126447] Process 1. Episode 1550, average_reward -0.073548
Episode 1550: Total Loss of tensor([[5.3014]], grad_fn=<SubBackward0>)
[2022-11-05 05:31:02.063641] Process 3. Episode 1550, average_reward -0.069032
Episode 1550: Total Loss of tensor([[9.1660]], grad_fn=<SubBackward0>)
[2022-11-05 05:31:07.485099] Process 4. Episode 1650, average_reward -0.074545
Episode 1650: Total Loss of tensor([[-3.8676]], grad_fn=<SubBackward0>)
[2022-11-05 05:31:57.884781] Process 2. Episode 1550, average_reward -0.078710
Episode 1550: Total Loss of tensor([[-5.6790]], grad_fn=<SubBackward0>)
[2022-11-05 05:32:02.968047] Process 5. Episode 1600, average_reward -0.068125
Episode 1600: Total Loss of tensor([[10.8569]], grad_fn=<SubBackward0>)
[2022-11-05 05:32:31.678796] Process 0. Episode 1600, average_reward -0.068750
Episode 1600: Total Loss of tensor([[5.7625]], grad_fn=<SubBackward0>)
[2022-11-05 05:32:51.609946] Process 1. Episode 1600, average_reward -0.073750
Episode 1600: Total Loss of tensor([[11.4641]], grad_fn=<SubBackward0>)
[2022-11-05 05:33:32.982181] Process 4. Episode 1700, average_reward -0.074706
Episode 1700: Total Loss of tensor([[7.1905]], grad_fn=<SubBackward0>)
[2022-11-05 05:33:35.903030] Process 3. Episode 1600, average_reward -0.068125
Episode 1600: Total Loss of tensor([[0.2066]], grad_fn=<SubBackward0>)
[2022-11-05 05:34:28.844904] Process 2. Episode 1600, average_reward -0.079375
Episode 1600: Total Loss of tensor([[15.7672]], grad_fn=<SubBackward0>)
[2022-11-05 05:34:31.854083] Process 5. Episode 1650, average_reward -0.067273
Episode 1650: Total Loss of tensor([[21.6192]], grad_fn=<SubBackward0>)
[2022-11-05 05:35:05.925959] Process 0. Episode 1650, average_reward -0.068485
Episode 1650: Total Loss of tensor([[11.9141]], grad_fn=<SubBackward0>)
[2022-11-05 05:35:23.045285] Process 1. Episode 1650, average_reward -0.073939
Episode 1650: Total Loss of tensor([[-56.6840]], grad_fn=<SubBackward0>)
[2022-11-05 05:35:57.782370] Process 4. Episode 1750, average_reward -0.076571
Episode 1750: Total Loss of tensor([[13.0063]], grad_fn=<SubBackward0>)
[2022-11-05 05:36:09.729849] Process 3. Episode 1650, average_reward -0.067273
Episode 1650: Total Loss of tensor([[-4.7776]], grad_fn=<SubBackward0>)
[2022-11-05 05:36:56.507527] Process 5. Episode 1700, average_reward -0.068235
Episode 1700: Total Loss of tensor([[7.4215]], grad_fn=<SubBackward0>)
[2022-11-05 05:37:00.699796] Process 2. Episode 1650, average_reward -0.076970
Episode 1650: Total Loss of tensor([[1.4619]], grad_fn=<SubBackward0>)
[2022-11-05 05:37:33.734579] Process 0. Episode 1700, average_reward -0.068235
Episode 1700: Total Loss of tensor([[6.6171]], grad_fn=<SubBackward0>)
[2022-11-05 05:38:02.370694] Process 1. Episode 1700, average_reward -0.074118
Episode 1700: Total Loss of tensor([[7.7190]], grad_fn=<SubBackward0>)
[2022-11-05 05:38:18.070521] Process 4. Episode 1800, average_reward -0.077222
Episode 1800: Total Loss of tensor([[4.9261]], grad_fn=<SubBackward0>)
[2022-11-05 05:38:49.947842] Process 3. Episode 1700, average_reward -0.067647
Episode 1700: Total Loss of tensor([[4.6703]], grad_fn=<SubBackward0>)
[2022-11-05 05:39:28.807345] Process 5. Episode 1750, average_reward -0.069143
Episode 1750: Total Loss of tensor([[-135.4555]], grad_fn=<SubBackward0>)
[2022-11-05 05:39:41.744173] Process 2. Episode 1700, average_reward -0.075294
Episode 1700: Total Loss of tensor([[15.2468]], grad_fn=<SubBackward0>)
[2022-11-05 05:39:56.031858] Process 0. Episode 1750, average_reward -0.069714
Episode 1750: Total Loss of tensor([[4.0963]], grad_fn=<SubBackward0>)
[2022-11-05 05:40:36.854655] Process 4. Episode 1850, average_reward -0.076216
Episode 1850: Total Loss of tensor([[8.4277]], grad_fn=<SubBackward0>)
[2022-11-05 05:40:45.779190] Process 1. Episode 1750, average_reward -0.072571
Episode 1750: Total Loss of tensor([[5.8532]], grad_fn=<SubBackward0>)
[2022-11-05 05:41:22.899680] Process 3. Episode 1750, average_reward -0.066857
Episode 1750: Total Loss of tensor([[8.0104]], grad_fn=<SubBackward0>)
[2022-11-05 05:42:00.839369] Process 5. Episode 1800, average_reward -0.068333
Episode 1800: Total Loss of tensor([[12.2900]], grad_fn=<SubBackward0>)
[2022-11-05 05:42:18.734093] Process 2. Episode 1750, average_reward -0.074857
Episode 1750: Total Loss of tensor([[7.8563]], grad_fn=<SubBackward0>)
[2022-11-05 05:42:22.312952] Process 0. Episode 1800, average_reward -0.071111
Episode 1800: Total Loss of tensor([[3.3656]], grad_fn=<SubBackward0>)
[2022-11-05 05:42:59.674592] Process 4. Episode 1900, average_reward -0.077368
Episode 1900: Total Loss of tensor([[17.2529]], grad_fn=<SubBackward0>)
[2022-11-05 05:43:16.467235] Process 1. Episode 1800, average_reward -0.072222
Episode 1800: Total Loss of tensor([[14.2078]], grad_fn=<SubBackward0>)
[2022-11-05 05:43:51.538910] Process 3. Episode 1800, average_reward -0.068889
Episode 1800: Total Loss of tensor([[6.1204]], grad_fn=<SubBackward0>)
[2022-11-05 05:44:24.867881] Process 5. Episode 1850, average_reward -0.068108
Episode 1850: Total Loss of tensor([[-8.3872]], grad_fn=<SubBackward0>)
[2022-11-05 05:44:50.034763] Process 2. Episode 1800, average_reward -0.076111
Episode 1800: Total Loss of tensor([[-57.8695]], grad_fn=<SubBackward0>)
[2022-11-05 05:45:06.563978] Process 0. Episode 1850, average_reward -0.070811
Episode 1850: Total Loss of tensor([[-3.1550]], grad_fn=<SubBackward0>)
[2022-11-05 05:45:19.358131] Process 4. Episode 1950, average_reward -0.079487
Episode 1950: Total Loss of tensor([[20.2333]], grad_fn=<SubBackward0>)
[2022-11-05 05:45:55.355278] Process 1. Episode 1850, average_reward -0.071892
Episode 1850: Total Loss of tensor([[-122.9115]], grad_fn=<SubBackward0>)
[2022-11-05 05:46:27.790594] Process 3. Episode 1850, average_reward -0.069189
Episode 1850: Total Loss of tensor([[12.8442]], grad_fn=<SubBackward0>)
[2022-11-05 05:46:54.266767] Process 5. Episode 1900, average_reward -0.068947
Episode 1900: Total Loss of tensor([[11.9583]], grad_fn=<SubBackward0>)
[2022-11-05 05:47:18.489799] Process 2. Episode 1850, average_reward -0.077297
Episode 1850: Total Loss of tensor([[6.7798]], grad_fn=<SubBackward0>)
[2022-11-05 05:47:41.105173] Process 0. Episode 1900, average_reward -0.072105
Episode 1900: Total Loss of tensor([[18.2794]], grad_fn=<SubBackward0>)
[2022-11-05 05:47:43.408869] Process 4. Episode 2000, average_reward -0.079000
Episode 2000: Total Loss of tensor([[25.4632]], grad_fn=<SubBackward0>)
[2022-11-05 05:48:24.510513] Process 1. Episode 1900, average_reward -0.074211
Episode 1900: Total Loss of tensor([[13.4056]], grad_fn=<SubBackward0>)
[2022-11-05 05:48:54.248332] Process 3. Episode 1900, average_reward -0.068947
Episode 1900: Total Loss of tensor([[1.8923]], grad_fn=<SubBackward0>)
[2022-11-05 05:49:19.855724] Process 5. Episode 1950, average_reward -0.068205
Episode 1950: Total Loss of tensor([[6.9267]], grad_fn=<SubBackward0>)
[2022-11-05 05:49:49.744278] Process 2. Episode 1900, average_reward -0.078421
Episode 1900: Total Loss of tensor([[-1.1505]], grad_fn=<SubBackward0>)
[2022-11-05 05:50:08.954310] Process 4. Episode 2050, average_reward -0.078049
Episode 2050: Total Loss of tensor([[-2.8021]], grad_fn=<SubBackward0>)
[2022-11-05 05:50:20.698647] Process 0. Episode 1950, average_reward -0.071282
Episode 1950: Total Loss of tensor([[1.6515]], grad_fn=<SubBackward0>)
[2022-11-05 05:50:52.954635] Process 1. Episode 1950, average_reward -0.074872
Episode 1950: Total Loss of tensor([[8.2398]], grad_fn=<SubBackward0>)
[2022-11-05 05:51:25.844049] Process 3. Episode 1950, average_reward -0.070256
Episode 1950: Total Loss of tensor([[5.2750]], grad_fn=<SubBackward0>)
[2022-11-05 05:51:57.000546] Process 5. Episode 2000, average_reward -0.067500
Episode 2000: Total Loss of tensor([[14.1561]], grad_fn=<SubBackward0>)
[2022-11-05 05:52:11.679417] Process 2. Episode 1950, average_reward -0.079487
Episode 1950: Total Loss of tensor([[-1.6186]], grad_fn=<SubBackward0>)
[2022-11-05 05:52:29.344647] Process 4. Episode 2100, average_reward -0.077619
Episode 2100: Total Loss of tensor([[23.5960]], grad_fn=<SubBackward0>)
[2022-11-05 05:52:55.234598] Process 0. Episode 2000, average_reward -0.072000
Episode 2000: Total Loss of tensor([[9.1821]], grad_fn=<SubBackward0>)
[2022-11-05 05:53:19.703859] Process 1. Episode 2000, average_reward -0.074500
Episode 2000: Total Loss of tensor([[8.1506]], grad_fn=<SubBackward0>)
[2022-11-05 05:54:04.486814] Process 3. Episode 2000, average_reward -0.071500
Episode 2000: Total Loss of tensor([[-9.0188]], grad_fn=<SubBackward0>)
[2022-11-05 05:54:28.876212] Process 5. Episode 2050, average_reward -0.067805
Episode 2050: Total Loss of tensor([[23.4853]], grad_fn=<SubBackward0>)
[2022-11-05 05:54:36.834323] Process 2. Episode 2000, average_reward -0.081000
Episode 2000: Total Loss of tensor([[-64.6833]], grad_fn=<SubBackward0>)
[2022-11-05 05:54:53.803548] Process 4. Episode 2150, average_reward -0.078140
Episode 2150: Total Loss of tensor([[21.1313]], grad_fn=<SubBackward0>)
[2022-11-05 05:55:32.044511] Process 0. Episode 2050, average_reward -0.071707
Episode 2050: Total Loss of tensor([[-6.0786]], grad_fn=<SubBackward0>)
[2022-11-05 05:55:55.542381] Process 1. Episode 2050, average_reward -0.074146
Episode 2050: Total Loss of tensor([[4.1302]], grad_fn=<SubBackward0>)
[2022-11-05 05:56:28.414339] Process 3. Episode 2050, average_reward -0.071220
Episode 2050: Total Loss of tensor([[0.4628]], grad_fn=<SubBackward0>)
[2022-11-05 05:56:55.830505] Process 5. Episode 2100, average_reward -0.067619
Episode 2100: Total Loss of tensor([[2.9726]], grad_fn=<SubBackward0>)
[2022-11-05 05:57:11.711225] Process 2. Episode 2050, average_reward -0.080488
Episode 2050: Total Loss of tensor([[14.5766]], grad_fn=<SubBackward0>)
[2022-11-05 05:57:20.297326] Process 4. Episode 2200, average_reward -0.077273
Episode 2200: Total Loss of tensor([[-127.6529]], grad_fn=<SubBackward0>)
[2022-11-05 05:58:04.862377] Process 0. Episode 2100, average_reward -0.070952
Episode 2100: Total Loss of tensor([[-4.0368]], grad_fn=<SubBackward0>)
[2022-11-05 05:58:38.188134] Process 1. Episode 2100, average_reward -0.073333
Episode 2100: Total Loss of tensor([[2.4586]], grad_fn=<SubBackward0>)
[2022-11-05 05:58:55.972256] Process 3. Episode 2100, average_reward -0.071905
Episode 2100: Total Loss of tensor([[7.2873]], grad_fn=<SubBackward0>)
[2022-11-05 05:59:32.900590] Process 5. Episode 2150, average_reward -0.067442
Episode 2150: Total Loss of tensor([[6.4769]], grad_fn=<SubBackward0>)
[2022-11-05 05:59:33.716496] Process 2. Episode 2100, average_reward -0.080000
Episode 2100: Total Loss of tensor([[2.4523]], grad_fn=<SubBackward0>)
[2022-11-05 05:59:40.247992] Process 4. Episode 2250, average_reward -0.076889
Episode 2250: Total Loss of tensor([[-133.2198]], grad_fn=<SubBackward0>)
[2022-11-05 06:00:35.078730] Process 0. Episode 2150, average_reward -0.069767
Episode 2150: Total Loss of tensor([[14.1467]], grad_fn=<SubBackward0>)
[2022-11-05 06:01:09.943771] Process 1. Episode 2150, average_reward -0.073023
Episode 2150: Total Loss of tensor([[-6.1351]], grad_fn=<SubBackward0>)
[2022-11-05 06:01:28.093899] Process 3. Episode 2150, average_reward -0.072093
Episode 2150: Total Loss of tensor([[1.5992]], grad_fn=<SubBackward0>)
[2022-11-05 06:01:58.907318] Process 2. Episode 2150, average_reward -0.079535
Episode 2150: Total Loss of tensor([[10.5040]], grad_fn=<SubBackward0>)
[2022-11-05 06:02:02.960099] Process 4. Episode 2300, average_reward -0.077826
Episode 2300: Total Loss of tensor([[-10.9981]], grad_fn=<SubBackward0>)
[2022-11-05 06:02:05.864029] Process 5. Episode 2200, average_reward -0.068182
Episode 2200: Total Loss of tensor([[0.4679]], grad_fn=<SubBackward0>)
[2022-11-05 06:03:00.069937] Process 0. Episode 2200, average_reward -0.070455
Episode 2200: Total Loss of tensor([[0.1612]], grad_fn=<SubBackward0>)
[2022-11-05 06:03:49.369658] Process 1. Episode 2200, average_reward -0.071818
Episode 2200: Total Loss of tensor([[-0.6133]], grad_fn=<SubBackward0>)
[2022-11-05 06:04:02.871794] Process 3. Episode 2200, average_reward -0.070909
Episode 2200: Total Loss of tensor([[0.3681]], grad_fn=<SubBackward0>)
[2022-11-05 06:04:27.005972] Process 4. Episode 2350, average_reward -0.078723
Episode 2350: Total Loss of tensor([[-103.6654]], grad_fn=<SubBackward0>)
[2022-11-05 06:04:31.527270] Process 2. Episode 2200, average_reward -0.079545
Episode 2200: Total Loss of tensor([[6.5941]], grad_fn=<SubBackward0>)
[2022-11-05 06:04:35.239272] Process 5. Episode 2250, average_reward -0.068000
Episode 2250: Total Loss of tensor([[9.9162]], grad_fn=<SubBackward0>)
[2022-11-05 06:05:31.321040] Process 0. Episode 2250, average_reward -0.071556
Episode 2250: Total Loss of tensor([[7.7076]], grad_fn=<SubBackward0>)
[2022-11-05 06:06:22.512102] Process 1. Episode 2250, average_reward -0.071556
Episode 2250: Total Loss of tensor([[17.2329]], grad_fn=<SubBackward0>)
[2022-11-05 06:06:46.540863] Process 4. Episode 2400, average_reward -0.078333
Episode 2400: Total Loss of tensor([[0.2016]], grad_fn=<SubBackward0>)
[2022-11-05 06:06:47.207915] Process 3. Episode 2250, average_reward -0.069778
Episode 2250: Total Loss of tensor([[-1.7248]], grad_fn=<SubBackward0>)
[2022-11-05 06:06:59.150004] Process 5. Episode 2300, average_reward -0.067391
Episode 2300: Total Loss of tensor([[0.8602]], grad_fn=<SubBackward0>)
[2022-11-05 06:07:10.040324] Process 2. Episode 2250, average_reward -0.079556
Episode 2250: Total Loss of tensor([[-4.5948]], grad_fn=<SubBackward0>)
[2022-11-05 06:07:57.780809] Process 0. Episode 2300, average_reward -0.072609
Episode 2300: Total Loss of tensor([[8.4533]], grad_fn=<SubBackward0>)
[2022-11-05 06:08:51.331484] Process 1. Episode 2300, average_reward -0.071304
Episode 2300: Total Loss of tensor([[13.1203]], grad_fn=<SubBackward0>)
[2022-11-05 06:09:09.831040] Process 4. Episode 2450, average_reward -0.077959
Episode 2450: Total Loss of tensor([[2.7058]], grad_fn=<SubBackward0>)
[2022-11-05 06:09:21.702761] Process 5. Episode 2350, average_reward -0.068936
Episode 2350: Total Loss of tensor([[1.5148]], grad_fn=<SubBackward0>)
[2022-11-05 06:09:27.294453] Process 3. Episode 2300, average_reward -0.070870
Episode 2300: Total Loss of tensor([[16.9841]], grad_fn=<SubBackward0>)
[2022-11-05 06:09:41.613834] Process 2. Episode 2300, average_reward -0.079130
Episode 2300: Total Loss of tensor([[1.6808]], grad_fn=<SubBackward0>)
[2022-11-05 06:10:25.157301] Process 0. Episode 2350, average_reward -0.072340
Episode 2350: Total Loss of tensor([[-127.0250]], grad_fn=<SubBackward0>)
[2022-11-05 06:11:34.750093] Process 4. Episode 2500, average_reward -0.078000
Episode 2500: Total Loss of tensor([[5.6375]], grad_fn=<SubBackward0>)
[2022-11-05 06:11:36.151226] Process 1. Episode 2350, average_reward -0.070638
Episode 2350: Total Loss of tensor([[17.5791]], grad_fn=<SubBackward0>)
[2022-11-05 06:11:49.042389] Process 5. Episode 2400, average_reward -0.068750
Episode 2400: Total Loss of tensor([[1.4553]], grad_fn=<SubBackward0>)
[2022-11-05 06:11:52.059775] Process 3. Episode 2350, average_reward -0.071064
Episode 2350: Total Loss of tensor([[14.2117]], grad_fn=<SubBackward0>)
[2022-11-05 06:12:15.059220] Process 2. Episode 2350, average_reward -0.079149
Episode 2350: Total Loss of tensor([[7.5238]], grad_fn=<SubBackward0>)
[2022-11-05 06:12:47.395920] Process 0. Episode 2400, average_reward -0.072917
Episode 2400: Total Loss of tensor([[7.3207]], grad_fn=<SubBackward0>)
[2022-11-05 06:14:01.438071] Process 4. Episode 2550, average_reward -0.076863
Episode 2550: Total Loss of tensor([[10.2703]], grad_fn=<SubBackward0>)
[2022-11-05 06:14:15.241269] Process 1. Episode 2400, average_reward -0.071250
Episode 2400: Total Loss of tensor([[10.0444]], grad_fn=<SubBackward0>)
[2022-11-05 06:14:23.705888] Process 5. Episode 2450, average_reward -0.068571
Episode 2450: Total Loss of tensor([[14.4516]], grad_fn=<SubBackward0>)
[2022-11-05 06:14:26.814594] Process 3. Episode 2400, average_reward -0.072917
Episode 2400: Total Loss of tensor([[10.8647]], grad_fn=<SubBackward0>)
[2022-11-05 06:14:41.263883] Process 2. Episode 2400, average_reward -0.077917
Episode 2400: Total Loss of tensor([[2.3101]], grad_fn=<SubBackward0>)
[2022-11-05 06:15:11.995603] Process 0. Episode 2450, average_reward -0.073878
Episode 2450: Total Loss of tensor([[12.8585]], grad_fn=<SubBackward0>)
[2022-11-05 06:16:26.442147] Process 4. Episode 2600, average_reward -0.076538
Episode 2600: Total Loss of tensor([[-14.9974]], grad_fn=<SubBackward0>)
[2022-11-05 06:16:50.751229] Process 1. Episode 2450, average_reward -0.071429
Episode 2450: Total Loss of tensor([[37.8405]], grad_fn=<SubBackward0>)
[2022-11-05 06:16:52.042446] Process 5. Episode 2500, average_reward -0.069200
Episode 2500: Total Loss of tensor([[32.6322]], grad_fn=<SubBackward0>)
[2022-11-05 06:16:56.391174] Process 3. Episode 2450, average_reward -0.074286
Episode 2450: Total Loss of tensor([[-39.8350]], grad_fn=<SubBackward0>)
[2022-11-05 06:17:09.599881] Process 2. Episode 2450, average_reward -0.077959
Episode 2450: Total Loss of tensor([[5.5693]], grad_fn=<SubBackward0>)
[2022-11-05 06:17:41.800650] Process 0. Episode 2500, average_reward -0.074800
Episode 2500: Total Loss of tensor([[8.3833]], grad_fn=<SubBackward0>)
[2022-11-05 06:18:51.177847] Process 4. Episode 2650, average_reward -0.076226
Episode 2650: Total Loss of tensor([[21.5281]], grad_fn=<SubBackward0>)
[2022-11-05 06:19:16.714499] Process 5. Episode 2550, average_reward -0.069020
Episode 2550: Total Loss of tensor([[-4.4212]], grad_fn=<SubBackward0>)
[2022-11-05 06:19:21.131488] Process 3. Episode 2500, average_reward -0.074800
Episode 2500: Total Loss of tensor([[-3.4269]], grad_fn=<SubBackward0>)
[2022-11-05 06:19:29.482682] Process 1. Episode 2500, average_reward -0.070000
Episode 2500: Total Loss of tensor([[7.9868]], grad_fn=<SubBackward0>)
[2022-11-05 06:19:40.360414] Process 2. Episode 2500, average_reward -0.077600
Episode 2500: Total Loss of tensor([[1.4477]], grad_fn=<SubBackward0>)
[2022-11-05 06:20:09.235477] Process 0. Episode 2550, average_reward -0.074902
Episode 2550: Total Loss of tensor([[16.8859]], grad_fn=<SubBackward0>)
[2022-11-05 06:21:12.409123] Process 4. Episode 2700, average_reward -0.075556
Episode 2700: Total Loss of tensor([[-14.5519]], grad_fn=<SubBackward0>)
[2022-11-05 06:21:42.041451] Process 5. Episode 2600, average_reward -0.069231
Episode 2600: Total Loss of tensor([[9.1647]], grad_fn=<SubBackward0>)
[2022-11-05 06:21:59.037839] Process 3. Episode 2550, average_reward -0.075686
Episode 2550: Total Loss of tensor([[-3.9771]], grad_fn=<SubBackward0>)
[2022-11-05 06:22:00.367477] Process 1. Episode 2550, average_reward -0.070588
Episode 2550: Total Loss of tensor([[-6.1619]], grad_fn=<SubBackward0>)
[2022-11-05 06:22:17.585454] Process 2. Episode 2550, average_reward -0.077255
Episode 2550: Total Loss of tensor([[13.9970]], grad_fn=<SubBackward0>)
[2022-11-05 06:22:37.275021] Process 0. Episode 2600, average_reward -0.075385
Episode 2600: Total Loss of tensor([[4.8814]], grad_fn=<SubBackward0>)
[2022-11-05 06:23:32.632421] Process 4. Episode 2750, average_reward -0.075273
Episode 2750: Total Loss of tensor([[-53.5518]], grad_fn=<SubBackward0>)
[2022-11-05 06:24:07.341459] Process 5. Episode 2650, average_reward -0.068302
Episode 2650: Total Loss of tensor([[13.8138]], grad_fn=<SubBackward0>)
[2022-11-05 06:24:30.401034] Process 3. Episode 2600, average_reward -0.075385
Episode 2600: Total Loss of tensor([[4.6651]], grad_fn=<SubBackward0>)
[2022-11-05 06:24:35.086467] Process 1. Episode 2600, average_reward -0.070769
Episode 2600: Total Loss of tensor([[9.7561]], grad_fn=<SubBackward0>)
[2022-11-05 06:24:46.670160] Process 2. Episode 2600, average_reward -0.076923
Episode 2600: Total Loss of tensor([[11.2307]], grad_fn=<SubBackward0>)
[2022-11-05 06:25:03.668870] Process 0. Episode 2650, average_reward -0.076604
Episode 2650: Total Loss of tensor([[12.5240]], grad_fn=<SubBackward0>)
[2022-11-05 06:25:55.669964] Process 4. Episode 2800, average_reward -0.076071
Episode 2800: Total Loss of tensor([[-12.7870]], grad_fn=<SubBackward0>)
[2022-11-05 06:26:33.662360] Process 5. Episode 2700, average_reward -0.068889
Episode 2700: Total Loss of tensor([[-3.6438]], grad_fn=<SubBackward0>)
[2022-11-05 06:27:01.134401] Process 1. Episode 2650, average_reward -0.069811
Episode 2650: Total Loss of tensor([[21.1957]], grad_fn=<SubBackward0>)
[2022-11-05 06:27:01.287110] Process 3. Episode 2650, average_reward -0.074717
Episode 2650: Total Loss of tensor([[9.6225]], grad_fn=<SubBackward0>)
[2022-11-05 06:27:17.010182] Process 2. Episode 2650, average_reward -0.076604
Episode 2650: Total Loss of tensor([[7.1213]], grad_fn=<SubBackward0>)
[2022-11-05 06:27:33.100447] Process 0. Episode 2700, average_reward -0.076667
Episode 2700: Total Loss of tensor([[-22.4941]], grad_fn=<SubBackward0>)
[2022-11-05 06:28:18.767645] Process 4. Episode 2850, average_reward -0.076491
Episode 2850: Total Loss of tensor([[21.5954]], grad_fn=<SubBackward0>)
[2022-11-05 06:28:59.803668] Process 5. Episode 2750, average_reward -0.068364
Episode 2750: Total Loss of tensor([[14.6946]], grad_fn=<SubBackward0>)
[2022-11-05 06:29:28.878247] Process 1. Episode 2700, average_reward -0.070000
Episode 2700: Total Loss of tensor([[0.4630]], grad_fn=<SubBackward0>)
[2022-11-05 06:29:31.060278] Process 3. Episode 2700, average_reward -0.074815
Episode 2700: Total Loss of tensor([[5.0608]], grad_fn=<SubBackward0>)
[2022-11-05 06:29:56.254380] Process 2. Episode 2700, average_reward -0.075926
Episode 2700: Total Loss of tensor([[3.9406]], grad_fn=<SubBackward0>)
[2022-11-05 06:30:08.862923] Process 0. Episode 2750, average_reward -0.077455
Episode 2750: Total Loss of tensor([[-78.1292]], grad_fn=<SubBackward0>)
[2022-11-05 06:30:44.316916] Process 4. Episode 2900, average_reward -0.078621
Episode 2900: Total Loss of tensor([[18.3352]], grad_fn=<SubBackward0>)
[2022-11-05 06:31:28.654095] Process 5. Episode 2800, average_reward -0.068571
Episode 2800: Total Loss of tensor([[-2.9110]], grad_fn=<SubBackward0>)
[2022-11-05 06:31:55.736542] Process 1. Episode 2750, average_reward -0.070182
Episode 2750: Total Loss of tensor([[12.1503]], grad_fn=<SubBackward0>)
[2022-11-05 06:32:08.593128] Process 3. Episode 2750, average_reward -0.074545
Episode 2750: Total Loss of tensor([[-0.2206]], grad_fn=<SubBackward0>)
[2022-11-05 06:32:32.975532] Process 2. Episode 2750, average_reward -0.076000
Episode 2750: Total Loss of tensor([[-3.1555]], grad_fn=<SubBackward0>)
[2022-11-05 06:32:40.332835] Process 0. Episode 2800, average_reward -0.076786
Episode 2800: Total Loss of tensor([[-6.7349]], grad_fn=<SubBackward0>)
[2022-11-05 06:33:10.748247] Process 4. Episode 2950, average_reward -0.078983
Episode 2950: Total Loss of tensor([[3.5415]], grad_fn=<SubBackward0>)
[2022-11-05 06:33:54.643939] Process 5. Episode 2850, average_reward -0.068772
Episode 2850: Total Loss of tensor([[3.1645]], grad_fn=<SubBackward0>)
[2022-11-05 06:34:27.237259] Process 1. Episode 2800, average_reward -0.070357
Episode 2800: Total Loss of tensor([[12.2293]], grad_fn=<SubBackward0>)
[2022-11-05 06:34:47.178117] Process 3. Episode 2800, average_reward -0.073571
Episode 2800: Total Loss of tensor([[-4.8632]], grad_fn=<SubBackward0>)
[2022-11-05 06:35:07.900208] Process 0. Episode 2850, average_reward -0.077193
Episode 2850: Total Loss of tensor([[-1.2800]], grad_fn=<SubBackward0>)
[2022-11-05 06:35:14.074827] Process 2. Episode 2800, average_reward -0.075714
Episode 2800: Total Loss of tensor([[20.4927]], grad_fn=<SubBackward0>)
[2022-11-05 06:35:29.891682] Process 4. Episode 3000, average_reward -0.078667
Episode 3000: Total Loss of tensor([[0.5142]], grad_fn=<SubBackward0>)
[2022-11-05 06:36:23.491209] Process 5. Episode 2900, average_reward -0.070000
Episode 2900: Total Loss of tensor([[-2.7668]], grad_fn=<SubBackward0>)
[2022-11-05 06:37:01.984661] Process 1. Episode 2850, average_reward -0.069825
Episode 2850: Total Loss of tensor([[-44.2453]], grad_fn=<SubBackward0>)
[2022-11-05 06:37:17.152946] Process 3. Episode 2850, average_reward -0.072632
Episode 2850: Total Loss of tensor([[-2.6372]], grad_fn=<SubBackward0>)
[2022-11-05 06:37:40.502008] Process 2. Episode 2850, average_reward -0.075789
Episode 2850: Total Loss of tensor([[-3.4554]], grad_fn=<SubBackward0>)
[2022-11-05 06:37:44.531565] Process 0. Episode 2900, average_reward -0.076552
Episode 2900: Total Loss of tensor([[-5.6925]], grad_fn=<SubBackward0>)
[2022-11-05 06:37:53.314550] Process 4. Episode 3050, average_reward -0.077377
Episode 3050: Total Loss of tensor([[-3.5468]], grad_fn=<SubBackward0>)
[2022-11-05 06:38:55.980680] Process 5. Episode 2950, average_reward -0.069831
Episode 2950: Total Loss of tensor([[7.0921]], grad_fn=<SubBackward0>)
[2022-11-05 06:39:32.768871] Process 1. Episode 2900, average_reward -0.068966
Episode 2900: Total Loss of tensor([[9.3539]], grad_fn=<SubBackward0>)
[2022-11-05 06:39:41.235658] Process 3. Episode 2900, average_reward -0.072414
Episode 2900: Total Loss of tensor([[4.9140]], grad_fn=<SubBackward0>)
[2022-11-05 06:40:08.664111] Process 2. Episode 2900, average_reward -0.075862
Episode 2900: Total Loss of tensor([[12.4954]], grad_fn=<SubBackward0>)
[2022-11-05 06:40:19.724728] Process 0. Episode 2950, average_reward -0.077288
Episode 2950: Total Loss of tensor([[9.4593]], grad_fn=<SubBackward0>)
[2022-11-05 06:40:29.261061] Process 4. Episode 3100, average_reward -0.076774
Episode 3100: Total Loss of tensor([[7.2291]], grad_fn=<SubBackward0>)
[2022-11-05 06:41:20.925164] Process 5. Episode 3000, average_reward -0.070333
Episode 3000: Total Loss of tensor([[1.7219]], grad_fn=<SubBackward0>)
[2022-11-05 06:42:04.892855] Process 3. Episode 2950, average_reward -0.072881
Episode 2950: Total Loss of tensor([[17.2059]], grad_fn=<SubBackward0>)
[2022-11-05 06:42:19.187327] Process 1. Episode 2950, average_reward -0.069492
Episode 2950: Total Loss of tensor([[-5.6859]], grad_fn=<SubBackward0>)
[2022-11-05 06:42:42.082583] Process 2. Episode 2950, average_reward -0.075932
Episode 2950: Total Loss of tensor([[14.7082]], grad_fn=<SubBackward0>)
[2022-11-05 06:42:43.948924] Process 0. Episode 3000, average_reward -0.076667
Episode 3000: Total Loss of tensor([[17.9597]], grad_fn=<SubBackward0>)
[2022-11-05 06:43:01.477231] Process 4. Episode 3150, average_reward -0.076508
Episode 3150: Total Loss of tensor([[26.2210]], grad_fn=<SubBackward0>)
[2022-11-05 06:43:44.632324] Process 5. Episode 3050, average_reward -0.070492
Episode 3050: Total Loss of tensor([[4.8451]], grad_fn=<SubBackward0>)
[2022-11-05 06:44:29.047437] Process 3. Episode 3000, average_reward -0.072667
Episode 3000: Total Loss of tensor([[14.9692]], grad_fn=<SubBackward0>)
[2022-11-05 06:45:02.683292] Process 1. Episode 3000, average_reward -0.069333
Episode 3000: Total Loss of tensor([[4.2395]], grad_fn=<SubBackward0>)
[2022-11-05 06:45:10.552908] Process 0. Episode 3050, average_reward -0.076721
Episode 3050: Total Loss of tensor([[-6.5715]], grad_fn=<SubBackward0>)
[2022-11-05 06:45:13.755191] Process 2. Episode 3000, average_reward -0.075667
Episode 3000: Total Loss of tensor([[2.9314]], grad_fn=<SubBackward0>)
[2022-11-05 06:45:23.312234] Process 4. Episode 3200, average_reward -0.075625
Episode 3200: Total Loss of tensor([[12.5188]], grad_fn=<SubBackward0>)
[2022-11-05 06:46:06.712090] Process 5. Episode 3100, average_reward -0.070968
Episode 3100: Total Loss of tensor([[3.7465]], grad_fn=<SubBackward0>)
[2022-11-05 06:47:08.260735] Process 3. Episode 3050, average_reward -0.073115
Episode 3050: Total Loss of tensor([[-0.2208]], grad_fn=<SubBackward0>)
[2022-11-05 06:47:38.011142] Process 1. Episode 3050, average_reward -0.069508
Episode 3050: Total Loss of tensor([[23.1922]], grad_fn=<SubBackward0>)
[2022-11-05 06:47:39.904156] Process 2. Episode 3050, average_reward -0.075738
Episode 3050: Total Loss of tensor([[11.8421]], grad_fn=<SubBackward0>)
[2022-11-05 06:47:43.843445] Process 4. Episode 3250, average_reward -0.075077
Episode 3250: Total Loss of tensor([[8.1002]], grad_fn=<SubBackward0>)
[2022-11-05 06:47:44.659391] Process 0. Episode 3100, average_reward -0.077097
Episode 3100: Total Loss of tensor([[24.8046]], grad_fn=<SubBackward0>)
[2022-11-05 06:48:30.991428] Process 5. Episode 3150, average_reward -0.071429
Episode 3150: Total Loss of tensor([[-6.0586]], grad_fn=<SubBackward0>)
[2022-11-05 06:49:37.560998] Process 3. Episode 3100, average_reward -0.073226
Episode 3100: Total Loss of tensor([[4.4740]], grad_fn=<SubBackward0>)
[2022-11-05 06:50:07.765143] Process 2. Episode 3100, average_reward -0.076129
Episode 3100: Total Loss of tensor([[-84.0378]], grad_fn=<SubBackward0>)
[2022-11-05 06:50:07.895866] Process 4. Episode 3300, average_reward -0.073939
Episode 3300: Total Loss of tensor([[-5.8817]], grad_fn=<SubBackward0>)
[2022-11-05 06:50:08.711856] Process 1. Episode 3100, average_reward -0.070323
Episode 3100: Total Loss of tensor([[-4.7407]], grad_fn=<SubBackward0>)
[2022-11-05 06:50:28.889374] Process 0. Episode 3150, average_reward -0.076508
Episode 3150: Total Loss of tensor([[-4.1709]], grad_fn=<SubBackward0>)
[2022-11-05 06:50:53.645792] Process 5. Episode 3200, average_reward -0.070312
Episode 3200: Total Loss of tensor([[-4.6722]], grad_fn=<SubBackward0>)
[2022-11-05 06:52:06.072785] Process 3. Episode 3150, average_reward -0.073016
Episode 3150: Total Loss of tensor([[-4.2041]], grad_fn=<SubBackward0>)
[2022-11-05 06:52:28.569336] Process 4. Episode 3350, average_reward -0.073731
Episode 3350: Total Loss of tensor([[30.0458]], grad_fn=<SubBackward0>)
[2022-11-05 06:52:45.465161] Process 2. Episode 3150, average_reward -0.075556
Episode 3150: Total Loss of tensor([[-4.1893]], grad_fn=<SubBackward0>)
[2022-11-05 06:52:52.967198] Process 1. Episode 3150, average_reward -0.070159
Episode 3150: Total Loss of tensor([[16.2978]], grad_fn=<SubBackward0>)
[2022-11-05 06:52:55.783466] Process 0. Episode 3200, average_reward -0.077188
Episode 3200: Total Loss of tensor([[-130.9884]], grad_fn=<SubBackward0>)
[2022-11-05 06:53:16.022314] Process 5. Episode 3250, average_reward -0.070769
Episode 3250: Total Loss of tensor([[7.9924]], grad_fn=<SubBackward0>)
[2022-11-05 06:54:37.689180] Process 3. Episode 3200, average_reward -0.073125
Episode 3200: Total Loss of tensor([[3.3990]], grad_fn=<SubBackward0>)
[2022-11-05 06:54:52.848194] Process 4. Episode 3400, average_reward -0.074412
Episode 3400: Total Loss of tensor([[4.9113]], grad_fn=<SubBackward0>)
[2022-11-05 06:55:07.557461] Process 2. Episode 3200, average_reward -0.075000
Episode 3200: Total Loss of tensor([[-1.6718]], grad_fn=<SubBackward0>)
[2022-11-05 06:55:28.432072] Process 1. Episode 3200, average_reward -0.070000
Episode 3200: Total Loss of tensor([[-3.3657]], grad_fn=<SubBackward0>)
[2022-11-05 06:55:29.976466] Process 0. Episode 3250, average_reward -0.077538
Episode 3250: Total Loss of tensor([[0.7876]], grad_fn=<SubBackward0>)
[2022-11-05 06:55:50.885343] Process 5. Episode 3300, average_reward -0.069697
Episode 3300: Total Loss of tensor([[0.2333]], grad_fn=<SubBackward0>)
[2022-11-05 06:57:06.617482] Process 3. Episode 3250, average_reward -0.072615
Episode 3250: Total Loss of tensor([[-4.2560]], grad_fn=<SubBackward0>)
[2022-11-05 06:57:17.073475] Process 4. Episode 3450, average_reward -0.074783
Episode 3450: Total Loss of tensor([[6.8071]], grad_fn=<SubBackward0>)
[2022-11-05 06:57:33.981508] Process 2. Episode 3250, average_reward -0.075692
Episode 3250: Total Loss of tensor([[3.3619]], grad_fn=<SubBackward0>)
[2022-11-05 06:58:02.658055] Process 1. Episode 3250, average_reward -0.069538
Episode 3250: Total Loss of tensor([[8.3319]], grad_fn=<SubBackward0>)
[2022-11-05 06:58:06.578451] Process 0. Episode 3300, average_reward -0.076364
Episode 3300: Total Loss of tensor([[9.5139]], grad_fn=<SubBackward0>)
[2022-11-05 06:58:16.611690] Process 5. Episode 3350, average_reward -0.069254
Episode 3350: Total Loss of tensor([[1.0527]], grad_fn=<SubBackward0>)
[2022-11-05 06:59:30.084049] Process 3. Episode 3300, average_reward -0.072424
Episode 3300: Total Loss of tensor([[9.5324]], grad_fn=<SubBackward0>)
[2022-11-05 06:59:45.256735] Process 4. Episode 3500, average_reward -0.074571
Episode 3500: Total Loss of tensor([[8.6286]], grad_fn=<SubBackward0>)
[2022-11-05 07:00:10.214315] Process 2. Episode 3300, average_reward -0.076970
Episode 3300: Total Loss of tensor([[-109.6233]], grad_fn=<SubBackward0>)
[2022-11-05 07:00:32.875682] Process 1. Episode 3300, average_reward -0.069697
Episode 3300: Total Loss of tensor([[-11.4646]], grad_fn=<SubBackward0>)
[2022-11-05 07:00:40.279437] Process 0. Episode 3350, average_reward -0.076119
Episode 3350: Total Loss of tensor([[20.0643]], grad_fn=<SubBackward0>)
[2022-11-05 07:00:48.377035] Process 5. Episode 3400, average_reward -0.070000
Episode 3400: Total Loss of tensor([[-12.7257]], grad_fn=<SubBackward0>)
[2022-11-05 07:02:03.018776] Process 3. Episode 3350, average_reward -0.072239
Episode 3350: Total Loss of tensor([[11.6945]], grad_fn=<SubBackward0>)
[2022-11-05 07:02:11.614778] Process 4. Episode 3550, average_reward -0.074085
Episode 3550: Total Loss of tensor([[-0.2271]], grad_fn=<SubBackward0>)
[2022-11-05 07:02:38.428089] Process 2. Episode 3350, average_reward -0.076418
Episode 3350: Total Loss of tensor([[12.5687]], grad_fn=<SubBackward0>)
[2022-11-05 07:03:04.568245] Process 1. Episode 3350, average_reward -0.069552
Episode 3350: Total Loss of tensor([[8.3821]], grad_fn=<SubBackward0>)
[2022-11-05 07:03:06.458153] Process 0. Episode 3400, average_reward -0.076471
Episode 3400: Total Loss of tensor([[-0.9172]], grad_fn=<SubBackward0>)
[2022-11-05 07:03:21.450566] Process 5. Episode 3450, average_reward -0.070145
Episode 3450: Total Loss of tensor([[9.8423]], grad_fn=<SubBackward0>)
[2022-11-05 07:04:29.772617] Process 3. Episode 3400, average_reward -0.071471
Episode 3400: Total Loss of tensor([[-0.0858]], grad_fn=<SubBackward0>)
[2022-11-05 07:04:35.851563] Process 4. Episode 3600, average_reward -0.074167
Episode 3600: Total Loss of tensor([[11.6349]], grad_fn=<SubBackward0>)
[2022-11-05 07:05:04.059786] Process 2. Episode 3400, average_reward -0.076765
Episode 3400: Total Loss of tensor([[-18.0178]], grad_fn=<SubBackward0>)
[2022-11-05 07:05:30.763645] Process 0. Episode 3450, average_reward -0.076812
Episode 3450: Total Loss of tensor([[-12.4794]], grad_fn=<SubBackward0>)
[2022-11-05 07:05:38.327771] Process 1. Episode 3400, average_reward -0.069706
Episode 3400: Total Loss of tensor([[14.5894]], grad_fn=<SubBackward0>)
[2022-11-05 07:05:50.980098] Process 5. Episode 3500, average_reward -0.070286
Episode 3500: Total Loss of tensor([[-124.7693]], grad_fn=<SubBackward0>)
[2022-11-05 07:07:02.445272] Process 4. Episode 3650, average_reward -0.074795
Episode 3650: Total Loss of tensor([[-9.8761]], grad_fn=<SubBackward0>)
[2022-11-05 07:07:07.472588] Process 3. Episode 3450, average_reward -0.071014
Episode 3450: Total Loss of tensor([[11.9135]], grad_fn=<SubBackward0>)
[2022-11-05 07:07:27.821509] Process 2. Episode 3450, average_reward -0.077681
Episode 3450: Total Loss of tensor([[12.7118]], grad_fn=<SubBackward0>)
[2022-11-05 07:07:57.951960] Process 0. Episode 3500, average_reward -0.076000
Episode 3500: Total Loss of tensor([[16.9616]], grad_fn=<SubBackward0>)
[2022-11-05 07:08:14.823096] Process 1. Episode 3450, average_reward -0.069855
Episode 3450: Total Loss of tensor([[4.1387]], grad_fn=<SubBackward0>)
[2022-11-05 07:08:20.461483] Process 5. Episode 3550, average_reward -0.071268
Episode 3550: Total Loss of tensor([[-88.0437]], grad_fn=<SubBackward0>)
[2022-11-05 07:09:27.855760] Process 4. Episode 3700, average_reward -0.074324
Episode 3700: Total Loss of tensor([[14.2120]], grad_fn=<SubBackward0>)
[2022-11-05 07:09:41.318378] Process 3. Episode 3500, average_reward -0.070857
Episode 3500: Total Loss of tensor([[-1.8253]], grad_fn=<SubBackward0>)
[2022-11-05 07:09:49.676628] Process 2. Episode 3500, average_reward -0.078000
Episode 3500: Total Loss of tensor([[13.0366]], grad_fn=<SubBackward0>)
[2022-11-05 07:10:24.398993] Process 0. Episode 3550, average_reward -0.075775
Episode 3550: Total Loss of tensor([[5.7781]], grad_fn=<SubBackward0>)
[2022-11-05 07:10:55.937293] Process 5. Episode 3600, average_reward -0.071111
Episode 3600: Total Loss of tensor([[1.7822]], grad_fn=<SubBackward0>)
[2022-11-05 07:10:59.621520] Process 1. Episode 3500, average_reward -0.069714
Episode 3500: Total Loss of tensor([[-147.4293]], grad_fn=<SubBackward0>)
[2022-11-05 07:11:49.977157] Process 4. Episode 3750, average_reward -0.074133
Episode 3750: Total Loss of tensor([[11.0335]], grad_fn=<SubBackward0>)
[2022-11-05 07:12:05.322078] Process 3. Episode 3550, average_reward -0.069859
Episode 3550: Total Loss of tensor([[2.2347]], grad_fn=<SubBackward0>)
[2022-11-05 07:12:13.447344] Process 2. Episode 3550, average_reward -0.079718
Episode 3550: Total Loss of tensor([[-6.3392]], grad_fn=<SubBackward0>)
[2022-11-05 07:13:00.565459] Process 0. Episode 3600, average_reward -0.076389
Episode 3600: Total Loss of tensor([[10.4647]], grad_fn=<SubBackward0>)
[2022-11-05 07:13:21.199539] Process 5. Episode 3650, average_reward -0.070959
Episode 3650: Total Loss of tensor([[6.1337]], grad_fn=<SubBackward0>)
[2022-11-05 07:13:36.712805] Process 1. Episode 3550, average_reward -0.069296
Episode 3550: Total Loss of tensor([[19.3235]], grad_fn=<SubBackward0>)
[2022-11-05 07:14:16.457854] Process 4. Episode 3800, average_reward -0.073947
Episode 3800: Total Loss of tensor([[25.2721]], grad_fn=<SubBackward0>)
[2022-11-05 07:14:27.414598] Process 3. Episode 3600, average_reward -0.070833
Episode 3600: Total Loss of tensor([[1.8231]], grad_fn=<SubBackward0>)
[2022-11-05 07:14:43.283135] Process 2. Episode 3600, average_reward -0.080000
Episode 3600: Total Loss of tensor([[-33.4208]], grad_fn=<SubBackward0>)
[2022-11-05 07:15:32.844802] Process 0. Episode 3650, average_reward -0.075616
Episode 3650: Total Loss of tensor([[-10.3317]], grad_fn=<SubBackward0>)
[2022-11-05 07:15:53.030481] Process 5. Episode 3700, average_reward -0.072703
Episode 3700: Total Loss of tensor([[8.3290]], grad_fn=<SubBackward0>)
[2022-11-05 07:16:09.114351] Process 1. Episode 3600, average_reward -0.069722
Episode 3600: Total Loss of tensor([[-0.3199]], grad_fn=<SubBackward0>)
[2022-11-05 07:16:40.924162] Process 4. Episode 3850, average_reward -0.074286
Episode 3850: Total Loss of tensor([[-1.9815]], grad_fn=<SubBackward0>)
[2022-11-05 07:16:56.629041] Process 3. Episode 3650, average_reward -0.070685
Episode 3650: Total Loss of tensor([[-2.7570]], grad_fn=<SubBackward0>)
[2022-11-05 07:17:07.184568] Process 2. Episode 3650, average_reward -0.079726
Episode 3650: Total Loss of tensor([[-9.0212]], grad_fn=<SubBackward0>)
[2022-11-05 07:18:02.355499] Process 0. Episode 3700, average_reward -0.075405
Episode 3700: Total Loss of tensor([[6.3189]], grad_fn=<SubBackward0>)
[2022-11-05 07:18:27.511173] Process 5. Episode 3750, average_reward -0.071733
Episode 3750: Total Loss of tensor([[4.1563]], grad_fn=<SubBackward0>)
[2022-11-05 07:18:44.837316] Process 1. Episode 3650, average_reward -0.069863
Episode 3650: Total Loss of tensor([[-7.9566]], grad_fn=<SubBackward0>)
[2022-11-05 07:19:04.747441] Process 4. Episode 3900, average_reward -0.074615
Episode 3900: Total Loss of tensor([[-5.4348]], grad_fn=<SubBackward0>)
[2022-11-05 07:19:32.740706] Process 3. Episode 3700, average_reward -0.070811
Episode 3700: Total Loss of tensor([[14.9536]], grad_fn=<SubBackward0>)
[2022-11-05 07:19:35.872644] Process 2. Episode 3700, average_reward -0.079189
Episode 3700: Total Loss of tensor([[-3.3984]], grad_fn=<SubBackward0>)
[2022-11-05 07:20:42.564005] Process 0. Episode 3750, average_reward -0.075200
Episode 3750: Total Loss of tensor([[8.0977]], grad_fn=<SubBackward0>)
[2022-11-05 07:21:01.378606] Process 5. Episode 3800, average_reward -0.071842
Episode 3800: Total Loss of tensor([[4.6974]], grad_fn=<SubBackward0>)
[2022-11-05 07:21:23.158141] Process 1. Episode 3700, average_reward -0.069730
Episode 3700: Total Loss of tensor([[-13.5789]], grad_fn=<SubBackward0>)
[2022-11-05 07:21:25.746216] Process 4. Episode 3950, average_reward -0.074684
Episode 3950: Total Loss of tensor([[-2.6646]], grad_fn=<SubBackward0>)
[2022-11-05 07:22:05.046264] Process 2. Episode 3750, average_reward -0.078933
Episode 3750: Total Loss of tensor([[-135.5272]], grad_fn=<SubBackward0>)
[2022-11-05 07:22:09.554750] Process 3. Episode 3750, average_reward -0.070933
Episode 3750: Total Loss of tensor([[1.3396]], grad_fn=<SubBackward0>)
[2022-11-05 07:23:20.936803] Process 0. Episode 3800, average_reward -0.074474
Episode 3800: Total Loss of tensor([[27.5115]], grad_fn=<SubBackward0>)
[2022-11-05 07:23:29.901942] Process 5. Episode 3850, average_reward -0.071948
Episode 3850: Total Loss of tensor([[10.1944]], grad_fn=<SubBackward0>)
[2022-11-05 07:23:45.850678] Process 4. Episode 4000, average_reward -0.075250
Episode 4000: Total Loss of tensor([[-9.6987]], grad_fn=<SubBackward0>)
[2022-11-05 07:24:01.375643] Process 1. Episode 3750, average_reward -0.069867
Episode 3750: Total Loss of tensor([[6.6859]], grad_fn=<SubBackward0>)
[2022-11-05 07:24:29.939436] Process 2. Episode 3800, average_reward -0.078684
Episode 3800: Total Loss of tensor([[8.3640]], grad_fn=<SubBackward0>)
[2022-11-05 07:24:44.298120] Process 3. Episode 3800, average_reward -0.070789
Episode 3800: Total Loss of tensor([[4.4518]], grad_fn=<SubBackward0>)
[2022-11-05 07:26:02.808361] Process 0. Episode 3850, average_reward -0.074545
Episode 3850: Total Loss of tensor([[6.8860]], grad_fn=<SubBackward0>)
[2022-11-05 07:26:03.174920] Process 5. Episode 3900, average_reward -0.071795
Episode 3900: Total Loss of tensor([[-1.1025]], grad_fn=<SubBackward0>)
[2022-11-05 07:26:05.331115] Process 4. Episode 4050, average_reward -0.075062
Episode 4050: Total Loss of tensor([[1.1964]], grad_fn=<SubBackward0>)
[2022-11-05 07:26:38.476484] Process 1. Episode 3800, average_reward -0.070526
Episode 3800: Total Loss of tensor([[-2.6540]], grad_fn=<SubBackward0>)
[2022-11-05 07:26:54.747086] Process 2. Episode 3850, average_reward -0.077922
Episode 3850: Total Loss of tensor([[8.3523]], grad_fn=<SubBackward0>)
[2022-11-05 07:27:21.716948] Process 3. Episode 3850, average_reward -0.070909
Episode 3850: Total Loss of tensor([[14.8562]], grad_fn=<SubBackward0>)
[2022-11-05 07:28:32.783001] Process 4. Episode 4100, average_reward -0.075122
Episode 4100: Total Loss of tensor([[29.4154]], grad_fn=<SubBackward0>)
[2022-11-05 07:28:33.864868] Process 0. Episode 3900, average_reward -0.075128
Episode 3900: Total Loss of tensor([[4.1202]], grad_fn=<SubBackward0>)
[2022-11-05 07:28:36.057827] Process 5. Episode 3950, average_reward -0.071899
Episode 3950: Total Loss of tensor([[15.3708]], grad_fn=<SubBackward0>)
[2022-11-05 07:29:16.317106] Process 1. Episode 3850, average_reward -0.070909
Episode 3850: Total Loss of tensor([[4.9028]], grad_fn=<SubBackward0>)
[2022-11-05 07:29:22.048157] Process 2. Episode 3900, average_reward -0.077692
Episode 3900: Total Loss of tensor([[8.2476]], grad_fn=<SubBackward0>)
[2022-11-05 07:29:48.922361] Process 3. Episode 3900, average_reward -0.071282
Episode 3900: Total Loss of tensor([[6.8682]], grad_fn=<SubBackward0>)
[2022-11-05 07:30:55.893144] Process 4. Episode 4150, average_reward -0.075663
Episode 4150: Total Loss of tensor([[2.4451]], grad_fn=<SubBackward0>)
[2022-11-05 07:31:10.011396] Process 0. Episode 3950, average_reward -0.074430
Episode 3950: Total Loss of tensor([[12.6340]], grad_fn=<SubBackward0>)
[2022-11-05 07:31:13.442588] Process 5. Episode 4000, average_reward -0.072000
Episode 4000: Total Loss of tensor([[2.2971]], grad_fn=<SubBackward0>)
[2022-11-05 07:31:52.544651] Process 1. Episode 3900, average_reward -0.070513
Episode 3900: Total Loss of tensor([[-1.2598]], grad_fn=<SubBackward0>)
[2022-11-05 07:31:58.384826] Process 2. Episode 3950, average_reward -0.077468
Episode 3950: Total Loss of tensor([[-1.4936]], grad_fn=<SubBackward0>)
[2022-11-05 07:32:11.763702] Process 3. Episode 3950, average_reward -0.070886
Episode 3950: Total Loss of tensor([[-3.1615]], grad_fn=<SubBackward0>)
[2022-11-05 07:33:16.825893] Process 4. Episode 4200, average_reward -0.075238
Episode 4200: Total Loss of tensor([[9.8047]], grad_fn=<SubBackward0>)
[2022-11-05 07:33:35.459527] Process 0. Episode 4000, average_reward -0.074500
Episode 4000: Total Loss of tensor([[10.2939]], grad_fn=<SubBackward0>)
[2022-11-05 07:33:41.521082] Process 5. Episode 4050, average_reward -0.072346
Episode 4050: Total Loss of tensor([[-6.4462]], grad_fn=<SubBackward0>)
[2022-11-05 07:34:30.931194] Process 2. Episode 4000, average_reward -0.077000
Episode 4000: Total Loss of tensor([[-0.9612]], grad_fn=<SubBackward0>)
[2022-11-05 07:34:31.647391] Process 1. Episode 3950, average_reward -0.070127
Episode 3950: Total Loss of tensor([[7.9599]], grad_fn=<SubBackward0>)
[2022-11-05 07:34:48.711043] Process 3. Episode 4000, average_reward -0.071000
Episode 4000: Total Loss of tensor([[9.4524]], grad_fn=<SubBackward0>)
[2022-11-05 07:35:40.821959] Process 4. Episode 4250, average_reward -0.075059
Episode 4250: Total Loss of tensor([[19.1763]], grad_fn=<SubBackward0>)
[2022-11-05 07:36:08.462783] Process 0. Episode 4050, average_reward -0.073827
Episode 4050: Total Loss of tensor([[8.9442]], grad_fn=<SubBackward0>)
[2022-11-05 07:36:13.664304] Process 5. Episode 4100, average_reward -0.072683
Episode 4100: Total Loss of tensor([[3.8936]], grad_fn=<SubBackward0>)
[2022-11-05 07:36:56.884325] Process 2. Episode 4050, average_reward -0.077531
Episode 4050: Total Loss of tensor([[-2.4256]], grad_fn=<SubBackward0>)
[2022-11-05 07:37:04.330984] Process 1. Episode 4000, average_reward -0.069500
Episode 4000: Total Loss of tensor([[-3.3463]], grad_fn=<SubBackward0>)
[2022-11-05 07:37:23.401693] Process 3. Episode 4050, average_reward -0.071358
Episode 4050: Total Loss of tensor([[-124.3846]], grad_fn=<SubBackward0>)
[2022-11-05 07:38:03.157367] Process 4. Episode 4300, average_reward -0.075349
Episode 4300: Total Loss of tensor([[-122.2073]], grad_fn=<SubBackward0>)
[2022-11-05 07:38:39.224274] Process 0. Episode 4100, average_reward -0.073659
Episode 4100: Total Loss of tensor([[17.7989]], grad_fn=<SubBackward0>)
[2022-11-05 07:38:57.553949] Process 5. Episode 4150, average_reward -0.072530
Episode 4150: Total Loss of tensor([[6.3357]], grad_fn=<SubBackward0>)
[2022-11-05 07:39:32.966801] Process 2. Episode 4100, average_reward -0.077805
Episode 4100: Total Loss of tensor([[10.1616]], grad_fn=<SubBackward0>)
[2022-11-05 07:39:41.746467] Process 1. Episode 4050, average_reward -0.069383
Episode 4050: Total Loss of tensor([[16.0239]], grad_fn=<SubBackward0>)
[2022-11-05 07:39:45.696324] Process 3. Episode 4100, average_reward -0.071951
Episode 4100: Total Loss of tensor([[5.9020]], grad_fn=<SubBackward0>)
[2022-11-05 07:40:26.366817] Process 4. Episode 4350, average_reward -0.075172
Episode 4350: Total Loss of tensor([[12.5235]], grad_fn=<SubBackward0>)
[2022-11-05 07:41:12.874694] Process 0. Episode 4150, average_reward -0.073976
Episode 4150: Total Loss of tensor([[21.1209]], grad_fn=<SubBackward0>)
[2022-11-05 07:41:33.576122] Process 5. Episode 4200, average_reward -0.073095
Episode 4200: Total Loss of tensor([[7.0332]], grad_fn=<SubBackward0>)
[2022-11-05 07:41:57.877739] Process 2. Episode 4150, average_reward -0.077108
Episode 4150: Total Loss of tensor([[4.1639]], grad_fn=<SubBackward0>)
[2022-11-05 07:42:18.831113] Process 1. Episode 4100, average_reward -0.069512
Episode 4100: Total Loss of tensor([[4.2271]], grad_fn=<SubBackward0>)
[2022-11-05 07:42:19.017301] Process 3. Episode 4150, average_reward -0.071807
Episode 4150: Total Loss of tensor([[7.5573]], grad_fn=<SubBackward0>)
[2022-11-05 07:42:53.397648] Process 4. Episode 4400, average_reward -0.074773
Episode 4400: Total Loss of tensor([[13.8837]], grad_fn=<SubBackward0>)
[2022-11-05 07:43:39.646638] Process 0. Episode 4200, average_reward -0.073333
Episode 4200: Total Loss of tensor([[6.6537]], grad_fn=<SubBackward0>)
[2022-11-05 07:44:08.760380] Process 5. Episode 4250, average_reward -0.072706
Episode 4250: Total Loss of tensor([[6.4737]], grad_fn=<SubBackward0>)
[2022-11-05 07:44:21.318485] Process 2. Episode 4200, average_reward -0.077619
Episode 4200: Total Loss of tensor([[8.0662]], grad_fn=<SubBackward0>)
[2022-11-05 07:44:51.787446] Process 3. Episode 4200, average_reward -0.071905
Episode 4200: Total Loss of tensor([[4.2873]], grad_fn=<SubBackward0>)
[2022-11-05 07:45:03.086350] Process 1. Episode 4150, average_reward -0.068916
Episode 4150: Total Loss of tensor([[7.0154]], grad_fn=<SubBackward0>)
[2022-11-05 07:45:15.960673] Process 4. Episode 4450, average_reward -0.074831
Episode 4450: Total Loss of tensor([[6.2111]], grad_fn=<SubBackward0>)
[2022-11-05 07:46:10.994644] Process 0. Episode 4250, average_reward -0.072706
Episode 4250: Total Loss of tensor([[5.1127]], grad_fn=<SubBackward0>)
[2022-11-05 07:46:37.925176] Process 5. Episode 4300, average_reward -0.072326
Episode 4300: Total Loss of tensor([[-4.6290]], grad_fn=<SubBackward0>)
[2022-11-05 07:46:44.858761] Process 2. Episode 4250, average_reward -0.076941
Episode 4250: Total Loss of tensor([[-6.9615]], grad_fn=<SubBackward0>)
[2022-11-05 07:47:18.256809] Process 3. Episode 4250, average_reward -0.071765
Episode 4250: Total Loss of tensor([[1.0057]], grad_fn=<SubBackward0>)
[2022-11-05 07:47:38.348632] Process 4. Episode 4500, average_reward -0.074667
Episode 4500: Total Loss of tensor([[5.4414]], grad_fn=<SubBackward0>)
[2022-11-05 07:47:40.295824] Process 1. Episode 4200, average_reward -0.068571
Episode 4200: Total Loss of tensor([[2.4977]], grad_fn=<SubBackward0>)
[2022-11-05 07:48:51.392780] Process 0. Episode 4300, average_reward -0.072093
Episode 4300: Total Loss of tensor([[-5.4525]], grad_fn=<SubBackward0>)
[2022-11-05 07:49:04.563763] Process 5. Episode 4350, average_reward -0.072184
Episode 4350: Total Loss of tensor([[7.4433]], grad_fn=<SubBackward0>)
[2022-11-05 07:49:17.897384] Process 2. Episode 4300, average_reward -0.076977
Episode 4300: Total Loss of tensor([[2.4480]], grad_fn=<SubBackward0>)
[2022-11-05 07:49:59.150803] Process 4. Episode 4550, average_reward -0.075385
Episode 4550: Total Loss of tensor([[-24.8841]], grad_fn=<SubBackward0>)
[2022-11-05 07:49:59.359437] Process 3. Episode 4300, average_reward -0.071395
Episode 4300: Total Loss of tensor([[-1.9657]], grad_fn=<SubBackward0>)
[2022-11-05 07:50:15.755145] Process 1. Episode 4250, average_reward -0.068471
Episode 4250: Total Loss of tensor([[9.2988]], grad_fn=<SubBackward0>)
[2022-11-05 07:51:17.202140] Process 0. Episode 4350, average_reward -0.072414
Episode 4350: Total Loss of tensor([[-86.7135]], grad_fn=<SubBackward0>)
[2022-11-05 07:51:37.953478] Process 2. Episode 4350, average_reward -0.077011
Episode 4350: Total Loss of tensor([[-7.8017]], grad_fn=<SubBackward0>)
[2022-11-05 07:51:40.226667] Process 5. Episode 4400, average_reward -0.072273
Episode 4400: Total Loss of tensor([[-4.8395]], grad_fn=<SubBackward0>)
[2022-11-05 07:52:23.212442] Process 4. Episode 4600, average_reward -0.076522
Episode 4600: Total Loss of tensor([[-2.0739]], grad_fn=<SubBackward0>)
[2022-11-05 07:52:37.128413] Process 3. Episode 4350, average_reward -0.071034
Episode 4350: Total Loss of tensor([[14.5146]], grad_fn=<SubBackward0>)
[2022-11-05 07:52:43.368778] Process 1. Episode 4300, average_reward -0.068837
Episode 4300: Total Loss of tensor([[25.4626]], grad_fn=<SubBackward0>)
[2022-11-05 07:53:54.356049] Process 0. Episode 4400, average_reward -0.072273
Episode 4400: Total Loss of tensor([[1.8090]], grad_fn=<SubBackward0>)
[2022-11-05 07:54:07.436275] Process 2. Episode 4400, average_reward -0.076818
Episode 4400: Total Loss of tensor([[18.4343]], grad_fn=<SubBackward0>)
[2022-11-05 07:54:15.367286] Process 5. Episode 4450, average_reward -0.073034
Episode 4450: Total Loss of tensor([[24.6946]], grad_fn=<SubBackward0>)
[2022-11-05 07:54:50.197400] Process 4. Episode 4650, average_reward -0.076129
Episode 4650: Total Loss of tensor([[6.7033]], grad_fn=<SubBackward0>)
[2022-11-05 07:55:07.095160] Process 3. Episode 4400, average_reward -0.071818
Episode 4400: Total Loss of tensor([[-128.0703]], grad_fn=<SubBackward0>)
[2022-11-05 07:55:12.517398] Process 1. Episode 4350, average_reward -0.069885
Episode 4350: Total Loss of tensor([[7.0843]], grad_fn=<SubBackward0>)
[2022-11-05 07:56:31.100949] Process 0. Episode 4450, average_reward -0.072135
Episode 4450: Total Loss of tensor([[8.2768]], grad_fn=<SubBackward0>)
[2022-11-05 07:56:45.916524] Process 5. Episode 4500, average_reward -0.072667
Episode 4500: Total Loss of tensor([[4.3504]], grad_fn=<SubBackward0>)
[2022-11-05 07:56:50.051132] Process 2. Episode 4450, average_reward -0.076180
Episode 4450: Total Loss of tensor([[2.2066]], grad_fn=<SubBackward0>)
[2022-11-05 07:57:12.847269] Process 4. Episode 4700, average_reward -0.075106
Episode 4700: Total Loss of tensor([[0.3444]], grad_fn=<SubBackward0>)
[2022-11-05 07:57:34.463200] Process 3. Episode 4450, average_reward -0.071910
Episode 4450: Total Loss of tensor([[-125.9241]], grad_fn=<SubBackward0>)
[2022-11-05 07:57:46.768097] Process 1. Episode 4400, average_reward -0.069318
Episode 4400: Total Loss of tensor([[1.7090]], grad_fn=<SubBackward0>)
[2022-11-05 07:59:03.457135] Process 0. Episode 4500, average_reward -0.073333
Episode 4500: Total Loss of tensor([[-30.8597]], grad_fn=<SubBackward0>)
[2022-11-05 07:59:17.982303] Process 5. Episode 4550, average_reward -0.072088
Episode 4550: Total Loss of tensor([[3.2328]], grad_fn=<SubBackward0>)
[2022-11-05 07:59:30.481934] Process 2. Episode 4500, average_reward -0.076889
Episode 4500: Total Loss of tensor([[13.6957]], grad_fn=<SubBackward0>)
[2022-11-05 07:59:33.635608] Process 4. Episode 4750, average_reward -0.074947
Episode 4750: Total Loss of tensor([[2.8181]], grad_fn=<SubBackward0>)
[2022-11-05 08:00:15.412986] Process 3. Episode 4500, average_reward -0.071778
Episode 4500: Total Loss of tensor([[7.6742]], grad_fn=<SubBackward0>)
[2022-11-05 08:00:24.024165] Process 1. Episode 4450, average_reward -0.069663
Episode 4450: Total Loss of tensor([[0.7564]], grad_fn=<SubBackward0>)
[2022-11-05 08:01:32.257133] Process 0. Episode 4550, average_reward -0.073407
Episode 4550: Total Loss of tensor([[-84.5389]], grad_fn=<SubBackward0>)
[2022-11-05 08:01:53.369646] Process 4. Episode 4800, average_reward -0.075208
Episode 4800: Total Loss of tensor([[8.1798]], grad_fn=<SubBackward0>)
[2022-11-05 08:01:59.993181] Process 2. Episode 4550, average_reward -0.077143
Episode 4550: Total Loss of tensor([[5.4453]], grad_fn=<SubBackward0>)
[2022-11-05 08:02:02.984375] Process 5. Episode 4600, average_reward -0.071957
Episode 4600: Total Loss of tensor([[9.1749]], grad_fn=<SubBackward0>)
[2022-11-05 08:02:50.653768] Process 3. Episode 4550, average_reward -0.071868
Episode 4550: Total Loss of tensor([[4.3347]], grad_fn=<SubBackward0>)
[2022-11-05 08:02:55.522460] Process 1. Episode 4500, average_reward -0.069778
Episode 4500: Total Loss of tensor([[-135.1133]], grad_fn=<SubBackward0>)
[2022-11-05 08:04:05.684649] Process 0. Episode 4600, average_reward -0.073696
Episode 4600: Total Loss of tensor([[5.6084]], grad_fn=<SubBackward0>)
[2022-11-05 08:04:15.843404] Process 4. Episode 4850, average_reward -0.074845
Episode 4850: Total Loss of tensor([[14.0087]], grad_fn=<SubBackward0>)
[2022-11-05 08:04:28.996138] Process 2. Episode 4600, average_reward -0.076957
Episode 4600: Total Loss of tensor([[-1.3128]], grad_fn=<SubBackward0>)
[2022-11-05 08:04:39.680451] Process 5. Episode 4650, average_reward -0.071828
Episode 4650: Total Loss of tensor([[-6.8801]], grad_fn=<SubBackward0>)
[2022-11-05 08:05:22.191097] Process 3. Episode 4600, average_reward -0.071522
Episode 4600: Total Loss of tensor([[0.9732]], grad_fn=<SubBackward0>)
[2022-11-05 08:05:28.738922] Process 1. Episode 4550, average_reward -0.069451
Episode 4550: Total Loss of tensor([[-1.6279]], grad_fn=<SubBackward0>)
[2022-11-05 08:06:34.942351] Process 0. Episode 4650, average_reward -0.073548
Episode 4650: Total Loss of tensor([[12.4989]], grad_fn=<SubBackward0>)
[2022-11-05 08:06:37.474723] Process 4. Episode 4900, average_reward -0.075510
Episode 4900: Total Loss of tensor([[6.1181]], grad_fn=<SubBackward0>)
[2022-11-05 08:06:54.217644] Process 2. Episode 4650, average_reward -0.077419
Episode 4650: Total Loss of tensor([[16.7183]], grad_fn=<SubBackward0>)
[2022-11-05 08:07:10.337973] Process 5. Episode 4700, average_reward -0.071702
Episode 4700: Total Loss of tensor([[14.4565]], grad_fn=<SubBackward0>)
[2022-11-05 08:08:07.544530] Process 3. Episode 4650, average_reward -0.071828
Episode 4650: Total Loss of tensor([[-13.0356]], grad_fn=<SubBackward0>)
[2022-11-05 08:08:07.695519] Process 1. Episode 4600, average_reward -0.069565
Episode 4600: Total Loss of tensor([[7.4365]], grad_fn=<SubBackward0>)
[2022-11-05 08:09:01.734179] Process 4. Episode 4950, average_reward -0.075556
Episode 4950: Total Loss of tensor([[-2.5826]], grad_fn=<SubBackward0>)
[2022-11-05 08:09:03.844107] Process 0. Episode 4700, average_reward -0.073191
Episode 4700: Total Loss of tensor([[6.3748]], grad_fn=<SubBackward0>)
[2022-11-05 08:09:17.192718] Process 2. Episode 4700, average_reward -0.077234
Episode 4700: Total Loss of tensor([[7.7478]], grad_fn=<SubBackward0>)
[2022-11-05 08:09:36.669953] Process 5. Episode 4750, average_reward -0.071368
Episode 4750: Total Loss of tensor([[-3.3253]], grad_fn=<SubBackward0>)
[2022-11-05 08:10:48.413318] Process 1. Episode 4650, average_reward -0.069892
Episode 4650: Total Loss of tensor([[7.2749]], grad_fn=<SubBackward0>)
[2022-11-05 08:10:54.237464] Process 3. Episode 4700, average_reward -0.071915
Episode 4700: Total Loss of tensor([[5.6233]], grad_fn=<SubBackward0>)
[2022-11-05 08:11:22.182963] Process 4. Episode 5000, average_reward -0.075000
Episode 5000: Total Loss of tensor([[12.7482]], grad_fn=<SubBackward0>)
[2022-11-05 08:11:34.468017] Process 0. Episode 4750, average_reward -0.073263
Episode 4750: Total Loss of tensor([[8.4384]], grad_fn=<SubBackward0>)
[2022-11-05 08:11:45.510982] Process 2. Episode 4750, average_reward -0.077474
Episode 4750: Total Loss of tensor([[-139.5446]], grad_fn=<SubBackward0>)
[2022-11-05 08:12:08.818287] Process 5. Episode 4800, average_reward -0.071667
Episode 4800: Total Loss of tensor([[1.1005]], grad_fn=<SubBackward0>)
[2022-11-05 08:13:19.938397] Process 3. Episode 4750, average_reward -0.071579
Episode 4750: Total Loss of tensor([[7.7651]], grad_fn=<SubBackward0>)
[2022-11-05 08:13:34.875376] Process 1. Episode 4700, average_reward -0.070426
Episode 4700: Total Loss of tensor([[4.2480]], grad_fn=<SubBackward0>)
[2022-11-05 08:13:42.802553] Process 4. Episode 5050, average_reward -0.075644
Episode 5050: Total Loss of tensor([[5.7881]], grad_fn=<SubBackward0>)
[2022-11-05 08:14:05.908170] Process 0. Episode 4800, average_reward -0.072500
Episode 4800: Total Loss of tensor([[5.5842]], grad_fn=<SubBackward0>)
[2022-11-05 08:14:08.685750] Process 2. Episode 4800, average_reward -0.077083
Episode 4800: Total Loss of tensor([[9.1665]], grad_fn=<SubBackward0>)
[2022-11-05 08:14:47.572266] Process 5. Episode 4850, average_reward -0.071753
Episode 4850: Total Loss of tensor([[-6.3808]], grad_fn=<SubBackward0>)
[2022-11-05 08:15:41.003069] Process 3. Episode 4800, average_reward -0.071875
Episode 4800: Total Loss of tensor([[9.6934]], grad_fn=<SubBackward0>)
[2022-11-05 08:16:05.453500] Process 4. Episode 5100, average_reward -0.075882
Episode 5100: Total Loss of tensor([[-3.0693]], grad_fn=<SubBackward0>)
[2022-11-05 08:16:14.903480] Process 1. Episode 4750, average_reward -0.070105
Episode 4750: Total Loss of tensor([[6.8205]], grad_fn=<SubBackward0>)
[2022-11-05 08:16:30.955158] Process 2. Episode 4850, average_reward -0.076907
Episode 4850: Total Loss of tensor([[24.9124]], grad_fn=<SubBackward0>)
[2022-11-05 08:16:53.150662] Process 0. Episode 4850, average_reward -0.071959
Episode 4850: Total Loss of tensor([[15.1442]], grad_fn=<SubBackward0>)
[2022-11-05 08:17:23.968438] Process 5. Episode 4900, average_reward -0.071224
Episode 4900: Total Loss of tensor([[11.5914]], grad_fn=<SubBackward0>)
[2022-11-05 08:18:04.783487] Process 3. Episode 4850, average_reward -0.072371
Episode 4850: Total Loss of tensor([[11.2519]], grad_fn=<SubBackward0>)
[2022-11-05 08:18:29.000939] Process 4. Episode 5150, average_reward -0.075728
Episode 5150: Total Loss of tensor([[-2.2421]], grad_fn=<SubBackward0>)
[2022-11-05 08:18:52.000069] Process 1. Episode 4800, average_reward -0.070417
Episode 4800: Total Loss of tensor([[5.1399]], grad_fn=<SubBackward0>)
[2022-11-05 08:18:54.213838] Process 2. Episode 4900, average_reward -0.076531
Episode 4900: Total Loss of tensor([[-0.1808]], grad_fn=<SubBackward0>)
[2022-11-05 08:19:36.015479] Process 0. Episode 4900, average_reward -0.071837
Episode 4900: Total Loss of tensor([[8.2812]], grad_fn=<SubBackward0>)
[2022-11-05 08:19:58.030712] Process 5. Episode 4950, average_reward -0.071313
Episode 4950: Total Loss of tensor([[1.6207]], grad_fn=<SubBackward0>)
[2022-11-05 08:20:30.445352] Process 3. Episode 4900, average_reward -0.072653
Episode 4900: Total Loss of tensor([[-2.0640]], grad_fn=<SubBackward0>)
[2022-11-05 08:20:50.732259] Process 4. Episode 5200, average_reward -0.075385
Episode 5200: Total Loss of tensor([[22.0517]], grad_fn=<SubBackward0>)
[2022-11-05 08:21:24.506538] Process 2. Episode 4950, average_reward -0.076364
Episode 4950: Total Loss of tensor([[-48.0894]], grad_fn=<SubBackward0>)
[2022-11-05 08:21:31.542625] Process 1. Episode 4850, average_reward -0.070309
Episode 4850: Total Loss of tensor([[2.1593]], grad_fn=<SubBackward0>)
[2022-11-05 08:22:07.623209] Process 0. Episode 4950, average_reward -0.072525
Episode 4950: Total Loss of tensor([[-5.2897]], grad_fn=<SubBackward0>)
[2022-11-05 08:22:25.582704] Process 5. Episode 5000, average_reward -0.071000
Episode 5000: Total Loss of tensor([[-10.4754]], grad_fn=<SubBackward0>)
[2022-11-05 08:22:56.636335] Process 3. Episode 4950, average_reward -0.073333
Episode 4950: Total Loss of tensor([[-157.4250]], grad_fn=<SubBackward0>)
[2022-11-05 08:23:14.414352] Process 4. Episode 5250, average_reward -0.075429
Episode 5250: Total Loss of tensor([[8.8328]], grad_fn=<SubBackward0>)
[2022-11-05 08:23:49.727703] Process 2. Episode 5000, average_reward -0.076400
Episode 5000: Total Loss of tensor([[20.5918]], grad_fn=<SubBackward0>)
[2022-11-05 08:24:10.735726] Process 1. Episode 4900, average_reward -0.070408
Episode 4900: Total Loss of tensor([[-3.5361]], grad_fn=<SubBackward0>)
[2022-11-05 08:24:46.338378] Process 0. Episode 5000, average_reward -0.073000
Episode 5000: Total Loss of tensor([[5.2228]], grad_fn=<SubBackward0>)
[2022-11-05 08:25:06.120933] Process 5. Episode 5050, average_reward -0.071089
Episode 5050: Total Loss of tensor([[3.4052]], grad_fn=<SubBackward0>)
[2022-11-05 08:25:19.950895] Process 3. Episode 5000, average_reward -0.073200
Episode 5000: Total Loss of tensor([[-0.9152]], grad_fn=<SubBackward0>)
[2022-11-05 08:25:37.329787] Process 4. Episode 5300, average_reward -0.074906
Episode 5300: Total Loss of tensor([[0.9422]], grad_fn=<SubBackward0>)
[2022-11-05 08:26:12.012043] Process 2. Episode 5050, average_reward -0.075644
Episode 5050: Total Loss of tensor([[0.9449]], grad_fn=<SubBackward0>)
[2022-11-05 08:26:44.160746] Process 1. Episode 4950, average_reward -0.070101
Episode 4950: Total Loss of tensor([[4.7748]], grad_fn=<SubBackward0>)
[2022-11-05 08:27:20.945793] Process 0. Episode 5050, average_reward -0.072871
Episode 5050: Total Loss of tensor([[3.3441]], grad_fn=<SubBackward0>)
[2022-11-05 08:27:44.065250] Process 3. Episode 5050, average_reward -0.073465
Episode 5050: Total Loss of tensor([[12.1159]], grad_fn=<SubBackward0>)
[2022-11-05 08:27:47.586142] Process 5. Episode 5100, average_reward -0.070980
Episode 5100: Total Loss of tensor([[-3.4994]], grad_fn=<SubBackward0>)
[2022-11-05 08:27:59.424833] Process 4. Episode 5350, average_reward -0.074766
Episode 5350: Total Loss of tensor([[22.7041]], grad_fn=<SubBackward0>)
[2022-11-05 08:28:32.137443] Process 2. Episode 5100, average_reward -0.075686
Episode 5100: Total Loss of tensor([[14.9152]], grad_fn=<SubBackward0>)
[2022-11-05 08:29:16.365498] Process 1. Episode 5000, average_reward -0.070000
Episode 5000: Total Loss of tensor([[-0.6480]], grad_fn=<SubBackward0>)
[2022-11-05 08:29:51.620086] Process 0. Episode 5100, average_reward -0.072941
Episode 5100: Total Loss of tensor([[3.5609]], grad_fn=<SubBackward0>)
[2022-11-05 08:30:17.241368] Process 3. Episode 5100, average_reward -0.073137
Episode 5100: Total Loss of tensor([[6.3845]], grad_fn=<SubBackward0>)
[2022-11-05 08:30:22.094678] Process 4. Episode 5400, average_reward -0.075000
Episode 5400: Total Loss of tensor([[11.8853]], grad_fn=<SubBackward0>)
[2022-11-05 08:30:25.473783] Process 5. Episode 5150, average_reward -0.071650
Episode 5150: Total Loss of tensor([[2.8955]], grad_fn=<SubBackward0>)
[2022-11-05 08:31:04.455111] Process 2. Episode 5150, average_reward -0.076117
Episode 5150: Total Loss of tensor([[-1.8964]], grad_fn=<SubBackward0>)
[2022-11-05 08:31:48.004926] Process 1. Episode 5050, average_reward -0.069901
Episode 5050: Total Loss of tensor([[10.3906]], grad_fn=<SubBackward0>)
[2022-11-05 08:32:23.702070] Process 0. Episode 5150, average_reward -0.073204
Episode 5150: Total Loss of tensor([[-16.8870]], grad_fn=<SubBackward0>)
[2022-11-05 08:32:47.873926] Process 3. Episode 5150, average_reward -0.072816
Episode 5150: Total Loss of tensor([[8.2842]], grad_fn=<SubBackward0>)
[2022-11-05 08:32:48.246813] Process 4. Episode 5450, average_reward -0.075413
Episode 5450: Total Loss of tensor([[-7.2411]], grad_fn=<SubBackward0>)
[2022-11-05 08:32:54.427312] Process 5. Episode 5200, average_reward -0.072115
Episode 5200: Total Loss of tensor([[12.8586]], grad_fn=<SubBackward0>)
[2022-11-05 08:33:31.286370] Process 2. Episode 5200, average_reward -0.075769
Episode 5200: Total Loss of tensor([[-4.1060]], grad_fn=<SubBackward0>)
[2022-11-05 08:34:18.629824] Process 1. Episode 5100, average_reward -0.069804
Episode 5100: Total Loss of tensor([[-11.5108]], grad_fn=<SubBackward0>)
[2022-11-05 08:35:11.326798] Process 4. Episode 5500, average_reward -0.075273
Episode 5500: Total Loss of tensor([[-2.2752]], grad_fn=<SubBackward0>)
[2022-11-05 08:35:12.238369] Process 0. Episode 5200, average_reward -0.072885
Episode 5200: Total Loss of tensor([[14.2353]], grad_fn=<SubBackward0>)
[2022-11-05 08:35:16.463264] Process 3. Episode 5200, average_reward -0.072308
Episode 5200: Total Loss of tensor([[14.9802]], grad_fn=<SubBackward0>)
[2022-11-05 08:35:33.365358] Process 5. Episode 5250, average_reward -0.072000
Episode 5250: Total Loss of tensor([[3.7284]], grad_fn=<SubBackward0>)
[2022-11-05 08:35:57.418002] Process 2. Episode 5250, average_reward -0.076762
Episode 5250: Total Loss of tensor([[-2.6700]], grad_fn=<SubBackward0>)
[2022-11-05 08:36:53.032458] Process 1. Episode 5150, average_reward -0.069903
Episode 5150: Total Loss of tensor([[-3.5101]], grad_fn=<SubBackward0>)
[2022-11-05 08:37:35.405266] Process 4. Episode 5550, average_reward -0.074775
Episode 5550: Total Loss of tensor([[12.1118]], grad_fn=<SubBackward0>)
[2022-11-05 08:37:43.417233] Process 0. Episode 5250, average_reward -0.073143
Episode 5250: Total Loss of tensor([[2.1901]], grad_fn=<SubBackward0>)
[2022-11-05 08:37:43.453977] Process 3. Episode 5250, average_reward -0.072571
Episode 5250: Total Loss of tensor([[-31.1471]], grad_fn=<SubBackward0>)
[2022-11-05 08:38:12.682708] Process 5. Episode 5300, average_reward -0.072642
Episode 5300: Total Loss of tensor([[-39.8403]], grad_fn=<SubBackward0>)
[2022-11-05 08:38:25.596953] Process 2. Episode 5300, average_reward -0.076981
Episode 5300: Total Loss of tensor([[18.4355]], grad_fn=<SubBackward0>)
[2022-11-05 08:39:20.850463] Process 1. Episode 5200, average_reward -0.070192
Episode 5200: Total Loss of tensor([[15.7333]], grad_fn=<SubBackward0>)
[2022-11-05 08:40:00.348560] Process 4. Episode 5600, average_reward -0.074821
Episode 5600: Total Loss of tensor([[8.0085]], grad_fn=<SubBackward0>)
[2022-11-05 08:40:09.461110] Process 0. Episode 5300, average_reward -0.073019
Episode 5300: Total Loss of tensor([[-3.9141]], grad_fn=<SubBackward0>)
[2022-11-05 08:40:19.523398] Process 3. Episode 5300, average_reward -0.072453
Episode 5300: Total Loss of tensor([[9.9226]], grad_fn=<SubBackward0>)
[2022-11-05 08:40:53.877682] Process 5. Episode 5350, average_reward -0.072897
Episode 5350: Total Loss of tensor([[-13.1388]], grad_fn=<SubBackward0>)
[2022-11-05 08:41:04.700961] Process 2. Episode 5350, average_reward -0.076449
Episode 5350: Total Loss of tensor([[9.0611]], grad_fn=<SubBackward0>)
[2022-11-05 08:41:53.997921] Process 1. Episode 5250, average_reward -0.070857
Episode 5250: Total Loss of tensor([[9.6523]], grad_fn=<SubBackward0>)
[2022-11-05 08:42:23.299176] Process 4. Episode 5650, average_reward -0.074867
Episode 5650: Total Loss of tensor([[1.5152]], grad_fn=<SubBackward0>)
[2022-11-05 08:42:32.707093] Process 0. Episode 5350, average_reward -0.073084
Episode 5350: Total Loss of tensor([[24.7713]], grad_fn=<SubBackward0>)
[2022-11-05 08:42:53.650948] Process 3. Episode 5350, average_reward -0.071963
Episode 5350: Total Loss of tensor([[9.7583]], grad_fn=<SubBackward0>)
[2022-11-05 08:43:21.218021] Process 5. Episode 5400, average_reward -0.073333
Episode 5400: Total Loss of tensor([[0.0061]], grad_fn=<SubBackward0>)
[2022-11-05 08:43:30.649516] Process 2. Episode 5400, average_reward -0.076296
Episode 5400: Total Loss of tensor([[-0.1048]], grad_fn=<SubBackward0>)
[2022-11-05 08:44:40.057151] Process 1. Episode 5300, average_reward -0.071321
Episode 5300: Total Loss of tensor([[-52.4179]], grad_fn=<SubBackward0>)
[2022-11-05 08:44:46.207771] Process 4. Episode 5700, average_reward -0.074386
Episode 5700: Total Loss of tensor([[24.8240]], grad_fn=<SubBackward0>)
[2022-11-05 08:44:55.914952] Process 0. Episode 5400, average_reward -0.072963
Episode 5400: Total Loss of tensor([[7.0123]], grad_fn=<SubBackward0>)
[2022-11-05 08:45:21.768121] Process 3. Episode 5400, average_reward -0.071481
Episode 5400: Total Loss of tensor([[0.3609]], grad_fn=<SubBackward0>)
[2022-11-05 08:45:53.614388] Process 2. Episode 5450, average_reward -0.076697
Episode 5450: Total Loss of tensor([[17.0447]], grad_fn=<SubBackward0>)
[2022-11-05 08:45:54.874317] Process 5. Episode 5450, average_reward -0.073028
Episode 5450: Total Loss of tensor([[3.5829]], grad_fn=<SubBackward0>)
[2022-11-05 08:47:08.963511] Process 4. Episode 5750, average_reward -0.074435
Episode 5750: Total Loss of tensor([[2.5918]], grad_fn=<SubBackward0>)
[2022-11-05 08:47:20.131452] Process 1. Episode 5350, average_reward -0.071028
Episode 5350: Total Loss of tensor([[4.7958]], grad_fn=<SubBackward0>)
[2022-11-05 08:47:24.007643] Process 0. Episode 5450, average_reward -0.072477
Episode 5450: Total Loss of tensor([[2.4106]], grad_fn=<SubBackward0>)
[2022-11-05 08:47:54.275496] Process 3. Episode 5450, average_reward -0.071193
Episode 5450: Total Loss of tensor([[-0.9614]], grad_fn=<SubBackward0>)
[2022-11-05 08:48:18.118280] Process 2. Episode 5500, average_reward -0.076364
Episode 5500: Total Loss of tensor([[0.9363]], grad_fn=<SubBackward0>)
[2022-11-05 08:48:25.998160] Process 5. Episode 5500, average_reward -0.073091
Episode 5500: Total Loss of tensor([[10.6396]], grad_fn=<SubBackward0>)
[2022-11-05 08:49:35.399099] Process 4. Episode 5800, average_reward -0.074828
Episode 5800: Total Loss of tensor([[15.6657]], grad_fn=<SubBackward0>)
[2022-11-05 08:49:49.394795] Process 1. Episode 5400, average_reward -0.071296
Episode 5400: Total Loss of tensor([[5.7405]], grad_fn=<SubBackward0>)
[2022-11-05 08:50:02.852476] Process 0. Episode 5500, average_reward -0.072545
Episode 5500: Total Loss of tensor([[4.3476]], grad_fn=<SubBackward0>)
[2022-11-05 08:50:22.030914] Process 3. Episode 5500, average_reward -0.071091
Episode 5500: Total Loss of tensor([[7.3457]], grad_fn=<SubBackward0>)
[2022-11-05 08:50:45.265608] Process 2. Episode 5550, average_reward -0.076036
Episode 5550: Total Loss of tensor([[32.3271]], grad_fn=<SubBackward0>)
[2022-11-05 08:50:59.733610] Process 5. Episode 5550, average_reward -0.072793
Episode 5550: Total Loss of tensor([[14.6997]], grad_fn=<SubBackward0>)
[2022-11-05 08:52:05.812733] Process 4. Episode 5850, average_reward -0.075043
Episode 5850: Total Loss of tensor([[16.5239]], grad_fn=<SubBackward0>)
[2022-11-05 08:52:19.629956] Process 1. Episode 5450, average_reward -0.071560
Episode 5450: Total Loss of tensor([[0.6258]], grad_fn=<SubBackward0>)
[2022-11-05 08:52:30.728411] Process 0. Episode 5550, average_reward -0.072793
Episode 5550: Total Loss of tensor([[15.4066]], grad_fn=<SubBackward0>)
[2022-11-05 08:53:11.315896] Process 2. Episode 5600, average_reward -0.075893
Episode 5600: Total Loss of tensor([[16.0919]], grad_fn=<SubBackward0>)
[2022-11-05 08:53:11.440911] Process 3. Episode 5550, average_reward -0.071351
Episode 5550: Total Loss of tensor([[16.9659]], grad_fn=<SubBackward0>)
[2022-11-05 08:53:27.343874] Process 5. Episode 5600, average_reward -0.072500
Episode 5600: Total Loss of tensor([[9.1167]], grad_fn=<SubBackward0>)
[2022-11-05 08:54:33.296783] Process 4. Episode 5900, average_reward -0.075254
Episode 5900: Total Loss of tensor([[-65.3020]], grad_fn=<SubBackward0>)
[2022-11-05 08:54:47.398110] Process 1. Episode 5500, average_reward -0.070909
Episode 5500: Total Loss of tensor([[-1.5573]], grad_fn=<SubBackward0>)
[2022-11-05 08:55:09.873514] Process 0. Episode 5600, average_reward -0.072321
Episode 5600: Total Loss of tensor([[6.0833]], grad_fn=<SubBackward0>)
[2022-11-05 08:55:34.267024] Process 2. Episode 5650, average_reward -0.075398
Episode 5650: Total Loss of tensor([[-0.2205]], grad_fn=<SubBackward0>)
[2022-11-05 08:55:46.879276] Process 3. Episode 5600, average_reward -0.070893
Episode 5600: Total Loss of tensor([[3.2163]], grad_fn=<SubBackward0>)
[2022-11-05 08:55:53.247412] Process 5. Episode 5650, average_reward -0.072566
Episode 5650: Total Loss of tensor([[2.7738]], grad_fn=<SubBackward0>)
[2022-11-05 08:56:57.760179] Process 4. Episode 5950, average_reward -0.075126
Episode 5950: Total Loss of tensor([[8.7646]], grad_fn=<SubBackward0>)
[2022-11-05 08:57:27.066654] Process 1. Episode 5550, average_reward -0.070811
Episode 5550: Total Loss of tensor([[11.1514]], grad_fn=<SubBackward0>)
[2022-11-05 08:57:46.235822] Process 0. Episode 5650, average_reward -0.072212
Episode 5650: Total Loss of tensor([[7.6100]], grad_fn=<SubBackward0>)
[2022-11-05 08:57:56.019441] Process 2. Episode 5700, average_reward -0.075614
Episode 5700: Total Loss of tensor([[6.1872]], grad_fn=<SubBackward0>)
[2022-11-05 08:58:18.104708] Process 3. Episode 5650, average_reward -0.070619
Episode 5650: Total Loss of tensor([[-2.7191]], grad_fn=<SubBackward0>)
[2022-11-05 08:58:23.767965] Process 5. Episode 5700, average_reward -0.072281
Episode 5700: Total Loss of tensor([[-2.4123]], grad_fn=<SubBackward0>)
[2022-11-05 08:59:19.107212] Process 4. Episode 6000, average_reward -0.075333
Episode 6000: Total Loss of tensor([[1.8800]], grad_fn=<SubBackward0>)
[2022-11-05 08:59:53.119165] Process 1. Episode 5600, average_reward -0.070714
Episode 5600: Total Loss of tensor([[3.3305]], grad_fn=<SubBackward0>)
[2022-11-05 09:00:13.266635] Process 0. Episode 5700, average_reward -0.072456
Episode 5700: Total Loss of tensor([[3.3233]], grad_fn=<SubBackward0>)
[2022-11-05 09:00:26.448373] Process 2. Episode 5750, average_reward -0.075478
Episode 5750: Total Loss of tensor([[5.6994]], grad_fn=<SubBackward0>)
[2022-11-05 09:00:49.060651] Process 3. Episode 5700, average_reward -0.070877
Episode 5700: Total Loss of tensor([[0.6000]], grad_fn=<SubBackward0>)
[2022-11-05 09:00:58.524924] Process 5. Episode 5750, average_reward -0.072348
Episode 5750: Total Loss of tensor([[12.7588]], grad_fn=<SubBackward0>)
[2022-11-05 09:01:41.365164] Process 4. Episode 6050, average_reward -0.075372
Episode 6050: Total Loss of tensor([[9.8116]], grad_fn=<SubBackward0>)
[2022-11-05 09:02:16.302211] Process 1. Episode 5650, average_reward -0.070442
Episode 5650: Total Loss of tensor([[14.0400]], grad_fn=<SubBackward0>)
[2022-11-05 09:02:39.145719] Process 0. Episode 5750, average_reward -0.072870
Episode 5750: Total Loss of tensor([[-8.9156]], grad_fn=<SubBackward0>)
[2022-11-05 09:03:06.548107] Process 2. Episode 5800, average_reward -0.075517
Episode 5800: Total Loss of tensor([[8.0327]], grad_fn=<SubBackward0>)
[2022-11-05 09:03:26.359941] Process 3. Episode 5750, average_reward -0.070609
Episode 5750: Total Loss of tensor([[14.7545]], grad_fn=<SubBackward0>)
[2022-11-05 09:03:26.518792] Process 5. Episode 5800, average_reward -0.072414
Episode 5800: Total Loss of tensor([[2.0355]], grad_fn=<SubBackward0>)
[2022-11-05 09:04:04.991499] Process 4. Episode 6100, average_reward -0.075738
Episode 6100: Total Loss of tensor([[4.5688]], grad_fn=<SubBackward0>)
[2022-11-05 09:04:44.718272] Process 1. Episode 5700, average_reward -0.070000
Episode 5700: Total Loss of tensor([[9.9635]], grad_fn=<SubBackward0>)
[2022-11-05 09:05:10.577545] Process 0. Episode 5800, average_reward -0.073103
Episode 5800: Total Loss of tensor([[-2.4146]], grad_fn=<SubBackward0>)
[2022-11-05 09:05:34.111973] Process 2. Episode 5850, average_reward -0.075556
Episode 5850: Total Loss of tensor([[5.9010]], grad_fn=<SubBackward0>)
[2022-11-05 09:05:53.345466] Process 3. Episode 5800, average_reward -0.070345
Episode 5800: Total Loss of tensor([[-0.7618]], grad_fn=<SubBackward0>)
[2022-11-05 09:05:58.991969] Process 5. Episode 5850, average_reward -0.072479
Episode 5850: Total Loss of tensor([[-1.6005]], grad_fn=<SubBackward0>)
[2022-11-05 09:06:29.256617] Process 4. Episode 6150, average_reward -0.076423
Episode 6150: Total Loss of tensor([[5.9733]], grad_fn=<SubBackward0>)
[2022-11-05 09:07:17.734297] Process 1. Episode 5750, average_reward -0.069565
Episode 5750: Total Loss of tensor([[6.9715]], grad_fn=<SubBackward0>)
[2022-11-05 09:07:55.589626] Process 2. Episode 5900, average_reward -0.075763
Episode 5900: Total Loss of tensor([[5.4986]], grad_fn=<SubBackward0>)
[2022-11-05 09:07:57.863166] Process 0. Episode 5850, average_reward -0.072991
Episode 5850: Total Loss of tensor([[4.4500]], grad_fn=<SubBackward0>)
[2022-11-05 09:08:24.195483] Process 3. Episode 5850, average_reward -0.070427
Episode 5850: Total Loss of tensor([[-8.2055]], grad_fn=<SubBackward0>)
[2022-11-05 09:08:34.275227] Process 5. Episode 5900, average_reward -0.072542
Episode 5900: Total Loss of tensor([[12.3290]], grad_fn=<SubBackward0>)
[2022-11-05 09:08:52.460917] Process 4. Episode 6200, average_reward -0.076290
Episode 6200: Total Loss of tensor([[-0.3851]], grad_fn=<SubBackward0>)
[2022-11-05 09:09:43.352985] Process 1. Episode 5800, average_reward -0.070172
Episode 5800: Total Loss of tensor([[7.5157]], grad_fn=<SubBackward0>)
[2022-11-05 09:10:13.939227] Process 2. Episode 5950, average_reward -0.075798
Episode 5950: Total Loss of tensor([[-0.9293]], grad_fn=<SubBackward0>)
[2022-11-05 09:10:33.133418] Process 0. Episode 5900, average_reward -0.072881
Episode 5900: Total Loss of tensor([[-3.5853]], grad_fn=<SubBackward0>)
[2022-11-05 09:11:00.768896] Process 3. Episode 5900, average_reward -0.070508
Episode 5900: Total Loss of tensor([[7.1799]], grad_fn=<SubBackward0>)
[2022-11-05 09:11:11.838750] Process 4. Episode 6250, average_reward -0.076000
Episode 6250: Total Loss of tensor([[13.1488]], grad_fn=<SubBackward0>)
[2022-11-05 09:11:14.944135] Process 5. Episode 5950, average_reward -0.072269
Episode 5950: Total Loss of tensor([[0.0717]], grad_fn=<SubBackward0>)
[2022-11-05 09:12:07.277465] Process 1. Episode 5850, average_reward -0.069915
Episode 5850: Total Loss of tensor([[13.7002]], grad_fn=<SubBackward0>)
[2022-11-05 09:12:39.815972] Process 2. Episode 6000, average_reward -0.075667
Episode 6000: Total Loss of tensor([[9.0828]], grad_fn=<SubBackward0>)
[2022-11-05 09:13:13.250406] Process 0. Episode 5950, average_reward -0.072941
Episode 5950: Total Loss of tensor([[12.6994]], grad_fn=<SubBackward0>)
[2022-11-05 09:13:31.874157] Process 4. Episode 6300, average_reward -0.076349
Episode 6300: Total Loss of tensor([[4.9048]], grad_fn=<SubBackward0>)
[2022-11-05 09:13:38.718941] Process 5. Episode 6000, average_reward -0.072333
Episode 6000: Total Loss of tensor([[-3.3033]], grad_fn=<SubBackward0>)
[2022-11-05 09:13:42.861315] Process 3. Episode 5950, average_reward -0.070420
Episode 5950: Total Loss of tensor([[25.7006]], grad_fn=<SubBackward0>)
[2022-11-05 09:14:32.338469] Process 1. Episode 5900, average_reward -0.069492
Episode 5900: Total Loss of tensor([[-4.0051]], grad_fn=<SubBackward0>)
[2022-11-05 09:15:11.411964] Process 2. Episode 6050, average_reward -0.075702
Episode 6050: Total Loss of tensor([[10.6011]], grad_fn=<SubBackward0>)
[2022-11-05 09:15:49.494961] Process 0. Episode 6000, average_reward -0.072833
Episode 6000: Total Loss of tensor([[12.2842]], grad_fn=<SubBackward0>)
[2022-11-05 09:15:54.906690] Process 4. Episode 6350, average_reward -0.076378
Episode 6350: Total Loss of tensor([[4.9721]], grad_fn=<SubBackward0>)
[2022-11-05 09:16:08.724259] Process 5. Episode 6050, average_reward -0.072231
Episode 6050: Total Loss of tensor([[8.5698]], grad_fn=<SubBackward0>)
[2022-11-05 09:16:08.970450] Process 3. Episode 6000, average_reward -0.070167
Episode 6000: Total Loss of tensor([[11.4562]], grad_fn=<SubBackward0>)
[2022-11-05 09:17:05.087066] Process 1. Episode 5950, average_reward -0.069580
Episode 5950: Total Loss of tensor([[18.5562]], grad_fn=<SubBackward0>)
[2022-11-05 09:17:37.299032] Process 2. Episode 6100, average_reward -0.075738
Episode 6100: Total Loss of tensor([[9.4653]], grad_fn=<SubBackward0>)
[2022-11-05 09:18:18.550985] Process 4. Episode 6400, average_reward -0.076406
Episode 6400: Total Loss of tensor([[10.7645]], grad_fn=<SubBackward0>)
[2022-11-05 09:18:20.695344] Process 0. Episode 6050, average_reward -0.073223
Episode 6050: Total Loss of tensor([[-86.2770]], grad_fn=<SubBackward0>)
[2022-11-05 09:18:37.104360] Process 3. Episode 6050, average_reward -0.070413
Episode 6050: Total Loss of tensor([[-136.2677]], grad_fn=<SubBackward0>)
[2022-11-05 09:18:42.744594] Process 5. Episode 6100, average_reward -0.071639
Episode 6100: Total Loss of tensor([[16.7584]], grad_fn=<SubBackward0>)
[2022-11-05 09:19:39.972015] Process 1. Episode 6000, average_reward -0.070167
Episode 6000: Total Loss of tensor([[-0.7970]], grad_fn=<SubBackward0>)
[2022-11-05 09:19:58.234438] Process 2. Episode 6150, average_reward -0.075610
Episode 6150: Total Loss of tensor([[10.5707]], grad_fn=<SubBackward0>)
[2022-11-05 09:20:39.029165] Process 4. Episode 6450, average_reward -0.076124
Episode 6450: Total Loss of tensor([[-115.3690]], grad_fn=<SubBackward0>)
[2022-11-05 09:20:51.615226] Process 0. Episode 6100, average_reward -0.073770
Episode 6100: Total Loss of tensor([[18.2938]], grad_fn=<SubBackward0>)
[2022-11-05 09:21:15.872986] Process 3. Episode 6100, average_reward -0.070492
Episode 6100: Total Loss of tensor([[8.3852]], grad_fn=<SubBackward0>)
[2022-11-05 09:21:24.895740] Process 5. Episode 6150, average_reward -0.072033
Episode 6150: Total Loss of tensor([[-7.5730]], grad_fn=<SubBackward0>)
[2022-11-05 09:22:16.367562] Process 1. Episode 6050, average_reward -0.070083
Episode 6050: Total Loss of tensor([[12.4546]], grad_fn=<SubBackward0>)
[2022-11-05 09:22:17.491910] Process 2. Episode 6200, average_reward -0.075968
Episode 6200: Total Loss of tensor([[0.1909]], grad_fn=<SubBackward0>)
[2022-11-05 09:23:00.571048] Process 4. Episode 6500, average_reward -0.076000
Episode 6500: Total Loss of tensor([[-3.3898]], grad_fn=<SubBackward0>)
[2022-11-05 09:23:19.011954] Process 0. Episode 6150, average_reward -0.073821
Episode 6150: Total Loss of tensor([[7.6084]], grad_fn=<SubBackward0>)
[2022-11-05 09:23:43.616740] Process 3. Episode 6150, average_reward -0.070244
Episode 6150: Total Loss of tensor([[6.7371]], grad_fn=<SubBackward0>)
[2022-11-05 09:24:02.474642] Process 5. Episode 6200, average_reward -0.072258
Episode 6200: Total Loss of tensor([[-82.9717]], grad_fn=<SubBackward0>)
[2022-11-05 09:24:43.144720] Process 1. Episode 6100, average_reward -0.069672
Episode 6100: Total Loss of tensor([[-106.1591]], grad_fn=<SubBackward0>)
[2022-11-05 09:24:47.305659] Process 2. Episode 6250, average_reward -0.076320
Episode 6250: Total Loss of tensor([[-87.0634]], grad_fn=<SubBackward0>)
[2022-11-05 09:25:25.907456] Process 4. Episode 6550, average_reward -0.076031
Episode 6550: Total Loss of tensor([[-5.5053]], grad_fn=<SubBackward0>)
[2022-11-05 09:25:44.397059] Process 0. Episode 6200, average_reward -0.073548
Episode 6200: Total Loss of tensor([[0.4554]], grad_fn=<SubBackward0>)
[2022-11-05 09:26:21.805512] Process 3. Episode 6200, average_reward -0.070000
Episode 6200: Total Loss of tensor([[-0.4961]], grad_fn=<SubBackward0>)
[2022-11-05 09:26:27.901086] Process 5. Episode 6250, average_reward -0.072320
Episode 6250: Total Loss of tensor([[19.9162]], grad_fn=<SubBackward0>)
[2022-11-05 09:27:10.103768] Process 1. Episode 6150, average_reward -0.069431
Episode 6150: Total Loss of tensor([[1.1285]], grad_fn=<SubBackward0>)
[2022-11-05 09:27:14.274642] Process 2. Episode 6300, average_reward -0.076190
Episode 6300: Total Loss of tensor([[7.4903]], grad_fn=<SubBackward0>)
[2022-11-05 09:27:48.901797] Process 4. Episode 6600, average_reward -0.076061
Episode 6600: Total Loss of tensor([[10.2982]], grad_fn=<SubBackward0>)
[2022-11-05 09:28:19.346623] Process 0. Episode 6250, average_reward -0.072960
Episode 6250: Total Loss of tensor([[20.6996]], grad_fn=<SubBackward0>)
[2022-11-05 09:28:51.726449] Process 3. Episode 6250, average_reward -0.069920
Episode 6250: Total Loss of tensor([[-6.9036]], grad_fn=<SubBackward0>)
[2022-11-05 09:28:57.443318] Process 5. Episode 6300, average_reward -0.072540
Episode 6300: Total Loss of tensor([[18.5700]], grad_fn=<SubBackward0>)
[2022-11-05 09:29:38.007234] Process 2. Episode 6350, average_reward -0.076220
Episode 6350: Total Loss of tensor([[22.8831]], grad_fn=<SubBackward0>)
[2022-11-05 09:29:38.353791] Process 1. Episode 6200, average_reward -0.069677
Episode 6200: Total Loss of tensor([[-127.3437]], grad_fn=<SubBackward0>)
[2022-11-05 09:30:14.246951] Process 4. Episode 6650, average_reward -0.076241
Episode 6650: Total Loss of tensor([[-0.6324]], grad_fn=<SubBackward0>)
[2022-11-05 09:31:03.459292] Process 0. Episode 6300, average_reward -0.073175
Episode 6300: Total Loss of tensor([[31.7111]], grad_fn=<SubBackward0>)
[2022-11-05 09:31:25.416049] Process 3. Episode 6300, average_reward -0.070159
Episode 6300: Total Loss of tensor([[-25.3581]], grad_fn=<SubBackward0>)
[2022-11-05 09:31:30.624706] Process 5. Episode 6350, average_reward -0.072441
Episode 6350: Total Loss of tensor([[16.2262]], grad_fn=<SubBackward0>)
[2022-11-05 09:32:03.982636] Process 2. Episode 6400, average_reward -0.076563
Episode 6400: Total Loss of tensor([[3.0560]], grad_fn=<SubBackward0>)
[2022-11-05 09:32:04.570227] Process 1. Episode 6250, average_reward -0.069760
Episode 6250: Total Loss of tensor([[-8.8762]], grad_fn=<SubBackward0>)
[2022-11-05 09:32:35.519655] Process 4. Episode 6700, average_reward -0.076119
Episode 6700: Total Loss of tensor([[24.1450]], grad_fn=<SubBackward0>)
[2022-11-05 09:33:35.004498] Process 0. Episode 6350, average_reward -0.072913
Episode 6350: Total Loss of tensor([[37.7084]], grad_fn=<SubBackward0>)
[2022-11-05 09:34:02.525392] Process 5. Episode 6400, average_reward -0.072813
Episode 6400: Total Loss of tensor([[-22.9185]], grad_fn=<SubBackward0>)
[2022-11-05 09:34:08.260090] Process 3. Episode 6350, average_reward -0.069921
Episode 6350: Total Loss of tensor([[12.1016]], grad_fn=<SubBackward0>)
[2022-11-05 09:34:25.406283] Process 2. Episode 6450, average_reward -0.076279
Episode 6450: Total Loss of tensor([[-64.7008]], grad_fn=<SubBackward0>)
[2022-11-05 09:34:32.429154] Process 1. Episode 6300, average_reward -0.069841
Episode 6300: Total Loss of tensor([[0.4975]], grad_fn=<SubBackward0>)
[2022-11-05 09:34:58.074888] Process 4. Episode 6750, average_reward -0.076148
Episode 6750: Total Loss of tensor([[6.4635]], grad_fn=<SubBackward0>)
[2022-11-05 09:36:06.716450] Process 0. Episode 6400, average_reward -0.072500
Episode 6400: Total Loss of tensor([[1.2656]], grad_fn=<SubBackward0>)
[2022-11-05 09:36:27.971954] Process 5. Episode 6450, average_reward -0.073178
Episode 6450: Total Loss of tensor([[-5.0827]], grad_fn=<SubBackward0>)
[2022-11-05 09:36:48.302064] Process 2. Episode 6500, average_reward -0.076154
Episode 6500: Total Loss of tensor([[5.8674]], grad_fn=<SubBackward0>)
[2022-11-05 09:36:59.040714] Process 3. Episode 6400, average_reward -0.069687
Episode 6400: Total Loss of tensor([[8.8579]], grad_fn=<SubBackward0>)
[2022-11-05 09:37:03.523680] Process 1. Episode 6350, average_reward -0.069921
Episode 6350: Total Loss of tensor([[-0.7289]], grad_fn=<SubBackward0>)
[2022-11-05 09:37:19.319662] Process 4. Episode 6800, average_reward -0.075882
Episode 6800: Total Loss of tensor([[-3.0685]], grad_fn=<SubBackward0>)
[2022-11-05 09:38:44.420508] Process 0. Episode 6450, average_reward -0.071938
Episode 6450: Total Loss of tensor([[-0.6931]], grad_fn=<SubBackward0>)
[2022-11-05 09:38:58.040164] Process 5. Episode 6500, average_reward -0.072615
Episode 6500: Total Loss of tensor([[-3.1877]], grad_fn=<SubBackward0>)
[2022-11-05 09:39:18.643134] Process 2. Episode 6550, average_reward -0.076183
Episode 6550: Total Loss of tensor([[5.1323]], grad_fn=<SubBackward0>)
[2022-11-05 09:39:35.326441] Process 3. Episode 6450, average_reward -0.069922
Episode 6450: Total Loss of tensor([[14.6233]], grad_fn=<SubBackward0>)
[2022-11-05 09:39:38.778347] Process 1. Episode 6400, average_reward -0.070156
Episode 6400: Total Loss of tensor([[13.5419]], grad_fn=<SubBackward0>)
[2022-11-05 09:39:42.764143] Process 4. Episode 6850, average_reward -0.075766
Episode 6850: Total Loss of tensor([[1.8086]], grad_fn=<SubBackward0>)
[2022-11-05 09:41:20.470511] Process 0. Episode 6500, average_reward -0.071846
Episode 6500: Total Loss of tensor([[11.4215]], grad_fn=<SubBackward0>)
[2022-11-05 09:41:27.007085] Process 5. Episode 6550, average_reward -0.072672
Episode 6550: Total Loss of tensor([[11.1880]], grad_fn=<SubBackward0>)
[2022-11-05 09:41:53.048423] Process 2. Episode 6600, average_reward -0.076364
Episode 6600: Total Loss of tensor([[7.3216]], grad_fn=<SubBackward0>)
[2022-11-05 09:42:10.805952] Process 4. Episode 6900, average_reward -0.075652
Episode 6900: Total Loss of tensor([[1.4985]], grad_fn=<SubBackward0>)
[2022-11-05 09:42:11.588195] Process 1. Episode 6450, average_reward -0.070078
Episode 6450: Total Loss of tensor([[20.4741]], grad_fn=<SubBackward0>)
[2022-11-05 09:42:12.429645] Process 3. Episode 6500, average_reward -0.070154
Episode 6500: Total Loss of tensor([[-0.2180]], grad_fn=<SubBackward0>)
[2022-11-05 09:43:48.740018] Process 5. Episode 6600, average_reward -0.072727
Episode 6600: Total Loss of tensor([[5.7532]], grad_fn=<SubBackward0>)
[2022-11-05 09:43:53.002996] Process 0. Episode 6550, average_reward -0.072061
Episode 6550: Total Loss of tensor([[-0.3709]], grad_fn=<SubBackward0>)
[2022-11-05 09:44:20.880781] Process 2. Episode 6650, average_reward -0.076391
Episode 6650: Total Loss of tensor([[15.7537]], grad_fn=<SubBackward0>)
[2022-11-05 09:44:33.632034] Process 4. Episode 6950, average_reward -0.075827
Episode 6950: Total Loss of tensor([[8.1916]], grad_fn=<SubBackward0>)
[2022-11-05 09:44:40.691590] Process 3. Episode 6550, average_reward -0.069771
Episode 6550: Total Loss of tensor([[-10.5944]], grad_fn=<SubBackward0>)
[2022-11-05 09:44:44.883801] Process 1. Episode 6500, average_reward -0.069846
Episode 6500: Total Loss of tensor([[6.3987]], grad_fn=<SubBackward0>)
[2022-11-05 09:46:14.418645] Process 5. Episode 6650, average_reward -0.072331
Episode 6650: Total Loss of tensor([[8.9896]], grad_fn=<SubBackward0>)
[2022-11-05 09:46:22.270589] Process 0. Episode 6600, average_reward -0.071667
Episode 6600: Total Loss of tensor([[5.2715]], grad_fn=<SubBackward0>)
[2022-11-05 09:46:57.424315] Process 4. Episode 7000, average_reward -0.075714
Episode 7000: Total Loss of tensor([[-5.5680]], grad_fn=<SubBackward0>)
[2022-11-05 09:47:00.868092] Process 2. Episode 6700, average_reward -0.076269
Episode 6700: Total Loss of tensor([[5.8539]], grad_fn=<SubBackward0>)
[2022-11-05 09:47:10.118719] Process 1. Episode 6550, average_reward -0.069771
Episode 6550: Total Loss of tensor([[19.8022]], grad_fn=<SubBackward0>)
[2022-11-05 09:47:10.565064] Process 3. Episode 6600, average_reward -0.070909
Episode 6600: Total Loss of tensor([[6.1327]], grad_fn=<SubBackward0>)
[2022-11-05 09:48:45.040178] Process 5. Episode 6700, average_reward -0.072090
Episode 6700: Total Loss of tensor([[11.6746]], grad_fn=<SubBackward0>)
[2022-11-05 09:48:54.549912] Process 0. Episode 6650, average_reward -0.071429
Episode 6650: Total Loss of tensor([[8.3204]], grad_fn=<SubBackward0>)
[2022-11-05 09:49:19.660579] Process 4. Episode 7050, average_reward -0.076028
Episode 7050: Total Loss of tensor([[-5.0317]], grad_fn=<SubBackward0>)
[2022-11-05 09:49:32.919140] Process 2. Episode 6750, average_reward -0.076148
Episode 6750: Total Loss of tensor([[6.5761]], grad_fn=<SubBackward0>)
[2022-11-05 09:49:41.237298] Process 1. Episode 6600, average_reward -0.069545
Episode 6600: Total Loss of tensor([[2.3862]], grad_fn=<SubBackward0>)
[2022-11-05 09:49:46.211096] Process 3. Episode 6650, average_reward -0.070827
Episode 6650: Total Loss of tensor([[5.4074]], grad_fn=<SubBackward0>)
[2022-11-05 09:51:17.251285] Process 5. Episode 6750, average_reward -0.072000
Episode 6750: Total Loss of tensor([[7.8254]], grad_fn=<SubBackward0>)
[2022-11-05 09:51:24.763169] Process 0. Episode 6700, average_reward -0.071791
Episode 6700: Total Loss of tensor([[-11.6973]], grad_fn=<SubBackward0>)
[2022-11-05 09:51:41.173818] Process 4. Episode 7100, average_reward -0.076479
Episode 7100: Total Loss of tensor([[1.9966]], grad_fn=<SubBackward0>)
[2022-11-05 09:51:55.386991] Process 2. Episode 6800, average_reward -0.076176
Episode 6800: Total Loss of tensor([[5.2114]], grad_fn=<SubBackward0>)
[2022-11-05 09:52:13.400359] Process 1. Episode 6650, average_reward -0.069323
Episode 6650: Total Loss of tensor([[10.9432]], grad_fn=<SubBackward0>)
[2022-11-05 09:52:23.185440] Process 3. Episode 6700, average_reward -0.070746
Episode 6700: Total Loss of tensor([[12.7630]], grad_fn=<SubBackward0>)
[2022-11-05 09:53:43.824444] Process 5. Episode 6800, average_reward -0.072059
Episode 6800: Total Loss of tensor([[1.1364]], grad_fn=<SubBackward0>)
[2022-11-05 09:54:02.060020] Process 4. Episode 7150, average_reward -0.076783
Episode 7150: Total Loss of tensor([[10.3625]], grad_fn=<SubBackward0>)
[2022-11-05 09:54:02.992656] Process 0. Episode 6750, average_reward -0.072148
Episode 6750: Total Loss of tensor([[-14.1101]], grad_fn=<SubBackward0>)
[2022-11-05 09:54:20.080440] Process 2. Episode 6850, average_reward -0.076204
Episode 6850: Total Loss of tensor([[17.1765]], grad_fn=<SubBackward0>)
[2022-11-05 09:54:50.641982] Process 1. Episode 6700, average_reward -0.068955
Episode 6700: Total Loss of tensor([[-48.3817]], grad_fn=<SubBackward0>)
[2022-11-05 09:55:02.973017] Process 3. Episode 6750, average_reward -0.070963
Episode 6750: Total Loss of tensor([[9.6472]], grad_fn=<SubBackward0>)
[2022-11-05 09:56:13.298075] Process 5. Episode 6850, average_reward -0.071679
Episode 6850: Total Loss of tensor([[19.7774]], grad_fn=<SubBackward0>)
[2022-11-05 09:56:19.984335] Process 4. Episode 7200, average_reward -0.077222
Episode 7200: Total Loss of tensor([[-6.5771]], grad_fn=<SubBackward0>)
[2022-11-05 09:56:32.612024] Process 0. Episode 6800, average_reward -0.072059
Episode 6800: Total Loss of tensor([[1.5220]], grad_fn=<SubBackward0>)
[2022-11-05 09:56:49.697090] Process 2. Episode 6900, average_reward -0.076232
Episode 6900: Total Loss of tensor([[-7.5978]], grad_fn=<SubBackward0>)
[2022-11-05 09:57:28.169133] Process 1. Episode 6750, average_reward -0.069037
Episode 6750: Total Loss of tensor([[9.8178]], grad_fn=<SubBackward0>)
[2022-11-05 09:57:34.113376] Process 3. Episode 6800, average_reward -0.071324
Episode 6800: Total Loss of tensor([[6.7962]], grad_fn=<SubBackward0>)
[2022-11-05 09:58:39.994259] Process 5. Episode 6900, average_reward -0.072029
Episode 6900: Total Loss of tensor([[15.9159]], grad_fn=<SubBackward0>)
[2022-11-05 09:58:44.374601] Process 4. Episode 7250, average_reward -0.077655
Episode 7250: Total Loss of tensor([[5.0939]], grad_fn=<SubBackward0>)
[2022-11-05 09:59:08.588596] Process 0. Episode 6850, average_reward -0.071971
Episode 6850: Total Loss of tensor([[1.4417]], grad_fn=<SubBackward0>)
[2022-11-05 09:59:15.991213] Process 2. Episode 6950, average_reward -0.076115
Episode 6950: Total Loss of tensor([[7.2411]], grad_fn=<SubBackward0>)
[2022-11-05 10:00:04.985799] Process 3. Episode 6850, average_reward -0.070949
Episode 6850: Total Loss of tensor([[6.5939]], grad_fn=<SubBackward0>)
[2022-11-05 10:00:07.454456] Process 1. Episode 6800, average_reward -0.069118
Episode 6800: Total Loss of tensor([[21.4267]], grad_fn=<SubBackward0>)
[2022-11-05 10:01:06.663738] Process 5. Episode 6950, average_reward -0.071799
Episode 6950: Total Loss of tensor([[11.1837]], grad_fn=<SubBackward0>)
[2022-11-05 10:01:08.441936] Process 4. Episode 7300, average_reward -0.077260
Episode 7300: Total Loss of tensor([[6.5258]], grad_fn=<SubBackward0>)
[2022-11-05 10:01:39.546003] Process 2. Episode 7000, average_reward -0.076571
Episode 7000: Total Loss of tensor([[4.4359]], grad_fn=<SubBackward0>)
[2022-11-05 10:01:45.537761] Process 0. Episode 6900, average_reward -0.071884
Episode 6900: Total Loss of tensor([[4.4105]], grad_fn=<SubBackward0>)
[2022-11-05 10:02:32.168098] Process 3. Episode 6900, average_reward -0.071014
Episode 6900: Total Loss of tensor([[-4.0338]], grad_fn=<SubBackward0>)
[2022-11-05 10:02:51.863734] Process 1. Episode 6850, average_reward -0.068613
Episode 6850: Total Loss of tensor([[-2.8533]], grad_fn=<SubBackward0>)
[2022-11-05 10:03:31.795082] Process 4. Episode 7350, average_reward -0.076871
Episode 7350: Total Loss of tensor([[-124.6694]], grad_fn=<SubBackward0>)
[2022-11-05 10:03:41.824948] Process 5. Episode 7000, average_reward -0.072000
Episode 7000: Total Loss of tensor([[0.2952]], grad_fn=<SubBackward0>)
[2022-11-05 10:04:03.921566] Process 2. Episode 7050, average_reward -0.076596
Episode 7050: Total Loss of tensor([[-9.0179]], grad_fn=<SubBackward0>)
[2022-11-05 10:04:11.655834] Process 0. Episode 6950, average_reward -0.072086
Episode 6950: Total Loss of tensor([[28.4340]], grad_fn=<SubBackward0>)
[2022-11-05 10:05:16.385826] Process 3. Episode 6950, average_reward -0.070935
Episode 6950: Total Loss of tensor([[3.4433]], grad_fn=<SubBackward0>)
[2022-11-05 10:05:36.834656] Process 1. Episode 6900, average_reward -0.068841
Episode 6900: Total Loss of tensor([[15.8338]], grad_fn=<SubBackward0>)
[2022-11-05 10:05:54.466832] Process 4. Episode 7400, average_reward -0.076486
Episode 7400: Total Loss of tensor([[-0.3522]], grad_fn=<SubBackward0>)
[2022-11-05 10:06:18.717765] Process 5. Episode 7050, average_reward -0.072199
Episode 7050: Total Loss of tensor([[-109.8049]], grad_fn=<SubBackward0>)
[2022-11-05 10:06:33.249412] Process 0. Episode 7000, average_reward -0.071857
Episode 7000: Total Loss of tensor([[-1.4243]], grad_fn=<SubBackward0>)
[2022-11-05 10:06:35.739679] Process 2. Episode 7100, average_reward -0.076479
Episode 7100: Total Loss of tensor([[16.2806]], grad_fn=<SubBackward0>)
[2022-11-05 10:07:54.836379] Process 3. Episode 7000, average_reward -0.071571
Episode 7000: Total Loss of tensor([[-9.9554]], grad_fn=<SubBackward0>)
[2022-11-05 10:08:21.343263] Process 1. Episode 6950, average_reward -0.068777
Episode 6950: Total Loss of tensor([[8.6126]], grad_fn=<SubBackward0>)
[2022-11-05 10:08:22.773186] Process 4. Episode 7450, average_reward -0.077181
Episode 7450: Total Loss of tensor([[11.7425]], grad_fn=<SubBackward0>)
[2022-11-05 10:08:45.661866] Process 5. Episode 7100, average_reward -0.072113
Episode 7100: Total Loss of tensor([[19.9256]], grad_fn=<SubBackward0>)
[2022-11-05 10:08:55.040970] Process 0. Episode 7050, average_reward -0.071915
Episode 7050: Total Loss of tensor([[5.5960]], grad_fn=<SubBackward0>)
[2022-11-05 10:09:11.465944] Process 2. Episode 7150, average_reward -0.076503
Episode 7150: Total Loss of tensor([[-5.4068]], grad_fn=<SubBackward0>)
[2022-11-05 10:10:22.402002] Process 3. Episode 7050, average_reward -0.071348
Episode 7050: Total Loss of tensor([[18.4499]], grad_fn=<SubBackward0>)
[2022-11-05 10:10:45.229549] Process 4. Episode 7500, average_reward -0.077067
Episode 7500: Total Loss of tensor([[14.4789]], grad_fn=<SubBackward0>)
[2022-11-05 10:11:01.366887] Process 1. Episode 7000, average_reward -0.069000
Episode 7000: Total Loss of tensor([[-1.8376]], grad_fn=<SubBackward0>)
[2022-11-05 10:11:22.013567] Process 0. Episode 7100, average_reward -0.071972
Episode 7100: Total Loss of tensor([[-9.7998]], grad_fn=<SubBackward0>)
[2022-11-05 10:11:22.105594] Process 5. Episode 7150, average_reward -0.072028
Episode 7150: Total Loss of tensor([[-20.0599]], grad_fn=<SubBackward0>)
[2022-11-05 10:11:47.638188] Process 2. Episode 7200, average_reward -0.077083
Episode 7200: Total Loss of tensor([[7.0044]], grad_fn=<SubBackward0>)
[2022-11-05 10:12:54.238720] Process 3. Episode 7100, average_reward -0.071268
Episode 7100: Total Loss of tensor([[0.8366]], grad_fn=<SubBackward0>)
[2022-11-05 10:13:09.247994] Process 4. Episode 7550, average_reward -0.077351
Episode 7550: Total Loss of tensor([[-8.8148]], grad_fn=<SubBackward0>)
[2022-11-05 10:13:31.216792] Process 1. Episode 7050, average_reward -0.069078
Episode 7050: Total Loss of tensor([[1.5822]], grad_fn=<SubBackward0>)
[2022-11-05 10:13:48.408846] Process 5. Episode 7200, average_reward -0.072222
Episode 7200: Total Loss of tensor([[1.1962]], grad_fn=<SubBackward0>)
[2022-11-05 10:13:53.305555] Process 0. Episode 7150, average_reward -0.071748
Episode 7150: Total Loss of tensor([[-7.8695]], grad_fn=<SubBackward0>)
[2022-11-05 10:14:27.781409] Process 2. Episode 7250, average_reward -0.076828
Episode 7250: Total Loss of tensor([[12.7455]], grad_fn=<SubBackward0>)
[2022-11-05 10:15:27.217478] Process 3. Episode 7150, average_reward -0.071189
Episode 7150: Total Loss of tensor([[-10.3394]], grad_fn=<SubBackward0>)
[2022-11-05 10:15:35.522609] Process 4. Episode 7600, average_reward -0.077237
Episode 7600: Total Loss of tensor([[-3.2504]], grad_fn=<SubBackward0>)
[2022-11-05 10:16:11.060579] Process 1. Episode 7100, average_reward -0.069155
Episode 7100: Total Loss of tensor([[14.0868]], grad_fn=<SubBackward0>)
[2022-11-05 10:16:21.560846] Process 5. Episode 7250, average_reward -0.072000
Episode 7250: Total Loss of tensor([[3.6755]], grad_fn=<SubBackward0>)
[2022-11-05 10:16:23.694668] Process 0. Episode 7200, average_reward -0.071806
Episode 7200: Total Loss of tensor([[8.9794]], grad_fn=<SubBackward0>)
[2022-11-05 10:16:55.645421] Process 2. Episode 7300, average_reward -0.076986
Episode 7300: Total Loss of tensor([[9.7980]], grad_fn=<SubBackward0>)
[2022-11-05 10:17:53.224016] Process 3. Episode 7200, average_reward -0.071250
Episode 7200: Total Loss of tensor([[10.3758]], grad_fn=<SubBackward0>)
[2022-11-05 10:18:00.653456] Process 4. Episode 7650, average_reward -0.077124
Episode 7650: Total Loss of tensor([[6.6015]], grad_fn=<SubBackward0>)
[2022-11-05 10:18:45.822097] Process 1. Episode 7150, average_reward -0.069091
Episode 7150: Total Loss of tensor([[2.7305]], grad_fn=<SubBackward0>)
[2022-11-05 10:18:55.126834] Process 5. Episode 7300, average_reward -0.071918
Episode 7300: Total Loss of tensor([[-1.8356]], grad_fn=<SubBackward0>)
[2022-11-05 10:19:02.475582] Process 0. Episode 7250, average_reward -0.072414
Episode 7250: Total Loss of tensor([[-26.7616]], grad_fn=<SubBackward0>)
[2022-11-05 10:19:15.993732] Process 2. Episode 7350, average_reward -0.077279
Episode 7350: Total Loss of tensor([[20.5934]], grad_fn=<SubBackward0>)
[2022-11-05 10:20:22.925772] Process 4. Episode 7700, average_reward -0.077273
Episode 7700: Total Loss of tensor([[-17.5812]], grad_fn=<SubBackward0>)
[2022-11-05 10:20:36.968875] Process 3. Episode 7250, average_reward -0.071034
Episode 7250: Total Loss of tensor([[-3.6336]], grad_fn=<SubBackward0>)
[2022-11-05 10:21:25.915039] Process 5. Episode 7350, average_reward -0.071837
Episode 7350: Total Loss of tensor([[-3.5190]], grad_fn=<SubBackward0>)
[2022-11-05 10:21:29.745145] Process 1. Episode 7200, average_reward -0.069028
Episode 7200: Total Loss of tensor([[8.6342]], grad_fn=<SubBackward0>)
[2022-11-05 10:21:37.993937] Process 0. Episode 7300, average_reward -0.072055
Episode 7300: Total Loss of tensor([[13.8682]], grad_fn=<SubBackward0>)
[2022-11-05 10:21:39.708178] Process 2. Episode 7400, average_reward -0.077027
Episode 7400: Total Loss of tensor([[2.0334]], grad_fn=<SubBackward0>)
[2022-11-05 10:22:44.916681] Process 4. Episode 7750, average_reward -0.077419
Episode 7750: Total Loss of tensor([[5.0322]], grad_fn=<SubBackward0>)
[2022-11-05 10:23:08.176421] Process 3. Episode 7300, average_reward -0.070685
Episode 7300: Total Loss of tensor([[11.9513]], grad_fn=<SubBackward0>)
[2022-11-05 10:23:58.596968] Process 5. Episode 7400, average_reward -0.071892
Episode 7400: Total Loss of tensor([[11.8535]], grad_fn=<SubBackward0>)
[2022-11-05 10:24:03.504421] Process 1. Episode 7250, average_reward -0.069103
Episode 7250: Total Loss of tensor([[3.5455]], grad_fn=<SubBackward0>)
[2022-11-05 10:24:03.617290] Process 2. Episode 7450, average_reward -0.076779
Episode 7450: Total Loss of tensor([[15.0609]], grad_fn=<SubBackward0>)
[2022-11-05 10:24:12.540705] Process 0. Episode 7350, average_reward -0.072381
Episode 7350: Total Loss of tensor([[15.5124]], grad_fn=<SubBackward0>)
[2022-11-05 10:25:05.100803] Process 4. Episode 7800, average_reward -0.077308
Episode 7800: Total Loss of tensor([[12.8817]], grad_fn=<SubBackward0>)
[2022-11-05 10:25:36.822913] Process 3. Episode 7350, average_reward -0.070884
Episode 7350: Total Loss of tensor([[-11.0230]], grad_fn=<SubBackward0>)
[2022-11-05 10:26:26.436118] Process 2. Episode 7500, average_reward -0.076667
Episode 7500: Total Loss of tensor([[16.0747]], grad_fn=<SubBackward0>)
[2022-11-05 10:26:29.082084] Process 5. Episode 7450, average_reward -0.071946
Episode 7450: Total Loss of tensor([[3.8441]], grad_fn=<SubBackward0>)
[2022-11-05 10:26:39.205885] Process 1. Episode 7300, average_reward -0.069178
Episode 7300: Total Loss of tensor([[4.1878]], grad_fn=<SubBackward0>)
[2022-11-05 10:26:48.087454] Process 0. Episode 7400, average_reward -0.072297
Episode 7400: Total Loss of tensor([[-6.3423]], grad_fn=<SubBackward0>)
[2022-11-05 10:27:27.259983] Process 4. Episode 7850, average_reward -0.077325
Episode 7850: Total Loss of tensor([[21.1213]], grad_fn=<SubBackward0>)
[2022-11-05 10:28:00.211292] Process 3. Episode 7400, average_reward -0.070811
Episode 7400: Total Loss of tensor([[13.9287]], grad_fn=<SubBackward0>)
[2022-11-05 10:28:49.505872] Process 2. Episode 7550, average_reward -0.076821
Episode 7550: Total Loss of tensor([[-115.7991]], grad_fn=<SubBackward0>)
[2022-11-05 10:28:58.532229] Process 5. Episode 7500, average_reward -0.072400
Episode 7500: Total Loss of tensor([[8.0246]], grad_fn=<SubBackward0>)
[2022-11-05 10:29:19.961304] Process 0. Episode 7450, average_reward -0.072617
Episode 7450: Total Loss of tensor([[45.3910]], grad_fn=<SubBackward0>)
[2022-11-05 10:29:23.248463] Process 1. Episode 7350, average_reward -0.069388
Episode 7350: Total Loss of tensor([[-4.1652]], grad_fn=<SubBackward0>)
[2022-11-05 10:29:51.653606] Process 4. Episode 7900, average_reward -0.077215
Episode 7900: Total Loss of tensor([[11.3906]], grad_fn=<SubBackward0>)
[2022-11-05 10:30:21.770531] Process 3. Episode 7450, average_reward -0.071007
Episode 7450: Total Loss of tensor([[9.4280]], grad_fn=<SubBackward0>)
[2022-11-05 10:31:12.309472] Process 2. Episode 7600, average_reward -0.076842
Episode 7600: Total Loss of tensor([[5.4929]], grad_fn=<SubBackward0>)
[2022-11-05 10:31:31.147024] Process 5. Episode 7550, average_reward -0.072318
Episode 7550: Total Loss of tensor([[0.6113]], grad_fn=<SubBackward0>)
[2022-11-05 10:31:59.787425] Process 0. Episode 7500, average_reward -0.072533
Episode 7500: Total Loss of tensor([[-1.5050]], grad_fn=<SubBackward0>)
[2022-11-05 10:32:02.026380] Process 1. Episode 7400, average_reward -0.069459
Episode 7400: Total Loss of tensor([[2.5285]], grad_fn=<SubBackward0>)
[2022-11-05 10:32:13.890934] Process 4. Episode 7950, average_reward -0.077233
Episode 7950: Total Loss of tensor([[13.8728]], grad_fn=<SubBackward0>)
[2022-11-05 10:32:47.055701] Process 3. Episode 7500, average_reward -0.071200
Episode 7500: Total Loss of tensor([[5.5580]], grad_fn=<SubBackward0>)
[2022-11-05 10:33:38.756341] Process 2. Episode 7650, average_reward -0.076863
Episode 7650: Total Loss of tensor([[-5.7207]], grad_fn=<SubBackward0>)
[2022-11-05 10:33:59.664783] Process 5. Episode 7600, average_reward -0.072237
Episode 7600: Total Loss of tensor([[20.7542]], grad_fn=<SubBackward0>)
[2022-11-05 10:34:33.131180] Process 0. Episode 7550, average_reward -0.072450
Episode 7550: Total Loss of tensor([[5.2216]], grad_fn=<SubBackward0>)
[2022-11-05 10:34:34.485436] Process 4. Episode 8000, average_reward -0.077000
Episode 8000: Total Loss of tensor([[11.2402]], grad_fn=<SubBackward0>)
[2022-11-05 10:34:44.374046] Process 1. Episode 7450, average_reward -0.069128
Episode 7450: Total Loss of tensor([[6.7766]], grad_fn=<SubBackward0>)
[2022-11-05 10:35:22.060904] Process 3. Episode 7550, average_reward -0.071391
Episode 7550: Total Loss of tensor([[2.6010]], grad_fn=<SubBackward0>)
[2022-11-05 10:36:04.164409] Process 2. Episode 7700, average_reward -0.077403
Episode 7700: Total Loss of tensor([[-111.0440]], grad_fn=<SubBackward0>)
[2022-11-05 10:36:21.958852] Process 5. Episode 7650, average_reward -0.072288
Episode 7650: Total Loss of tensor([[2.0014]], grad_fn=<SubBackward0>)
[2022-11-05 10:36:54.034574] Process 4. Episode 8050, average_reward -0.077019
Episode 8050: Total Loss of tensor([[6.2313]], grad_fn=<SubBackward0>)
[2022-11-05 10:37:09.049637] Process 0. Episode 7600, average_reward -0.072500
Episode 7600: Total Loss of tensor([[13.0414]], grad_fn=<SubBackward0>)
[2022-11-05 10:37:31.685723] Process 1. Episode 7500, average_reward -0.069467
Episode 7500: Total Loss of tensor([[12.0567]], grad_fn=<SubBackward0>)
[2022-11-05 10:37:53.809947] Process 3. Episode 7600, average_reward -0.071053
Episode 7600: Total Loss of tensor([[7.4250]], grad_fn=<SubBackward0>)
[2022-11-05 10:38:30.935028] Process 2. Episode 7750, average_reward -0.077161
Episode 7750: Total Loss of tensor([[-111.7209]], grad_fn=<SubBackward0>)
[2022-11-05 10:38:54.771282] Process 5. Episode 7700, average_reward -0.072468
Episode 7700: Total Loss of tensor([[1.2417]], grad_fn=<SubBackward0>)
[2022-11-05 10:39:14.556345] Process 4. Episode 8100, average_reward -0.077037
Episode 8100: Total Loss of tensor([[9.9838]], grad_fn=<SubBackward0>)
[2022-11-05 10:39:39.702958] Process 0. Episode 7650, average_reward -0.072288
Episode 7650: Total Loss of tensor([[11.4990]], grad_fn=<SubBackward0>)
[2022-11-05 10:40:09.450200] Process 1. Episode 7550, average_reward -0.069536
Episode 7550: Total Loss of tensor([[-2.0529]], grad_fn=<SubBackward0>)
[2022-11-05 10:40:25.647438] Process 3. Episode 7650, average_reward -0.071503
Episode 7650: Total Loss of tensor([[10.4250]], grad_fn=<SubBackward0>)
[2022-11-05 10:41:02.096736] Process 2. Episode 7800, average_reward -0.076923
Episode 7800: Total Loss of tensor([[17.8884]], grad_fn=<SubBackward0>)
[2022-11-05 10:41:19.879322] Process 5. Episode 7750, average_reward -0.072258
Episode 7750: Total Loss of tensor([[13.3299]], grad_fn=<SubBackward0>)
[2022-11-05 10:41:38.290973] Process 4. Episode 8150, average_reward -0.076933
Episode 8150: Total Loss of tensor([[-18.6912]], grad_fn=<SubBackward0>)
[2022-11-05 10:42:05.026885] Process 0. Episode 7700, average_reward -0.072338
Episode 7700: Total Loss of tensor([[-12.3452]], grad_fn=<SubBackward0>)
[2022-11-05 10:42:48.023913] Process 1. Episode 7600, average_reward -0.069737
Episode 7600: Total Loss of tensor([[8.3431]], grad_fn=<SubBackward0>)
[2022-11-05 10:43:11.649242] Process 3. Episode 7700, average_reward -0.071429
Episode 7700: Total Loss of tensor([[7.9985]], grad_fn=<SubBackward0>)
[2022-11-05 10:43:27.867222] Process 2. Episode 7850, average_reward -0.076943
Episode 7850: Total Loss of tensor([[-4.2257]], grad_fn=<SubBackward0>)
[2022-11-05 10:43:49.117621] Process 5. Episode 7800, average_reward -0.071795
Episode 7800: Total Loss of tensor([[7.3414]], grad_fn=<SubBackward0>)
[2022-11-05 10:44:00.537657] Process 4. Episode 8200, average_reward -0.076707
Episode 8200: Total Loss of tensor([[-4.3186]], grad_fn=<SubBackward0>)
[2022-11-05 10:44:28.858126] Process 0. Episode 7750, average_reward -0.072129
Episode 7750: Total Loss of tensor([[0.6088]], grad_fn=<SubBackward0>)
[2022-11-05 10:45:25.867018] Process 1. Episode 7650, average_reward -0.069673
Episode 7650: Total Loss of tensor([[13.1151]], grad_fn=<SubBackward0>)
[2022-11-05 10:45:47.091619] Process 3. Episode 7750, average_reward -0.072000
Episode 7750: Total Loss of tensor([[-65.1351]], grad_fn=<SubBackward0>)
[2022-11-05 10:45:56.629456] Process 2. Episode 7900, average_reward -0.076835
Episode 7900: Total Loss of tensor([[17.6894]], grad_fn=<SubBackward0>)
[2022-11-05 10:46:19.900652] Process 5. Episode 7850, average_reward -0.071592
Episode 7850: Total Loss of tensor([[12.6071]], grad_fn=<SubBackward0>)
[2022-11-05 10:46:24.930246] Process 4. Episode 8250, average_reward -0.077212
Episode 8250: Total Loss of tensor([[-120.7065]], grad_fn=<SubBackward0>)
[2022-11-05 10:46:52.558717] Process 0. Episode 7800, average_reward -0.071923
Episode 7800: Total Loss of tensor([[5.0476]], grad_fn=<SubBackward0>)
[2022-11-05 10:48:01.534181] Process 1. Episode 7700, average_reward -0.070000
Episode 7700: Total Loss of tensor([[4.4334]], grad_fn=<SubBackward0>)
[2022-11-05 10:48:34.947068] Process 3. Episode 7800, average_reward -0.072179
Episode 7800: Total Loss of tensor([[3.9556]], grad_fn=<SubBackward0>)
[2022-11-05 10:48:37.009224] Process 2. Episode 7950, average_reward -0.077107
Episode 7950: Total Loss of tensor([[1.0986]], grad_fn=<SubBackward0>)
[2022-11-05 10:48:50.662476] Process 5. Episode 7900, average_reward -0.071392
Episode 7900: Total Loss of tensor([[18.9158]], grad_fn=<SubBackward0>)
[2022-11-05 10:48:53.301007] Process 4. Episode 8300, average_reward -0.076988
Episode 8300: Total Loss of tensor([[18.3335]], grad_fn=<SubBackward0>)
[2022-11-05 10:49:19.662910] Process 0. Episode 7850, average_reward -0.072229
Episode 7850: Total Loss of tensor([[-3.3652]], grad_fn=<SubBackward0>)
[2022-11-05 10:50:35.682181] Process 1. Episode 7750, average_reward -0.069935
Episode 7750: Total Loss of tensor([[25.2668]], grad_fn=<SubBackward0>)
[2022-11-05 10:51:10.022272] Process 2. Episode 8000, average_reward -0.077250
Episode 8000: Total Loss of tensor([[-3.4263]], grad_fn=<SubBackward0>)
[2022-11-05 10:51:18.047127] Process 4. Episode 8350, average_reward -0.077246
Episode 8350: Total Loss of tensor([[2.4260]], grad_fn=<SubBackward0>)
[2022-11-05 10:51:23.326074] Process 3. Episode 7850, average_reward -0.071847
Episode 7850: Total Loss of tensor([[13.4970]], grad_fn=<SubBackward0>)
[2022-11-05 10:51:25.296431] Process 5. Episode 7950, average_reward -0.071447
Episode 7950: Total Loss of tensor([[-1.9848]], grad_fn=<SubBackward0>)
[2022-11-05 10:51:52.488056] Process 0. Episode 7900, average_reward -0.072278
Episode 7900: Total Loss of tensor([[10.6803]], grad_fn=<SubBackward0>)
[2022-11-05 10:53:03.756552] Process 1. Episode 7800, average_reward -0.069744
Episode 7800: Total Loss of tensor([[12.2971]], grad_fn=<SubBackward0>)
[2022-11-05 10:53:39.395053] Process 2. Episode 8050, average_reward -0.077764
Episode 8050: Total Loss of tensor([[8.1932]], grad_fn=<SubBackward0>)
[2022-11-05 10:53:44.793894] Process 4. Episode 8400, average_reward -0.077143
Episode 8400: Total Loss of tensor([[1.0945]], grad_fn=<SubBackward0>)
[2022-11-05 10:53:58.220423] Process 5. Episode 8000, average_reward -0.071375
Episode 8000: Total Loss of tensor([[3.8212]], grad_fn=<SubBackward0>)
[2022-11-05 10:53:58.963791] Process 3. Episode 7900, average_reward -0.071899
Episode 7900: Total Loss of tensor([[9.3002]], grad_fn=<SubBackward0>)
[2022-11-05 10:54:20.599909] Process 0. Episode 7950, average_reward -0.072075
Episode 7950: Total Loss of tensor([[14.6126]], grad_fn=<SubBackward0>)
[2022-11-05 10:55:42.902697] Process 1. Episode 7850, average_reward -0.069936
Episode 7850: Total Loss of tensor([[-4.2585]], grad_fn=<SubBackward0>)
[2022-11-05 10:56:09.613000] Process 2. Episode 8100, average_reward -0.077654
Episode 8100: Total Loss of tensor([[19.1937]], grad_fn=<SubBackward0>)
[2022-11-05 10:56:12.072993] Process 4. Episode 8450, average_reward -0.076923
Episode 8450: Total Loss of tensor([[12.3272]], grad_fn=<SubBackward0>)
[2022-11-05 10:56:27.778888] Process 5. Episode 8050, average_reward -0.071429
Episode 8050: Total Loss of tensor([[-108.9997]], grad_fn=<SubBackward0>)
[2022-11-05 10:56:38.039522] Process 3. Episode 7950, average_reward -0.072075
Episode 7950: Total Loss of tensor([[8.6839]], grad_fn=<SubBackward0>)
[2022-11-05 10:56:47.768584] Process 0. Episode 8000, average_reward -0.072375
Episode 8000: Total Loss of tensor([[2.9066]], grad_fn=<SubBackward0>)
[2022-11-05 10:58:27.645132] Process 1. Episode 7900, average_reward -0.069873
Episode 7900: Total Loss of tensor([[-0.7291]], grad_fn=<SubBackward0>)
[2022-11-05 10:58:38.457140] Process 4. Episode 8500, average_reward -0.076706
Episode 8500: Total Loss of tensor([[1.2865]], grad_fn=<SubBackward0>)
[2022-11-05 10:58:43.236474] Process 2. Episode 8150, average_reward -0.077301
Episode 8150: Total Loss of tensor([[0.7852]], grad_fn=<SubBackward0>)
[2022-11-05 10:58:53.966785] Process 5. Episode 8100, average_reward -0.071111
Episode 8100: Total Loss of tensor([[-4.1563]], grad_fn=<SubBackward0>)
[2022-11-05 10:59:17.809086] Process 3. Episode 8000, average_reward -0.072000
Episode 8000: Total Loss of tensor([[4.4943]], grad_fn=<SubBackward0>)
[2022-11-05 10:59:24.750786] Process 0. Episode 8050, average_reward -0.072422
Episode 8050: Total Loss of tensor([[-4.3349]], grad_fn=<SubBackward0>)
[2022-11-05 11:00:58.135282] Process 1. Episode 7950, average_reward -0.069560
Episode 7950: Total Loss of tensor([[2.3931]], grad_fn=<SubBackward0>)
[2022-11-05 11:01:04.690862] Process 4. Episode 8550, average_reward -0.076257
Episode 8550: Total Loss of tensor([[-0.3137]], grad_fn=<SubBackward0>)
[2022-11-05 11:01:18.288256] Process 5. Episode 8150, average_reward -0.071166
Episode 8150: Total Loss of tensor([[1.1323]], grad_fn=<SubBackward0>)
[2022-11-05 11:01:22.437072] Process 2. Episode 8200, average_reward -0.077439
Episode 8200: Total Loss of tensor([[4.7965]], grad_fn=<SubBackward0>)
[2022-11-05 11:01:59.377180] Process 3. Episode 8050, average_reward -0.071801
Episode 8050: Total Loss of tensor([[-125.5238]], grad_fn=<SubBackward0>)
[2022-11-05 11:01:59.762754] Process 0. Episode 8100, average_reward -0.072469
Episode 8100: Total Loss of tensor([[10.2013]], grad_fn=<SubBackward0>)
[2022-11-05 11:03:26.788990] Process 4. Episode 8600, average_reward -0.076163
Episode 8600: Total Loss of tensor([[20.4296]], grad_fn=<SubBackward0>)
[2022-11-05 11:03:32.661668] Process 1. Episode 8000, average_reward -0.069750
Episode 8000: Total Loss of tensor([[0.6703]], grad_fn=<SubBackward0>)
[2022-11-05 11:03:42.824392] Process 5. Episode 8200, average_reward -0.071341
Episode 8200: Total Loss of tensor([[-95.4607]], grad_fn=<SubBackward0>)
[2022-11-05 11:03:52.185994] Process 2. Episode 8250, average_reward -0.077939
Episode 8250: Total Loss of tensor([[9.0593]], grad_fn=<SubBackward0>)
[2022-11-05 11:04:30.614459] Process 0. Episode 8150, average_reward -0.072638
Episode 8150: Total Loss of tensor([[14.4626]], grad_fn=<SubBackward0>)
[2022-11-05 11:04:30.737822] Process 3. Episode 8100, average_reward -0.071605
Episode 8100: Total Loss of tensor([[12.1576]], grad_fn=<SubBackward0>)
[2022-11-05 11:05:54.510537] Process 4. Episode 8650, average_reward -0.076069
Episode 8650: Total Loss of tensor([[10.2386]], grad_fn=<SubBackward0>)
[2022-11-05 11:06:04.538418] Process 1. Episode 8050, average_reward -0.069814
Episode 8050: Total Loss of tensor([[18.4465]], grad_fn=<SubBackward0>)
[2022-11-05 11:06:14.482397] Process 5. Episode 8250, average_reward -0.071515
Episode 8250: Total Loss of tensor([[10.0339]], grad_fn=<SubBackward0>)
[2022-11-05 11:06:21.226020] Process 2. Episode 8300, average_reward -0.077831
Episode 8300: Total Loss of tensor([[-8.7150]], grad_fn=<SubBackward0>)
[2022-11-05 11:07:06.928461] Process 3. Episode 8150, average_reward -0.071779
Episode 8150: Total Loss of tensor([[8.4604]], grad_fn=<SubBackward0>)
[2022-11-05 11:07:07.131897] Process 0. Episode 8200, average_reward -0.072439
Episode 8200: Total Loss of tensor([[11.2045]], grad_fn=<SubBackward0>)
[2022-11-05 11:08:22.229050] Process 4. Episode 8700, average_reward -0.075977
Episode 8700: Total Loss of tensor([[12.0410]], grad_fn=<SubBackward0>)
[2022-11-05 11:08:28.136341] Process 1. Episode 8100, average_reward -0.069753
Episode 8100: Total Loss of tensor([[21.1622]], grad_fn=<SubBackward0>)
[2022-11-05 11:08:41.327374] Process 5. Episode 8300, average_reward -0.071446
Episode 8300: Total Loss of tensor([[-1.0755]], grad_fn=<SubBackward0>)
[2022-11-05 11:08:50.681636] Process 2. Episode 8350, average_reward -0.077844
Episode 8350: Total Loss of tensor([[-25.1287]], grad_fn=<SubBackward0>)
[2022-11-05 11:09:41.899257] Process 3. Episode 8200, average_reward -0.072073
Episode 8200: Total Loss of tensor([[4.2418]], grad_fn=<SubBackward0>)
[2022-11-05 11:09:47.173840] Process 0. Episode 8250, average_reward -0.072364
Episode 8250: Total Loss of tensor([[2.9860]], grad_fn=<SubBackward0>)
[2022-11-05 11:10:46.133249] Process 4. Episode 8750, average_reward -0.076343
Episode 8750: Total Loss of tensor([[10.2024]], grad_fn=<SubBackward0>)
[2022-11-05 11:11:03.700493] Process 1. Episode 8150, average_reward -0.069571
Episode 8150: Total Loss of tensor([[15.4272]], grad_fn=<SubBackward0>)
[2022-11-05 11:11:11.154492] Process 5. Episode 8350, average_reward -0.071617
Episode 8350: Total Loss of tensor([[3.7301]], grad_fn=<SubBackward0>)
[2022-11-05 11:11:13.429186] Process 2. Episode 8400, average_reward -0.077976
Episode 8400: Total Loss of tensor([[14.6753]], grad_fn=<SubBackward0>)
[2022-11-05 11:12:17.721731] Process 0. Episode 8300, average_reward -0.072651
Episode 8300: Total Loss of tensor([[17.2485]], grad_fn=<SubBackward0>)
[2022-11-05 11:12:22.489027] Process 3. Episode 8250, average_reward -0.072000
Episode 8250: Total Loss of tensor([[16.5929]], grad_fn=<SubBackward0>)
[2022-11-05 11:13:11.567855] Process 4. Episode 8800, average_reward -0.076705
Episode 8800: Total Loss of tensor([[9.0340]], grad_fn=<SubBackward0>)
[2022-11-05 11:13:42.064663] Process 2. Episode 8450, average_reward -0.077988
Episode 8450: Total Loss of tensor([[10.0400]], grad_fn=<SubBackward0>)
[2022-11-05 11:13:42.747378] Process 5. Episode 8400, average_reward -0.071548
Episode 8400: Total Loss of tensor([[11.6865]], grad_fn=<SubBackward0>)
[2022-11-05 11:13:45.515196] Process 1. Episode 8200, average_reward -0.069634
Episode 8200: Total Loss of tensor([[-2.6626]], grad_fn=<SubBackward0>)
[2022-11-05 11:14:50.637839] Process 0. Episode 8350, average_reward -0.072695
Episode 8350: Total Loss of tensor([[-1.3810]], grad_fn=<SubBackward0>)
[2022-11-05 11:15:00.268625] Process 3. Episode 8300, average_reward -0.072289
Episode 8300: Total Loss of tensor([[14.0970]], grad_fn=<SubBackward0>)
[2022-11-05 11:15:38.901459] Process 4. Episode 8850, average_reward -0.076836
Episode 8850: Total Loss of tensor([[-3.4660]], grad_fn=<SubBackward0>)
[2022-11-05 11:16:05.996059] Process 2. Episode 8500, average_reward -0.078000
Episode 8500: Total Loss of tensor([[-6.4648]], grad_fn=<SubBackward0>)
[2022-11-05 11:16:07.574618] Process 5. Episode 8450, average_reward -0.071598
Episode 8450: Total Loss of tensor([[1.6487]], grad_fn=<SubBackward0>)
[2022-11-05 11:16:19.359781] Process 1. Episode 8250, average_reward -0.069576
Episode 8250: Total Loss of tensor([[11.3438]], grad_fn=<SubBackward0>)
[2022-11-05 11:17:17.067954] Process 0. Episode 8400, average_reward -0.072619
Episode 8400: Total Loss of tensor([[14.8314]], grad_fn=<SubBackward0>)
[2022-11-05 11:17:28.569730] Process 3. Episode 8350, average_reward -0.072575
Episode 8350: Total Loss of tensor([[0.4647]], grad_fn=<SubBackward0>)
[2022-11-05 11:18:03.065044] Process 4. Episode 8900, average_reward -0.076854
Episode 8900: Total Loss of tensor([[32.8690]], grad_fn=<SubBackward0>)
[2022-11-05 11:18:33.708217] Process 5. Episode 8500, average_reward -0.071647
Episode 8500: Total Loss of tensor([[9.7576]], grad_fn=<SubBackward0>)
[2022-11-05 11:18:36.622189] Process 2. Episode 8550, average_reward -0.078480
Episode 8550: Total Loss of tensor([[7.3009]], grad_fn=<SubBackward0>)
[2022-11-05 11:19:07.391223] Process 1. Episode 8300, average_reward -0.069277
Episode 8300: Total Loss of tensor([[-118.5286]], grad_fn=<SubBackward0>)
[2022-11-05 11:19:49.415820] Process 0. Episode 8450, average_reward -0.072781
Episode 8450: Total Loss of tensor([[8.8602]], grad_fn=<SubBackward0>)
[2022-11-05 11:20:04.401770] Process 3. Episode 8400, average_reward -0.072500
Episode 8400: Total Loss of tensor([[6.0851]], grad_fn=<SubBackward0>)
[2022-11-05 11:20:27.756973] Process 4. Episode 8950, average_reward -0.076983
Episode 8950: Total Loss of tensor([[-1.2916]], grad_fn=<SubBackward0>)
[2022-11-05 11:21:06.404286] Process 5. Episode 8550, average_reward -0.071579
Episode 8550: Total Loss of tensor([[12.3975]], grad_fn=<SubBackward0>)
[2022-11-05 11:21:08.062735] Process 2. Episode 8600, average_reward -0.078488
Episode 8600: Total Loss of tensor([[17.4513]], grad_fn=<SubBackward0>)
[2022-11-05 11:21:40.490714] Process 1. Episode 8350, average_reward -0.069102
Episode 8350: Total Loss of tensor([[-123.1772]], grad_fn=<SubBackward0>)
[2022-11-05 11:22:28.074937] Process 0. Episode 8500, average_reward -0.072941
Episode 8500: Total Loss of tensor([[16.0023]], grad_fn=<SubBackward0>)
[2022-11-05 11:22:43.579939] Process 3. Episode 8450, average_reward -0.072426
Episode 8450: Total Loss of tensor([[6.9907]], grad_fn=<SubBackward0>)
[2022-11-05 11:22:52.750751] Process 4. Episode 9000, average_reward -0.076889
Episode 9000: Total Loss of tensor([[1.9346]], grad_fn=<SubBackward0>)
[2022-11-05 11:23:30.639133] Process 5. Episode 8600, average_reward -0.071395
Episode 8600: Total Loss of tensor([[3.8480]], grad_fn=<SubBackward0>)
[2022-11-05 11:23:38.712619] Process 2. Episode 8650, average_reward -0.078266
Episode 8650: Total Loss of tensor([[-1.5554]], grad_fn=<SubBackward0>)
[2022-11-05 11:24:22.497954] Process 1. Episode 8400, average_reward -0.068690
Episode 8400: Total Loss of tensor([[1.1725]], grad_fn=<SubBackward0>)
[2022-11-05 11:24:53.369510] Process 0. Episode 8550, average_reward -0.072982
Episode 8550: Total Loss of tensor([[-115.5202]], grad_fn=<SubBackward0>)
[2022-11-05 11:25:18.026642] Process 4. Episode 9050, average_reward -0.077017
Episode 9050: Total Loss of tensor([[21.5033]], grad_fn=<SubBackward0>)
[2022-11-05 11:25:20.157005] Process 3. Episode 8500, average_reward -0.072824
Episode 8500: Total Loss of tensor([[10.6319]], grad_fn=<SubBackward0>)
[2022-11-05 11:25:58.488155] Process 5. Episode 8650, average_reward -0.071098
Episode 8650: Total Loss of tensor([[10.0253]], grad_fn=<SubBackward0>)
[2022-11-05 11:26:09.596457] Process 2. Episode 8700, average_reward -0.078736
Episode 8700: Total Loss of tensor([[21.4519]], grad_fn=<SubBackward0>)
[2022-11-05 11:27:11.815591] Process 1. Episode 8450, average_reward -0.068402
Episode 8450: Total Loss of tensor([[31.0006]], grad_fn=<SubBackward0>)
[2022-11-05 11:27:28.044150] Process 0. Episode 8600, average_reward -0.073140
Episode 8600: Total Loss of tensor([[17.7022]], grad_fn=<SubBackward0>)
[2022-11-05 11:27:41.567812] Process 4. Episode 9100, average_reward -0.077033
Episode 9100: Total Loss of tensor([[18.9291]], grad_fn=<SubBackward0>)
[2022-11-05 11:27:52.880342] Process 3. Episode 8550, average_reward -0.072982
Episode 8550: Total Loss of tensor([[21.5218]], grad_fn=<SubBackward0>)
[2022-11-05 11:28:26.788146] Process 5. Episode 8700, average_reward -0.071494
Episode 8700: Total Loss of tensor([[9.4756]], grad_fn=<SubBackward0>)
[2022-11-05 11:28:41.603794] Process 2. Episode 8750, average_reward -0.078743
Episode 8750: Total Loss of tensor([[19.1569]], grad_fn=<SubBackward0>)
[2022-11-05 11:29:50.103196] Process 1. Episode 8500, average_reward -0.068353
Episode 8500: Total Loss of tensor([[3.9644]], grad_fn=<SubBackward0>)
[2022-11-05 11:29:56.169444] Process 0. Episode 8650, average_reward -0.073064
Episode 8650: Total Loss of tensor([[-9.1390]], grad_fn=<SubBackward0>)
[2022-11-05 11:30:06.549511] Process 4. Episode 9150, average_reward -0.076831
Episode 9150: Total Loss of tensor([[7.3250]], grad_fn=<SubBackward0>)
[2022-11-05 11:30:35.607522] Process 3. Episode 8600, average_reward -0.072907
Episode 8600: Total Loss of tensor([[-114.1073]], grad_fn=<SubBackward0>)
[2022-11-05 11:30:54.758326] Process 5. Episode 8750, average_reward -0.071314
Episode 8750: Total Loss of tensor([[2.9168]], grad_fn=<SubBackward0>)
[2022-11-05 11:31:10.748522] Process 2. Episode 8800, average_reward -0.078523
Episode 8800: Total Loss of tensor([[-39.7856]], grad_fn=<SubBackward0>)
[2022-11-05 11:32:22.177888] Process 0. Episode 8700, average_reward -0.073103
Episode 8700: Total Loss of tensor([[-1.3216]], grad_fn=<SubBackward0>)
[2022-11-05 11:32:28.479081] Process 1. Episode 8550, average_reward -0.068421
Episode 8550: Total Loss of tensor([[5.8295]], grad_fn=<SubBackward0>)
[2022-11-05 11:32:30.594539] Process 4. Episode 9200, average_reward -0.076630
Episode 9200: Total Loss of tensor([[8.1940]], grad_fn=<SubBackward0>)
[2022-11-05 11:33:20.948200] Process 3. Episode 8650, average_reward -0.072948
Episode 8650: Total Loss of tensor([[8.1821]], grad_fn=<SubBackward0>)
[2022-11-05 11:33:22.030805] Process 5. Episode 8800, average_reward -0.071250
Episode 8800: Total Loss of tensor([[-176.4928]], grad_fn=<SubBackward0>)
[2022-11-05 11:33:35.125989] Process 2. Episode 8850, average_reward -0.078418
Episode 8850: Total Loss of tensor([[0.3204]], grad_fn=<SubBackward0>)
[2022-11-05 11:34:54.274779] Process 4. Episode 9250, average_reward -0.076649
Episode 9250: Total Loss of tensor([[10.3855]], grad_fn=<SubBackward0>)
[2022-11-05 11:34:54.303120] Process 0. Episode 8750, average_reward -0.073143
Episode 8750: Total Loss of tensor([[1.6332]], grad_fn=<SubBackward0>)
[2022-11-05 11:35:05.090355] Process 1. Episode 8600, average_reward -0.068372
Episode 8600: Total Loss of tensor([[-0.7113]], grad_fn=<SubBackward0>)
[2022-11-05 11:35:52.404470] Process 5. Episode 8850, average_reward -0.071186
Episode 8850: Total Loss of tensor([[2.2846]], grad_fn=<SubBackward0>)
[2022-11-05 11:36:02.212001] Process 3. Episode 8700, average_reward -0.072759
Episode 8700: Total Loss of tensor([[9.9949]], grad_fn=<SubBackward0>)
[2022-11-05 11:36:12.997030] Process 2. Episode 8900, average_reward -0.078427
Episode 8900: Total Loss of tensor([[9.7029]], grad_fn=<SubBackward0>)
[2022-11-05 11:37:19.822709] Process 4. Episode 9300, average_reward -0.076559
Episode 9300: Total Loss of tensor([[4.3801]], grad_fn=<SubBackward0>)
[2022-11-05 11:37:34.088103] Process 0. Episode 8800, average_reward -0.072841
Episode 8800: Total Loss of tensor([[-7.7387]], grad_fn=<SubBackward0>)
[2022-11-05 11:37:45.434670] Process 1. Episode 8650, average_reward -0.068208
Episode 8650: Total Loss of tensor([[7.0168]], grad_fn=<SubBackward0>)
[2022-11-05 11:38:26.122057] Process 5. Episode 8900, average_reward -0.071124
Episode 8900: Total Loss of tensor([[-0.6531]], grad_fn=<SubBackward0>)
[2022-11-05 11:38:35.323761] Process 3. Episode 8750, average_reward -0.072457
Episode 8750: Total Loss of tensor([[-1.6542]], grad_fn=<SubBackward0>)
[2022-11-05 11:38:41.719815] Process 2. Episode 8950, average_reward -0.078436
Episode 8950: Total Loss of tensor([[-17.8147]], grad_fn=<SubBackward0>)
[2022-11-05 11:39:44.931034] Process 4. Episode 9350, average_reward -0.076150
Episode 9350: Total Loss of tensor([[12.0322]], grad_fn=<SubBackward0>)
[2022-11-05 11:40:01.355520] Process 0. Episode 8850, average_reward -0.072768
Episode 8850: Total Loss of tensor([[6.0200]], grad_fn=<SubBackward0>)
[2022-11-05 11:40:26.791894] Process 1. Episode 8700, average_reward -0.068506
Episode 8700: Total Loss of tensor([[1.9164]], grad_fn=<SubBackward0>)
[2022-11-05 11:41:04.466987] Process 2. Episode 9000, average_reward -0.078111
Episode 9000: Total Loss of tensor([[8.0099]], grad_fn=<SubBackward0>)
[2022-11-05 11:41:07.967826] Process 5. Episode 8950, average_reward -0.071173
Episode 8950: Total Loss of tensor([[5.0626]], grad_fn=<SubBackward0>)
[2022-11-05 11:41:12.463956] Process 3. Episode 8800, average_reward -0.072273
Episode 8800: Total Loss of tensor([[5.8120]], grad_fn=<SubBackward0>)
[2022-11-05 11:42:16.375584] Process 4. Episode 9400, average_reward -0.076170
Episode 9400: Total Loss of tensor([[1.7924]], grad_fn=<SubBackward0>)
[2022-11-05 11:42:29.027901] Process 0. Episode 8900, average_reward -0.072584
Episode 8900: Total Loss of tensor([[-21.4951]], grad_fn=<SubBackward0>)
[2022-11-05 11:43:10.075061] Process 1. Episode 8750, average_reward -0.068457
Episode 8750: Total Loss of tensor([[4.5190]], grad_fn=<SubBackward0>)
[2022-11-05 11:43:39.304094] Process 2. Episode 9050, average_reward -0.078122
Episode 9050: Total Loss of tensor([[7.6588]], grad_fn=<SubBackward0>)
[2022-11-05 11:43:45.306516] Process 5. Episode 9000, average_reward -0.071111
Episode 9000: Total Loss of tensor([[13.0009]], grad_fn=<SubBackward0>)
[2022-11-05 11:43:47.914253] Process 3. Episode 8850, average_reward -0.071977
Episode 8850: Total Loss of tensor([[7.4736]], grad_fn=<SubBackward0>)
[2022-11-05 11:44:39.261201] Process 4. Episode 9450, average_reward -0.075979
Episode 9450: Total Loss of tensor([[-5.8982]], grad_fn=<SubBackward0>)
[2022-11-05 11:44:55.982035] Process 0. Episode 8950, average_reward -0.072961
Episode 8950: Total Loss of tensor([[23.0644]], grad_fn=<SubBackward0>)
[2022-11-05 11:45:44.672637] Process 1. Episode 8800, average_reward -0.068636
Episode 8800: Total Loss of tensor([[7.1790]], grad_fn=<SubBackward0>)
[2022-11-05 11:46:07.823245] Process 2. Episode 9100, average_reward -0.078132
Episode 9100: Total Loss of tensor([[-5.8992]], grad_fn=<SubBackward0>)
[2022-11-05 11:46:24.277955] Process 5. Episode 9050, average_reward -0.070939
Episode 9050: Total Loss of tensor([[-8.9385]], grad_fn=<SubBackward0>)
[2022-11-05 11:46:33.352289] Process 3. Episode 8900, average_reward -0.072360
Episode 8900: Total Loss of tensor([[-96.9097]], grad_fn=<SubBackward0>)
[2022-11-05 11:47:02.585563] Process 4. Episode 9500, average_reward -0.075895
Episode 9500: Total Loss of tensor([[-0.6699]], grad_fn=<SubBackward0>)
[2022-11-05 11:47:30.289286] Process 0. Episode 9000, average_reward -0.072778
Episode 9000: Total Loss of tensor([[16.9114]], grad_fn=<SubBackward0>)
[2022-11-05 11:48:33.083927] Process 1. Episode 8850, average_reward -0.068701
Episode 8850: Total Loss of tensor([[11.4417]], grad_fn=<SubBackward0>)
[2022-11-05 11:48:37.683803] Process 2. Episode 9150, average_reward -0.077814
Episode 9150: Total Loss of tensor([[1.1243]], grad_fn=<SubBackward0>)
[2022-11-05 11:48:54.361942] Process 5. Episode 9100, average_reward -0.070769
Episode 9100: Total Loss of tensor([[1.3749]], grad_fn=<SubBackward0>)
[2022-11-05 11:49:18.023951] Process 3. Episode 8950, average_reward -0.072291
Episode 8950: Total Loss of tensor([[-1.0982]], grad_fn=<SubBackward0>)
[2022-11-05 11:49:26.396101] Process 4. Episode 9550, average_reward -0.075812
Episode 9550: Total Loss of tensor([[-1.4924]], grad_fn=<SubBackward0>)
[2022-11-05 11:49:56.993340] Process 0. Episode 9050, average_reward -0.072597
Episode 9050: Total Loss of tensor([[2.5380]], grad_fn=<SubBackward0>)
[2022-11-05 11:51:04.563135] Process 2. Episode 9200, average_reward -0.077935
Episode 9200: Total Loss of tensor([[1.2011]], grad_fn=<SubBackward0>)
[2022-11-05 11:51:16.911088] Process 1. Episode 8900, average_reward -0.068427
Episode 8900: Total Loss of tensor([[6.4038]], grad_fn=<SubBackward0>)
[2022-11-05 11:51:29.003264] Process 5. Episode 9150, average_reward -0.070601
Episode 9150: Total Loss of tensor([[2.3132]], grad_fn=<SubBackward0>)
[2022-11-05 11:51:50.850717] Process 4. Episode 9600, average_reward -0.075938
Episode 9600: Total Loss of tensor([[8.2556]], grad_fn=<SubBackward0>)
[2022-11-05 11:51:55.557112] Process 3. Episode 9000, average_reward -0.072000
Episode 9000: Total Loss of tensor([[9.7955]], grad_fn=<SubBackward0>)
[2022-11-05 11:52:30.628705] Process 0. Episode 9100, average_reward -0.072418
Episode 9100: Total Loss of tensor([[15.9108]], grad_fn=<SubBackward0>)
[2022-11-05 11:53:30.741385] Process 2. Episode 9250, average_reward -0.077730
Episode 9250: Total Loss of tensor([[-4.4410]], grad_fn=<SubBackward0>)
[2022-11-05 11:53:54.863858] Process 1. Episode 8950, average_reward -0.068268
Episode 8950: Total Loss of tensor([[7.3868]], grad_fn=<SubBackward0>)
[2022-11-05 11:54:05.197256] Process 5. Episode 9200, average_reward -0.070652
Episode 9200: Total Loss of tensor([[8.1789]], grad_fn=<SubBackward0>)
[2022-11-05 11:54:16.201849] Process 4. Episode 9650, average_reward -0.076166
Episode 9650: Total Loss of tensor([[0.5023]], grad_fn=<SubBackward0>)
[2022-11-05 11:54:28.796674] Process 3. Episode 9050, average_reward -0.072265
Episode 9050: Total Loss of tensor([[-101.4180]], grad_fn=<SubBackward0>)
[2022-11-05 11:55:12.443575] Process 0. Episode 9150, average_reward -0.072350
Episode 9150: Total Loss of tensor([[3.2263]], grad_fn=<SubBackward0>)
[2022-11-05 11:56:07.972006] Process 2. Episode 9300, average_reward -0.077742
Episode 9300: Total Loss of tensor([[14.4646]], grad_fn=<SubBackward0>)
[2022-11-05 11:56:21.604368] Process 1. Episode 9000, average_reward -0.068111
Episode 9000: Total Loss of tensor([[11.8216]], grad_fn=<SubBackward0>)
[2022-11-05 11:56:32.406647] Process 5. Episode 9250, average_reward -0.070703
Episode 9250: Total Loss of tensor([[4.7940]], grad_fn=<SubBackward0>)
[2022-11-05 11:56:37.800666] Process 4. Episode 9700, average_reward -0.076701
Episode 9700: Total Loss of tensor([[-128.2452]], grad_fn=<SubBackward0>)
[2022-11-05 11:57:09.309034] Process 3. Episode 9100, average_reward -0.071978
Episode 9100: Total Loss of tensor([[22.3312]], grad_fn=<SubBackward0>)
[2022-11-05 11:57:53.224559] Process 0. Episode 9200, average_reward -0.072500
Episode 9200: Total Loss of tensor([[9.8346]], grad_fn=<SubBackward0>)
[2022-11-05 11:58:32.738834] Process 2. Episode 9350, average_reward -0.077647
Episode 9350: Total Loss of tensor([[-91.4097]], grad_fn=<SubBackward0>)
[2022-11-05 11:58:57.732647] Process 4. Episode 9750, average_reward -0.076513
Episode 9750: Total Loss of tensor([[10.0146]], grad_fn=<SubBackward0>)
[2022-11-05 11:58:57.806974] Process 1. Episode 9050, average_reward -0.067845
Episode 9050: Total Loss of tensor([[2.4876]], grad_fn=<SubBackward0>)
[2022-11-05 11:58:57.913875] Process 5. Episode 9300, average_reward -0.070538
Episode 9300: Total Loss of tensor([[9.7868]], grad_fn=<SubBackward0>)
[2022-11-05 11:59:53.407405] Process 3. Episode 9150, average_reward -0.072022
Episode 9150: Total Loss of tensor([[10.5040]], grad_fn=<SubBackward0>)
[2022-11-05 12:00:33.291111] Process 0. Episode 9250, average_reward -0.072324
Episode 9250: Total Loss of tensor([[5.0450]], grad_fn=<SubBackward0>)
[2022-11-05 12:01:06.329780] Process 2. Episode 9400, average_reward -0.077553
Episode 9400: Total Loss of tensor([[4.0629]], grad_fn=<SubBackward0>)
[2022-11-05 12:01:19.312303] Process 4. Episode 9800, average_reward -0.076429
Episode 9800: Total Loss of tensor([[-2.9317]], grad_fn=<SubBackward0>)
[2022-11-05 12:01:27.753564] Process 1. Episode 9100, average_reward -0.067582
Episode 9100: Total Loss of tensor([[11.2520]], grad_fn=<SubBackward0>)
[2022-11-05 12:01:34.455058] Process 5. Episode 9350, average_reward -0.070481
Episode 9350: Total Loss of tensor([[-0.5775]], grad_fn=<SubBackward0>)
[2022-11-05 12:02:22.329620] Process 3. Episode 9200, average_reward -0.071848
Episode 9200: Total Loss of tensor([[-1.7282]], grad_fn=<SubBackward0>)
[2022-11-05 12:03:05.843741] Process 0. Episode 9300, average_reward -0.072043
Episode 9300: Total Loss of tensor([[5.2875]], grad_fn=<SubBackward0>)
[2022-11-05 12:03:41.761039] Process 4. Episode 9850, average_reward -0.076244
Episode 9850: Total Loss of tensor([[-1.9427]], grad_fn=<SubBackward0>)
[2022-11-05 12:03:53.636113] Process 2. Episode 9450, average_reward -0.077460
Episode 9450: Total Loss of tensor([[-35.5776]], grad_fn=<SubBackward0>)
[2022-11-05 12:03:54.345684] Process 1. Episode 9150, average_reward -0.067650
Episode 9150: Total Loss of tensor([[5.8326]], grad_fn=<SubBackward0>)
[2022-11-05 12:04:11.510206] Process 5. Episode 9400, average_reward -0.070426
Episode 9400: Total Loss of tensor([[-7.9244]], grad_fn=<SubBackward0>)
[2022-11-05 12:05:04.740754] Process 3. Episode 9250, average_reward -0.071892
Episode 9250: Total Loss of tensor([[10.0933]], grad_fn=<SubBackward0>)
[2022-11-05 12:05:34.340522] Process 0. Episode 9350, average_reward -0.072193
Episode 9350: Total Loss of tensor([[-3.4086]], grad_fn=<SubBackward0>)
[2022-11-05 12:06:05.842046] Process 4. Episode 9900, average_reward -0.076263
Episode 9900: Total Loss of tensor([[0.7736]], grad_fn=<SubBackward0>)
[2022-11-05 12:06:24.466387] Process 2. Episode 9500, average_reward -0.077474
Episode 9500: Total Loss of tensor([[5.8664]], grad_fn=<SubBackward0>)
[2022-11-05 12:06:33.462054] Process 1. Episode 9200, average_reward -0.067500
Episode 9200: Total Loss of tensor([[8.3775]], grad_fn=<SubBackward0>)
[2022-11-05 12:06:37.247724] Process 5. Episode 9450, average_reward -0.070370
Episode 9450: Total Loss of tensor([[1.7403]], grad_fn=<SubBackward0>)
[2022-11-05 12:07:46.190078] Process 3. Episode 9300, average_reward -0.071935
Episode 9300: Total Loss of tensor([[-21.1934]], grad_fn=<SubBackward0>)
[2022-11-05 12:08:08.518265] Process 0. Episode 9400, average_reward -0.072021
Episode 9400: Total Loss of tensor([[1.7854]], grad_fn=<SubBackward0>)
[2022-11-05 12:08:31.481977] Process 4. Episode 9950, average_reward -0.076281
Episode 9950: Total Loss of tensor([[-18.1783]], grad_fn=<SubBackward0>)
[2022-11-05 12:08:51.719836] Process 2. Episode 9550, average_reward -0.077382
Episode 9550: Total Loss of tensor([[18.0704]], grad_fn=<SubBackward0>)
[2022-11-05 12:09:00.861508] Process 1. Episode 9250, average_reward -0.067351
Episode 9250: Total Loss of tensor([[2.3191]], grad_fn=<SubBackward0>)
[2022-11-05 12:09:08.899477] Process 5. Episode 9500, average_reward -0.070316
Episode 9500: Total Loss of tensor([[10.5903]], grad_fn=<SubBackward0>)
[2022-11-05 12:10:29.223437] Process 3. Episode 9350, average_reward -0.072086
Episode 9350: Total Loss of tensor([[8.1244]], grad_fn=<SubBackward0>)
[2022-11-05 12:10:40.302636] Process 0. Episode 9450, average_reward -0.072063
Episode 9450: Total Loss of tensor([[6.2722]], grad_fn=<SubBackward0>)
[2022-11-05 12:10:51.601223] Process 4. Episode 10000, average_reward -0.076400
Episode 10000: Total Loss of tensor([[-1.2076]], grad_fn=<SubBackward0>)
[2022-11-05 12:11:16.252901] Process 2. Episode 9600, average_reward -0.077083
Episode 9600: Total Loss of tensor([[8.9658]], grad_fn=<SubBackward0>)
[2022-11-05 12:11:42.287133] Process 1. Episode 9300, average_reward -0.067204
Episode 9300: Total Loss of tensor([[9.3136]], grad_fn=<SubBackward0>)
[2022-11-05 12:11:47.087342] Process 5. Episode 9550, average_reward -0.070366
Episode 9550: Total Loss of tensor([[5.3299]], grad_fn=<SubBackward0>)
[2022-11-05 12:12:58.674358] Process 3. Episode 9400, average_reward -0.072340
Episode 9400: Total Loss of tensor([[8.9012]], grad_fn=<SubBackward0>)
[2022-11-05 12:13:15.052846] Process 4. Episode 10050, average_reward -0.076418
Episode 10050: Total Loss of tensor([[-101.1795]], grad_fn=<SubBackward0>)
[2022-11-05 12:13:15.799807] Process 0. Episode 9500, average_reward -0.071895
Episode 9500: Total Loss of tensor([[-11.2850]], grad_fn=<SubBackward0>)
[2022-11-05 12:13:47.148241] Process 2. Episode 9650, average_reward -0.076891
Episode 9650: Total Loss of tensor([[-12.4021]], grad_fn=<SubBackward0>)
[2022-11-05 12:14:17.238588] Process 5. Episode 9600, average_reward -0.070521
Episode 9600: Total Loss of tensor([[13.8865]], grad_fn=<SubBackward0>)
[2022-11-05 12:14:30.585949] Process 1. Episode 9350, average_reward -0.067166
Episode 9350: Total Loss of tensor([[24.5260]], grad_fn=<SubBackward0>)
[2022-11-05 12:15:31.045360] Process 3. Episode 9450, average_reward -0.072593
Episode 9450: Total Loss of tensor([[-46.9968]], grad_fn=<SubBackward0>)
[2022-11-05 12:15:36.957514] Process 4. Episode 10100, average_reward -0.076436
Episode 10100: Total Loss of tensor([[-20.4295]], grad_fn=<SubBackward0>)
[2022-11-05 12:15:42.473636] Process 0. Episode 9550, average_reward -0.071937
Episode 9550: Total Loss of tensor([[17.1054]], grad_fn=<SubBackward0>)
[2022-11-05 12:16:33.404380] Process 2. Episode 9700, average_reward -0.076701
Episode 9700: Total Loss of tensor([[9.4436]], grad_fn=<SubBackward0>)
[2022-11-05 12:16:57.155826] Process 5. Episode 9650, average_reward -0.070363
Episode 9650: Total Loss of tensor([[8.2962]], grad_fn=<SubBackward0>)
[2022-11-05 12:17:12.966991] Process 1. Episode 9400, average_reward -0.067234
Episode 9400: Total Loss of tensor([[10.9316]], grad_fn=<SubBackward0>)
[2022-11-05 12:17:58.484684] Process 4. Episode 10150, average_reward -0.076650
Episode 10150: Total Loss of tensor([[15.7420]], grad_fn=<SubBackward0>)
[2022-11-05 12:18:03.986136] Process 3. Episode 9500, average_reward -0.072737
Episode 9500: Total Loss of tensor([[14.9667]], grad_fn=<SubBackward0>)
[2022-11-05 12:18:17.406412] Process 0. Episode 9600, average_reward -0.071667
Episode 9600: Total Loss of tensor([[3.2411]], grad_fn=<SubBackward0>)
[2022-11-05 12:19:01.726269] Process 2. Episode 9750, average_reward -0.076615
Episode 9750: Total Loss of tensor([[-60.7655]], grad_fn=<SubBackward0>)
[2022-11-05 12:19:26.207596] Process 5. Episode 9700, average_reward -0.070619
Episode 9700: Total Loss of tensor([[17.4743]], grad_fn=<SubBackward0>)
[2022-11-05 12:19:59.446629] Process 1. Episode 9450, average_reward -0.067302
Episode 9450: Total Loss of tensor([[17.6308]], grad_fn=<SubBackward0>)
[2022-11-05 12:20:21.278407] Process 4. Episode 10200, average_reward -0.076471
Episode 10200: Total Loss of tensor([[10.5140]], grad_fn=<SubBackward0>)
[2022-11-05 12:20:43.994532] Process 3. Episode 9550, average_reward -0.072984
Episode 9550: Total Loss of tensor([[4.8082]], grad_fn=<SubBackward0>)
[2022-11-05 12:20:49.018229] Process 0. Episode 9650, average_reward -0.071606
Episode 9650: Total Loss of tensor([[-123.2665]], grad_fn=<SubBackward0>)
[2022-11-05 12:21:40.446080] Process 2. Episode 9800, average_reward -0.076531
Episode 9800: Total Loss of tensor([[2.9770]], grad_fn=<SubBackward0>)
[2022-11-05 12:22:02.444433] Process 5. Episode 9750, average_reward -0.070872
Episode 9750: Total Loss of tensor([[14.3632]], grad_fn=<SubBackward0>)
[2022-11-05 12:22:27.965950] Process 1. Episode 9500, average_reward -0.067158
Episode 9500: Total Loss of tensor([[4.7635]], grad_fn=<SubBackward0>)
[2022-11-05 12:22:46.990468] Process 4. Episode 10250, average_reward -0.076390
Episode 10250: Total Loss of tensor([[1.6491]], grad_fn=<SubBackward0>)
[2022-11-05 12:23:17.089510] Process 3. Episode 9600, average_reward -0.072708
Episode 9600: Total Loss of tensor([[-90.8436]], grad_fn=<SubBackward0>)
[2022-11-05 12:23:31.700849] Process 0. Episode 9700, average_reward -0.071443
Episode 9700: Total Loss of tensor([[4.3770]], grad_fn=<SubBackward0>)
[2022-11-05 12:24:14.679272] Process 2. Episode 9850, average_reward -0.076447
Episode 9850: Total Loss of tensor([[0.4928]], grad_fn=<SubBackward0>)
[2022-11-05 12:24:38.731353] Process 5. Episode 9800, average_reward -0.070612
Episode 9800: Total Loss of tensor([[11.7452]], grad_fn=<SubBackward0>)
[2022-11-05 12:24:56.347565] Process 1. Episode 9550, average_reward -0.067120
Episode 9550: Total Loss of tensor([[10.1562]], grad_fn=<SubBackward0>)
[2022-11-05 12:25:12.782369] Process 4. Episode 10300, average_reward -0.076505
Episode 10300: Total Loss of tensor([[11.8069]], grad_fn=<SubBackward0>)
[2022-11-05 12:25:43.164033] Process 3. Episode 9650, average_reward -0.072850
Episode 9650: Total Loss of tensor([[2.7422]], grad_fn=<SubBackward0>)
[2022-11-05 12:26:15.343874] Process 0. Episode 9750, average_reward -0.071590
Episode 9750: Total Loss of tensor([[13.8339]], grad_fn=<SubBackward0>)
[2022-11-05 12:26:53.643072] Process 2. Episode 9900, average_reward -0.076566
Episode 9900: Total Loss of tensor([[3.5874]], grad_fn=<SubBackward0>)
[2022-11-05 12:27:12.092210] Process 5. Episode 9850, average_reward -0.070457
Episode 9850: Total Loss of tensor([[19.2596]], grad_fn=<SubBackward0>)
[2022-11-05 12:27:27.777858] Process 1. Episode 9600, average_reward -0.067187
Episode 9600: Total Loss of tensor([[5.0046]], grad_fn=<SubBackward0>)
[2022-11-05 12:27:35.523892] Process 4. Episode 10350, average_reward -0.076329
Episode 10350: Total Loss of tensor([[12.7199]], grad_fn=<SubBackward0>)
[2022-11-05 12:28:09.466793] Process 3. Episode 9700, average_reward -0.072887
Episode 9700: Total Loss of tensor([[12.0454]], grad_fn=<SubBackward0>)
[2022-11-05 12:28:51.178793] Process 0. Episode 9800, average_reward -0.071429
Episode 9800: Total Loss of tensor([[6.2161]], grad_fn=<SubBackward0>)
[2022-11-05 12:29:26.887760] Process 2. Episode 9950, average_reward -0.076884
Episode 9950: Total Loss of tensor([[-49.7112]], grad_fn=<SubBackward0>)
[2022-11-05 12:29:43.960932] Process 5. Episode 9900, average_reward -0.070202
Episode 9900: Total Loss of tensor([[-28.4697]], grad_fn=<SubBackward0>)
[2022-11-05 12:29:58.500810] Process 4. Episode 10400, average_reward -0.076346
Episode 10400: Total Loss of tensor([[-1.5000]], grad_fn=<SubBackward0>)
[2022-11-05 12:30:11.371055] Process 1. Episode 9650, average_reward -0.067047
Episode 9650: Total Loss of tensor([[-14.4741]], grad_fn=<SubBackward0>)
[2022-11-05 12:30:47.833621] Process 3. Episode 9750, average_reward -0.073231
Episode 9750: Total Loss of tensor([[-10.6795]], grad_fn=<SubBackward0>)
[2022-11-05 12:31:22.565605] Process 0. Episode 9850, average_reward -0.071269
Episode 9850: Total Loss of tensor([[-4.4035]], grad_fn=<SubBackward0>)
[2022-11-05 12:31:54.528075] Process 2. Episode 10000, average_reward -0.076800
Episode 10000: Total Loss of tensor([[14.2186]], grad_fn=<SubBackward0>)
[2022-11-05 12:32:22.447984] Process 5. Episode 9950, average_reward -0.070050
Episode 9950: Total Loss of tensor([[13.5539]], grad_fn=<SubBackward0>)
[2022-11-05 12:32:24.420483] Process 4. Episode 10450, average_reward -0.076268
Episode 10450: Total Loss of tensor([[2.5708]], grad_fn=<SubBackward0>)
[2022-11-05 12:32:43.121005] Process 1. Episode 9700, average_reward -0.067113
Episode 9700: Total Loss of tensor([[16.6663]], grad_fn=<SubBackward0>)
[2022-11-05 12:33:18.816975] Process 3. Episode 9800, average_reward -0.073163
Episode 9800: Total Loss of tensor([[-98.0404]], grad_fn=<SubBackward0>)
[2022-11-05 12:33:54.541976] Process 0. Episode 9900, average_reward -0.071212
Episode 9900: Total Loss of tensor([[11.3341]], grad_fn=<SubBackward0>)
[2022-11-05 12:34:25.817191] Process 2. Episode 10050, average_reward -0.076617
Episode 10050: Total Loss of tensor([[3.3237]], grad_fn=<SubBackward0>)
[2022-11-05 12:34:48.843657] Process 4. Episode 10500, average_reward -0.076286
Episode 10500: Total Loss of tensor([[3.7255]], grad_fn=<SubBackward0>)
[2022-11-05 12:35:06.695001] Process 5. Episode 10000, average_reward -0.070300
Episode 10000: Total Loss of tensor([[3.0922]], grad_fn=<SubBackward0>)
[2022-11-05 12:35:13.758656] Process 1. Episode 9750, average_reward -0.067282
Episode 9750: Total Loss of tensor([[3.1082]], grad_fn=<SubBackward0>)
[2022-11-05 12:35:47.593614] Process 3. Episode 9850, average_reward -0.073096
Episode 9850: Total Loss of tensor([[11.2744]], grad_fn=<SubBackward0>)
[2022-11-05 12:36:25.344454] Process 0. Episode 9950, average_reward -0.071256
Episode 9950: Total Loss of tensor([[-3.1048]], grad_fn=<SubBackward0>)
[2022-11-05 12:37:02.269525] Process 2. Episode 10100, average_reward -0.076733
Episode 10100: Total Loss of tensor([[-7.4868]], grad_fn=<SubBackward0>)
[2022-11-05 12:37:11.763233] Process 4. Episode 10550, average_reward -0.076209
Episode 10550: Total Loss of tensor([[-1.6952]], grad_fn=<SubBackward0>)
[2022-11-05 12:37:43.736999] Process 5. Episode 10050, average_reward -0.070348
Episode 10050: Total Loss of tensor([[12.8261]], grad_fn=<SubBackward0>)
[2022-11-05 12:37:52.685199] Process 1. Episode 9800, average_reward -0.067041
Episode 9800: Total Loss of tensor([[23.5461]], grad_fn=<SubBackward0>)
[2022-11-05 12:38:18.605341] Process 3. Episode 9900, average_reward -0.072929
Episode 9900: Total Loss of tensor([[0.5935]], grad_fn=<SubBackward0>)
[2022-11-05 12:39:02.958656] Process 0. Episode 10000, average_reward -0.071400
Episode 10000: Total Loss of tensor([[18.6928]], grad_fn=<SubBackward0>)
[2022-11-05 12:39:28.192315] Process 2. Episode 10150, average_reward -0.076847
Episode 10150: Total Loss of tensor([[-76.1630]], grad_fn=<SubBackward0>)
[2022-11-05 12:39:38.924415] Process 4. Episode 10600, average_reward -0.076415
Episode 10600: Total Loss of tensor([[5.1395]], grad_fn=<SubBackward0>)
[2022-11-05 12:40:23.846853] Process 5. Episode 10100, average_reward -0.070396
Episode 10100: Total Loss of tensor([[21.2370]], grad_fn=<SubBackward0>)
[2022-11-05 12:40:27.622778] Process 1. Episode 9850, average_reward -0.067310
Episode 9850: Total Loss of tensor([[15.8891]], grad_fn=<SubBackward0>)
[2022-11-05 12:40:44.758242] Process 3. Episode 9950, average_reward -0.073065
Episode 9950: Total Loss of tensor([[12.5792]], grad_fn=<SubBackward0>)
[2022-11-05 12:41:40.463018] Process 0. Episode 10050, average_reward -0.071542
Episode 10050: Total Loss of tensor([[6.7022]], grad_fn=<SubBackward0>)
[2022-11-05 12:41:53.888328] Process 2. Episode 10200, average_reward -0.076667
Episode 10200: Total Loss of tensor([[-44.8690]], grad_fn=<SubBackward0>)
[2022-11-05 12:42:00.891538] Process 4. Episode 10650, average_reward -0.076526
Episode 10650: Total Loss of tensor([[4.3108]], grad_fn=<SubBackward0>)
[2022-11-05 12:42:53.117646] Process 5. Episode 10150, average_reward -0.070246
Episode 10150: Total Loss of tensor([[6.1484]], grad_fn=<SubBackward0>)
[2022-11-05 12:43:12.613454] Process 1. Episode 9900, average_reward -0.067576
Episode 9900: Total Loss of tensor([[6.9496]], grad_fn=<SubBackward0>)
[2022-11-05 12:43:18.753288] Process 3. Episode 10000, average_reward -0.073100
Episode 10000: Total Loss of tensor([[14.8677]], grad_fn=<SubBackward0>)
[2022-11-05 12:44:15.398386] Process 2. Episode 10250, average_reward -0.076585
Episode 10250: Total Loss of tensor([[-6.1414]], grad_fn=<SubBackward0>)
[2022-11-05 12:44:20.695995] Process 4. Episode 10700, average_reward -0.076262
Episode 10700: Total Loss of tensor([[2.5487]], grad_fn=<SubBackward0>)
[2022-11-05 12:44:28.773073] Process 0. Episode 10100, average_reward -0.071683
Episode 10100: Total Loss of tensor([[6.4728]], grad_fn=<SubBackward0>)
[2022-11-05 12:45:20.931078] Process 5. Episode 10200, average_reward -0.070098
Episode 10200: Total Loss of tensor([[15.6569]], grad_fn=<SubBackward0>)
[2022-11-05 12:45:54.406975] Process 1. Episode 9950, average_reward -0.067839
Episode 9950: Total Loss of tensor([[15.4837]], grad_fn=<SubBackward0>)
[2022-11-05 12:45:57.453146] Process 3. Episode 10050, average_reward -0.073234
Episode 10050: Total Loss of tensor([[6.7852]], grad_fn=<SubBackward0>)
[2022-11-05 12:46:44.649416] Process 4. Episode 10750, average_reward -0.075907
Episode 10750: Total Loss of tensor([[8.1599]], grad_fn=<SubBackward0>)
[2022-11-05 12:46:44.829907] Process 2. Episode 10300, average_reward -0.076699
Episode 10300: Total Loss of tensor([[-23.3405]], grad_fn=<SubBackward0>)
[2022-11-05 12:47:15.030111] Process 0. Episode 10150, average_reward -0.072217
Episode 10150: Total Loss of tensor([[-2.0968]], grad_fn=<SubBackward0>)
[2022-11-05 12:47:50.929262] Process 5. Episode 10250, average_reward -0.070049
Episode 10250: Total Loss of tensor([[4.6297]], grad_fn=<SubBackward0>)
[2022-11-05 12:48:25.624987] Process 3. Episode 10100, average_reward -0.073267
Episode 10100: Total Loss of tensor([[4.6370]], grad_fn=<SubBackward0>)
[2022-11-05 12:48:27.837032] Process 1. Episode 10000, average_reward -0.067800
Episode 10000: Total Loss of tensor([[15.2088]], grad_fn=<SubBackward0>)
[2022-11-05 12:49:08.948369] Process 4. Episode 10800, average_reward -0.076019
Episode 10800: Total Loss of tensor([[4.9839]], grad_fn=<SubBackward0>)
[2022-11-05 12:49:18.951231] Process 2. Episode 10350, average_reward -0.076522
Episode 10350: Total Loss of tensor([[10.1656]], grad_fn=<SubBackward0>)
[2022-11-05 12:49:40.179947] Process 0. Episode 10200, average_reward -0.071961
Episode 10200: Total Loss of tensor([[13.6351]], grad_fn=<SubBackward0>)
[2022-11-05 12:50:19.537342] Process 5. Episode 10300, average_reward -0.070000
Episode 10300: Total Loss of tensor([[10.2585]], grad_fn=<SubBackward0>)
[2022-11-05 12:51:07.897915] Process 3. Episode 10150, average_reward -0.073005
Episode 10150: Total Loss of tensor([[-69.8569]], grad_fn=<SubBackward0>)
[2022-11-05 12:51:08.542735] Process 1. Episode 10050, average_reward -0.067861
Episode 10050: Total Loss of tensor([[-1.5476]], grad_fn=<SubBackward0>)
[2022-11-05 12:51:30.397356] Process 4. Episode 10850, average_reward -0.075668
Episode 10850: Total Loss of tensor([[2.0814]], grad_fn=<SubBackward0>)
[2022-11-05 12:51:55.348395] Process 2. Episode 10400, average_reward -0.076154
Episode 10400: Total Loss of tensor([[-0.2823]], grad_fn=<SubBackward0>)
[2022-11-05 12:52:05.762377] Process 0. Episode 10250, average_reward -0.072098
Episode 10250: Total Loss of tensor([[16.1771]], grad_fn=<SubBackward0>)
[2022-11-05 12:52:58.083985] Process 5. Episode 10350, average_reward -0.070048
Episode 10350: Total Loss of tensor([[5.5584]], grad_fn=<SubBackward0>)
[2022-11-05 12:53:41.294003] Process 3. Episode 10200, average_reward -0.073039
Episode 10200: Total Loss of tensor([[3.1575]], grad_fn=<SubBackward0>)
[2022-11-05 12:53:46.612188] Process 1. Episode 10100, average_reward -0.068416
Episode 10100: Total Loss of tensor([[9.9732]], grad_fn=<SubBackward0>)
[2022-11-05 12:53:54.256105] Process 4. Episode 10900, average_reward -0.075596
Episode 10900: Total Loss of tensor([[-130.2737]], grad_fn=<SubBackward0>)
[2022-11-05 12:54:34.685448] Process 2. Episode 10450, average_reward -0.076172
Episode 10450: Total Loss of tensor([[2.4748]], grad_fn=<SubBackward0>)
[2022-11-05 12:54:35.278915] Process 0. Episode 10300, average_reward -0.072039
Episode 10300: Total Loss of tensor([[9.2461]], grad_fn=<SubBackward0>)
[2022-11-05 12:55:31.188222] Process 5. Episode 10400, average_reward -0.070288
Episode 10400: Total Loss of tensor([[18.8480]], grad_fn=<SubBackward0>)
[2022-11-05 12:56:13.676657] Process 3. Episode 10250, average_reward -0.073073
Episode 10250: Total Loss of tensor([[25.4406]], grad_fn=<SubBackward0>)
[2022-11-05 12:56:19.750701] Process 4. Episode 10950, average_reward -0.075342
Episode 10950: Total Loss of tensor([[-1.8381]], grad_fn=<SubBackward0>)
[2022-11-05 12:56:21.353679] Process 1. Episode 10150, average_reward -0.068670
Episode 10150: Total Loss of tensor([[-84.7334]], grad_fn=<SubBackward0>)
[2022-11-05 12:57:03.229998] Process 0. Episode 10350, average_reward -0.071787
Episode 10350: Total Loss of tensor([[11.6643]], grad_fn=<SubBackward0>)
[2022-11-05 12:57:04.855752] Process 2. Episode 10500, average_reward -0.076190
Episode 10500: Total Loss of tensor([[0.7331]], grad_fn=<SubBackward0>)
[2022-11-05 12:58:05.680359] Process 5. Episode 10450, average_reward -0.070239
Episode 10450: Total Loss of tensor([[-2.2973]], grad_fn=<SubBackward0>)
[2022-11-05 12:58:43.132413] Process 4. Episode 11000, average_reward -0.075182
Episode 11000: Total Loss of tensor([[2.0361]], grad_fn=<SubBackward0>)
[2022-11-05 12:58:48.677828] Process 1. Episode 10200, average_reward -0.068824
Episode 10200: Total Loss of tensor([[0.6446]], grad_fn=<SubBackward0>)
[2022-11-05 12:59:03.496014] Process 3. Episode 10300, average_reward -0.073010
Episode 10300: Total Loss of tensor([[18.3713]], grad_fn=<SubBackward0>)
[2022-11-05 12:59:30.342703] Process 0. Episode 10400, average_reward -0.071731
Episode 10400: Total Loss of tensor([[3.2654]], grad_fn=<SubBackward0>)
[2022-11-05 12:59:44.387401] Process 2. Episode 10550, average_reward -0.076303
Episode 10550: Total Loss of tensor([[-23.4954]], grad_fn=<SubBackward0>)
[2022-11-05 13:00:44.427691] Process 5. Episode 10500, average_reward -0.070476
Episode 10500: Total Loss of tensor([[17.9041]], grad_fn=<SubBackward0>)
[2022-11-05 13:01:04.543914] Process 4. Episode 11050, average_reward -0.075113
Episode 11050: Total Loss of tensor([[11.0066]], grad_fn=<SubBackward0>)
[2022-11-05 13:01:30.441800] Process 1. Episode 10250, average_reward -0.068780
Episode 10250: Total Loss of tensor([[33.3258]], grad_fn=<SubBackward0>)
[2022-11-05 13:01:50.503898] Process 3. Episode 10350, average_reward -0.073140
Episode 10350: Total Loss of tensor([[-111.6640]], grad_fn=<SubBackward0>)
[2022-11-05 13:02:01.983070] Process 0. Episode 10450, average_reward -0.072153
Episode 10450: Total Loss of tensor([[1.2102]], grad_fn=<SubBackward0>)
[2022-11-05 13:02:12.627029] Process 2. Episode 10600, average_reward -0.076226
Episode 10600: Total Loss of tensor([[14.6676]], grad_fn=<SubBackward0>)
[2022-11-05 13:03:16.258685] Process 5. Episode 10550, average_reward -0.070332
Episode 10550: Total Loss of tensor([[0.2469]], grad_fn=<SubBackward0>)
[2022-11-05 13:03:25.786735] Process 4. Episode 11100, average_reward -0.074865
Episode 11100: Total Loss of tensor([[12.4780]], grad_fn=<SubBackward0>)
[2022-11-05 13:03:58.845086] Process 1. Episode 10300, average_reward -0.069223
Episode 10300: Total Loss of tensor([[1.4917]], grad_fn=<SubBackward0>)
[2022-11-05 13:04:28.301121] Process 3. Episode 10400, average_reward -0.072981
Episode 10400: Total Loss of tensor([[-6.5426]], grad_fn=<SubBackward0>)
[2022-11-05 13:04:39.686685] Process 2. Episode 10650, average_reward -0.076150
Episode 10650: Total Loss of tensor([[-14.5107]], grad_fn=<SubBackward0>)
[2022-11-05 13:04:43.650200] Process 0. Episode 10500, average_reward -0.072000
Episode 10500: Total Loss of tensor([[-3.8922]], grad_fn=<SubBackward0>)
[2022-11-05 13:05:47.679545] Process 4. Episode 11150, average_reward -0.074798
Episode 11150: Total Loss of tensor([[23.2103]], grad_fn=<SubBackward0>)
[2022-11-05 13:05:56.077031] Process 5. Episode 10600, average_reward -0.070094
Episode 10600: Total Loss of tensor([[4.4617]], grad_fn=<SubBackward0>)
[2022-11-05 13:06:30.358448] Process 1. Episode 10350, average_reward -0.069082
Episode 10350: Total Loss of tensor([[14.2376]], grad_fn=<SubBackward0>)
[2022-11-05 13:07:07.385172] Process 3. Episode 10450, average_reward -0.073206
Episode 10450: Total Loss of tensor([[10.6565]], grad_fn=<SubBackward0>)
[2022-11-05 13:07:09.795229] Process 0. Episode 10550, average_reward -0.072038
Episode 10550: Total Loss of tensor([[-7.6314]], grad_fn=<SubBackward0>)
[2022-11-05 13:07:19.697018] Process 2. Episode 10700, average_reward -0.076168
Episode 10700: Total Loss of tensor([[11.2885]], grad_fn=<SubBackward0>)
[2022-11-05 13:08:10.622427] Process 4. Episode 11200, average_reward -0.074643
Episode 11200: Total Loss of tensor([[15.1510]], grad_fn=<SubBackward0>)
[2022-11-05 13:08:38.871065] Process 5. Episode 10650, average_reward -0.070047
Episode 10650: Total Loss of tensor([[14.0016]], grad_fn=<SubBackward0>)
[2022-11-05 13:09:11.632965] Process 1. Episode 10400, average_reward -0.069423
Episode 10400: Total Loss of tensor([[13.1580]], grad_fn=<SubBackward0>)
[2022-11-05 13:09:31.623760] Process 0. Episode 10600, average_reward -0.072075
Episode 10600: Total Loss of tensor([[-103.6514]], grad_fn=<SubBackward0>)
[2022-11-05 13:09:47.805807] Process 3. Episode 10500, average_reward -0.073143
Episode 10500: Total Loss of tensor([[-3.5978]], grad_fn=<SubBackward0>)
[2022-11-05 13:09:54.405779] Process 2. Episode 10750, average_reward -0.076372
Episode 10750: Total Loss of tensor([[9.0455]], grad_fn=<SubBackward0>)
[2022-11-05 13:10:33.498565] Process 4. Episode 11250, average_reward -0.074578
Episode 11250: Total Loss of tensor([[3.0882]], grad_fn=<SubBackward0>)
[2022-11-05 13:11:08.419005] Process 5. Episode 10700, average_reward -0.069907
Episode 10700: Total Loss of tensor([[4.1285]], grad_fn=<SubBackward0>)
[2022-11-05 13:11:48.042965] Process 1. Episode 10450, average_reward -0.069378
Episode 10450: Total Loss of tensor([[2.7943]], grad_fn=<SubBackward0>)
[2022-11-05 13:12:13.781829] Process 0. Episode 10650, average_reward -0.072207
Episode 10650: Total Loss of tensor([[-3.0010]], grad_fn=<SubBackward0>)
[2022-11-05 13:12:17.035317] Process 2. Episode 10800, average_reward -0.076204
Episode 10800: Total Loss of tensor([[15.6369]], grad_fn=<SubBackward0>)
[2022-11-05 13:12:30.854950] Process 3. Episode 10550, average_reward -0.073175
Episode 10550: Total Loss of tensor([[0.2619]], grad_fn=<SubBackward0>)
[2022-11-05 13:12:56.411740] Process 4. Episode 11300, average_reward -0.074602
Episode 11300: Total Loss of tensor([[-113.7855]], grad_fn=<SubBackward0>)
[2022-11-05 13:13:31.426674] Process 5. Episode 10750, average_reward -0.069953
Episode 10750: Total Loss of tensor([[3.8587]], grad_fn=<SubBackward0>)
[2022-11-05 13:14:20.377035] Process 1. Episode 10500, average_reward -0.069238
Episode 10500: Total Loss of tensor([[2.8384]], grad_fn=<SubBackward0>)
[2022-11-05 13:14:49.298580] Process 2. Episode 10850, average_reward -0.076313
Episode 10850: Total Loss of tensor([[13.0398]], grad_fn=<SubBackward0>)
[2022-11-05 13:14:52.382344] Process 0. Episode 10700, average_reward -0.072150
Episode 10700: Total Loss of tensor([[-0.4989]], grad_fn=<SubBackward0>)
[2022-11-05 13:15:10.253064] Process 3. Episode 10600, average_reward -0.073396
Episode 10600: Total Loss of tensor([[-4.7853]], grad_fn=<SubBackward0>)
[2022-11-05 13:15:22.842995] Process 4. Episode 11350, average_reward -0.074626
Episode 11350: Total Loss of tensor([[12.9543]], grad_fn=<SubBackward0>)
[2022-11-05 13:16:00.102464] Process 5. Episode 10800, average_reward -0.069722
Episode 10800: Total Loss of tensor([[15.1912]], grad_fn=<SubBackward0>)
[2022-11-05 13:16:45.506316] Process 1. Episode 10550, average_reward -0.069194
Episode 10550: Total Loss of tensor([[3.4718]], grad_fn=<SubBackward0>)
[2022-11-05 13:17:22.224605] Process 2. Episode 10900, average_reward -0.076330
Episode 10900: Total Loss of tensor([[-121.3550]], grad_fn=<SubBackward0>)
[2022-11-05 13:17:23.854815] Process 0. Episode 10750, average_reward -0.072186
Episode 10750: Total Loss of tensor([[7.1333]], grad_fn=<SubBackward0>)
[2022-11-05 13:17:43.803144] Process 4. Episode 11400, average_reward -0.074298
Episode 11400: Total Loss of tensor([[16.0652]], grad_fn=<SubBackward0>)
[2022-11-05 13:17:54.395956] Process 3. Episode 10650, average_reward -0.073239
Episode 10650: Total Loss of tensor([[5.6850]], grad_fn=<SubBackward0>)
[2022-11-05 13:18:28.257081] Process 5. Episode 10850, average_reward -0.069770
Episode 10850: Total Loss of tensor([[10.2279]], grad_fn=<SubBackward0>)
[2022-11-05 13:19:18.856450] Process 1. Episode 10600, average_reward -0.069434
Episode 10600: Total Loss of tensor([[-2.5154]], grad_fn=<SubBackward0>)
[2022-11-05 13:19:52.983299] Process 0. Episode 10800, average_reward -0.072222
Episode 10800: Total Loss of tensor([[12.1254]], grad_fn=<SubBackward0>)
[2022-11-05 13:19:54.711596] Process 2. Episode 10950, average_reward -0.076256
Episode 10950: Total Loss of tensor([[6.8585]], grad_fn=<SubBackward0>)
[2022-11-05 13:20:11.587159] Process 4. Episode 11450, average_reward -0.074061
Episode 11450: Total Loss of tensor([[-9.1145]], grad_fn=<SubBackward0>)
[2022-11-05 13:20:30.765843] Process 3. Episode 10700, average_reward -0.073458
Episode 10700: Total Loss of tensor([[0.7431]], grad_fn=<SubBackward0>)
[2022-11-05 13:20:56.057261] Process 5. Episode 10900, average_reward -0.070183
Episode 10900: Total Loss of tensor([[6.7133]], grad_fn=<SubBackward0>)
[2022-11-05 13:22:00.152304] Process 1. Episode 10650, average_reward -0.069296
Episode 10650: Total Loss of tensor([[20.8407]], grad_fn=<SubBackward0>)
[2022-11-05 13:22:18.139092] Process 0. Episode 10850, average_reward -0.072074
Episode 10850: Total Loss of tensor([[-1.0831]], grad_fn=<SubBackward0>)
[2022-11-05 13:22:28.203628] Process 2. Episode 11000, average_reward -0.076182
Episode 11000: Total Loss of tensor([[17.1450]], grad_fn=<SubBackward0>)
[2022-11-05 13:22:34.534030] Process 4. Episode 11500, average_reward -0.074087
Episode 11500: Total Loss of tensor([[16.5938]], grad_fn=<SubBackward0>)
[2022-11-05 13:23:16.034183] Process 3. Episode 10750, average_reward -0.073395
Episode 10750: Total Loss of tensor([[5.5090]], grad_fn=<SubBackward0>)
[2022-11-05 13:23:27.352818] Process 5. Episode 10950, average_reward -0.070594
Episode 10950: Total Loss of tensor([[-10.4858]], grad_fn=<SubBackward0>)
[2022-11-05 13:24:41.093128] Process 1. Episode 10700, average_reward -0.069346
Episode 10700: Total Loss of tensor([[-94.8381]], grad_fn=<SubBackward0>)
[2022-11-05 13:24:52.004251] Process 0. Episode 10900, average_reward -0.072110
Episode 10900: Total Loss of tensor([[24.0696]], grad_fn=<SubBackward0>)
[2022-11-05 13:24:59.175228] Process 2. Episode 11050, average_reward -0.076199
Episode 11050: Total Loss of tensor([[22.8357]], grad_fn=<SubBackward0>)
[2022-11-05 13:25:00.520656] Process 4. Episode 11550, average_reward -0.074545
Episode 11550: Total Loss of tensor([[18.0373]], grad_fn=<SubBackward0>)
[2022-11-05 13:25:43.863526] Process 3. Episode 10800, average_reward -0.073796
Episode 10800: Total Loss of tensor([[12.0193]], grad_fn=<SubBackward0>)
[2022-11-05 13:25:54.023022] Process 5. Episode 11000, average_reward -0.070727
Episode 11000: Total Loss of tensor([[29.5393]], grad_fn=<SubBackward0>)
[2022-11-05 13:27:16.368596] Process 1. Episode 10750, average_reward -0.069302
Episode 10750: Total Loss of tensor([[16.0755]], grad_fn=<SubBackward0>)
[2022-11-05 13:27:22.710260] Process 2. Episode 11100, average_reward -0.076216
Episode 11100: Total Loss of tensor([[6.8306]], grad_fn=<SubBackward0>)
[2022-11-05 13:27:26.132745] Process 4. Episode 11600, average_reward -0.074655
Episode 11600: Total Loss of tensor([[22.3722]], grad_fn=<SubBackward0>)
[2022-11-05 13:27:39.012909] Process 0. Episode 10950, average_reward -0.072237
Episode 10950: Total Loss of tensor([[3.3097]], grad_fn=<SubBackward0>)
[2022-11-05 13:28:16.409625] Process 3. Episode 10850, average_reward -0.073733
Episode 10850: Total Loss of tensor([[7.2496]], grad_fn=<SubBackward0>)
[2022-11-05 13:28:28.736444] Process 5. Episode 11050, average_reward -0.070498
Episode 11050: Total Loss of tensor([[-26.5866]], grad_fn=<SubBackward0>)
[2022-11-05 13:29:43.226841] Process 1. Episode 10800, average_reward -0.069259
Episode 10800: Total Loss of tensor([[6.8077]], grad_fn=<SubBackward0>)
[2022-11-05 13:29:50.526261] Process 2. Episode 11150, average_reward -0.076323
Episode 11150: Total Loss of tensor([[7.5432]], grad_fn=<SubBackward0>)
[2022-11-05 13:29:51.863257] Process 4. Episode 11650, average_reward -0.074678
Episode 11650: Total Loss of tensor([[11.5099]], grad_fn=<SubBackward0>)
[2022-11-05 13:30:27.771584] Process 0. Episode 11000, average_reward -0.072182
Episode 11000: Total Loss of tensor([[-110.0831]], grad_fn=<SubBackward0>)
[2022-11-05 13:30:52.581321] Process 3. Episode 10900, average_reward -0.073578
Episode 10900: Total Loss of tensor([[14.4340]], grad_fn=<SubBackward0>)
[2022-11-05 13:30:57.385821] Process 5. Episode 11100, average_reward -0.070721
Episode 11100: Total Loss of tensor([[-30.1822]], grad_fn=<SubBackward0>)
[2022-11-05 13:32:12.256533] Process 4. Episode 11700, average_reward -0.074444
Episode 11700: Total Loss of tensor([[13.1594]], grad_fn=<SubBackward0>)
[2022-11-05 13:32:19.201829] Process 1. Episode 10850, average_reward -0.069309
Episode 10850: Total Loss of tensor([[11.7167]], grad_fn=<SubBackward0>)
[2022-11-05 13:32:32.308944] Process 2. Episode 11200, average_reward -0.076339
Episode 11200: Total Loss of tensor([[-117.3439]], grad_fn=<SubBackward0>)
[2022-11-05 13:33:09.133852] Process 0. Episode 11050, average_reward -0.071855
Episode 11050: Total Loss of tensor([[2.0900]], grad_fn=<SubBackward0>)
[2022-11-05 13:33:30.057255] Process 3. Episode 10950, average_reward -0.073425
Episode 10950: Total Loss of tensor([[4.0835]], grad_fn=<SubBackward0>)
[2022-11-05 13:33:32.125372] Process 5. Episode 11150, average_reward -0.070493
Episode 11150: Total Loss of tensor([[6.4297]], grad_fn=<SubBackward0>)
[2022-11-05 13:34:34.730148] Process 4. Episode 11750, average_reward -0.074468
Episode 11750: Total Loss of tensor([[7.7244]], grad_fn=<SubBackward0>)
[2022-11-05 13:34:53.144424] Process 1. Episode 10900, average_reward -0.069450
Episode 10900: Total Loss of tensor([[-1.3093]], grad_fn=<SubBackward0>)
[2022-11-05 13:34:58.834871] Process 2. Episode 11250, average_reward -0.076444
Episode 11250: Total Loss of tensor([[6.9083]], grad_fn=<SubBackward0>)
[2022-11-05 13:35:40.409048] Process 0. Episode 11100, average_reward -0.071622
Episode 11100: Total Loss of tensor([[0.3254]], grad_fn=<SubBackward0>)
[2022-11-05 13:36:01.873878] Process 5. Episode 11200, average_reward -0.070446
Episode 11200: Total Loss of tensor([[8.4753]], grad_fn=<SubBackward0>)
[2022-11-05 13:36:09.045388] Process 3. Episode 11000, average_reward -0.073364
Episode 11000: Total Loss of tensor([[-1.9059]], grad_fn=<SubBackward0>)
[2022-11-05 13:37:01.275206] Process 4. Episode 11800, average_reward -0.074322
Episode 11800: Total Loss of tensor([[3.6227]], grad_fn=<SubBackward0>)
[2022-11-05 13:37:23.747174] Process 2. Episode 11300, average_reward -0.076372
Episode 11300: Total Loss of tensor([[1.7207]], grad_fn=<SubBackward0>)
[2022-11-05 13:37:30.207214] Process 1. Episode 10950, average_reward -0.069498
Episode 10950: Total Loss of tensor([[-0.1132]], grad_fn=<SubBackward0>)
[2022-11-05 13:38:10.357778] Process 0. Episode 11150, average_reward -0.071839
Episode 11150: Total Loss of tensor([[13.0653]], grad_fn=<SubBackward0>)
[2022-11-05 13:38:28.524679] Process 5. Episode 11250, average_reward -0.070489
Episode 11250: Total Loss of tensor([[4.1959]], grad_fn=<SubBackward0>)
[2022-11-05 13:38:49.559371] Process 3. Episode 11050, average_reward -0.073484
Episode 11050: Total Loss of tensor([[-83.1351]], grad_fn=<SubBackward0>)
[2022-11-05 13:39:27.083129] Process 4. Episode 11850, average_reward -0.074346
Episode 11850: Total Loss of tensor([[-0.0946]], grad_fn=<SubBackward0>)
[2022-11-05 13:40:01.246155] Process 2. Episode 11350, average_reward -0.076123
Episode 11350: Total Loss of tensor([[6.0269]], grad_fn=<SubBackward0>)
[2022-11-05 13:40:03.889830] Process 1. Episode 11000, average_reward -0.069545
Episode 11000: Total Loss of tensor([[-31.5543]], grad_fn=<SubBackward0>)
[2022-11-05 13:40:52.679384] Process 5. Episode 11300, average_reward -0.070177
Episode 11300: Total Loss of tensor([[6.1131]], grad_fn=<SubBackward0>)
[2022-11-05 13:40:54.123052] Process 0. Episode 11200, average_reward -0.071875
Episode 11200: Total Loss of tensor([[12.3573]], grad_fn=<SubBackward0>)
[2022-11-05 13:41:17.231678] Process 3. Episode 11100, average_reward -0.073423
Episode 11100: Total Loss of tensor([[6.2327]], grad_fn=<SubBackward0>)
[2022-11-05 13:41:54.349385] Process 4. Episode 11900, average_reward -0.074538
Episode 11900: Total Loss of tensor([[14.8589]], grad_fn=<SubBackward0>)
[2022-11-05 13:42:29.517148] Process 1. Episode 11050, average_reward -0.069593
Episode 11050: Total Loss of tensor([[1.4856]], grad_fn=<SubBackward0>)
[2022-11-05 13:42:32.371773] Process 2. Episode 11400, average_reward -0.076140
Episode 11400: Total Loss of tensor([[2.1324]], grad_fn=<SubBackward0>)
[2022-11-05 13:43:32.579219] Process 0. Episode 11250, average_reward -0.071822
Episode 11250: Total Loss of tensor([[-1.0025]], grad_fn=<SubBackward0>)
[2022-11-05 13:43:33.613323] Process 5. Episode 11350, average_reward -0.070044
Episode 11350: Total Loss of tensor([[9.0301]], grad_fn=<SubBackward0>)
[2022-11-05 13:43:47.937314] Process 3. Episode 11150, average_reward -0.073274
Episode 11150: Total Loss of tensor([[9.0383]], grad_fn=<SubBackward0>)
[2022-11-05 13:44:18.610262] Process 4. Episode 11950, average_reward -0.074728
Episode 11950: Total Loss of tensor([[14.2145]], grad_fn=<SubBackward0>)
[2022-11-05 13:44:55.932377] Process 1. Episode 11100, average_reward -0.069730
Episode 11100: Total Loss of tensor([[9.7305]], grad_fn=<SubBackward0>)
[2022-11-05 13:44:59.016558] Process 2. Episode 11450, average_reward -0.076507
Episode 11450: Total Loss of tensor([[-61.2839]], grad_fn=<SubBackward0>)
[2022-11-05 13:46:06.142922] Process 5. Episode 11400, average_reward -0.069912
Episode 11400: Total Loss of tensor([[12.4674]], grad_fn=<SubBackward0>)
[2022-11-05 13:46:16.774840] Process 0. Episode 11300, average_reward -0.071770
Episode 11300: Total Loss of tensor([[0.0869]], grad_fn=<SubBackward0>)
[2022-11-05 13:46:17.367032] Process 3. Episode 11200, average_reward -0.073125
Episode 11200: Total Loss of tensor([[5.9727]], grad_fn=<SubBackward0>)
[2022-11-05 13:46:42.673530] Process 4. Episode 12000, average_reward -0.074750
Episode 12000: Total Loss of tensor([[10.4827]], grad_fn=<SubBackward0>)
[2022-11-05 13:47:21.063467] Process 1. Episode 11150, average_reward -0.069865
Episode 11150: Total Loss of tensor([[10.7103]], grad_fn=<SubBackward0>)
[2022-11-05 13:47:33.611933] Process 2. Episode 11500, average_reward -0.076261
Episode 11500: Total Loss of tensor([[0.3393]], grad_fn=<SubBackward0>)
[2022-11-05 13:48:42.852663] Process 5. Episode 11450, average_reward -0.070218
Episode 11450: Total Loss of tensor([[-127.2318]], grad_fn=<SubBackward0>)
[2022-11-05 13:48:59.095060] Process 3. Episode 11250, average_reward -0.073156
Episode 11250: Total Loss of tensor([[5.7998]], grad_fn=<SubBackward0>)
[2022-11-05 13:49:00.347037] Process 0. Episode 11350, average_reward -0.071806
Episode 11350: Total Loss of tensor([[8.8459]], grad_fn=<SubBackward0>)
[2022-11-05 13:49:06.463688] Process 4. Episode 12050, average_reward -0.074523
Episode 12050: Total Loss of tensor([[3.8121]], grad_fn=<SubBackward0>)
[2022-11-05 13:49:52.258992] Process 1. Episode 11200, average_reward -0.069911
Episode 11200: Total Loss of tensor([[11.3001]], grad_fn=<SubBackward0>)
[2022-11-05 13:49:59.821655] Process 2. Episode 11550, average_reward -0.076364
Episode 11550: Total Loss of tensor([[1.5210]], grad_fn=<SubBackward0>)
[2022-11-05 13:51:23.830515] Process 5. Episode 11500, average_reward -0.070087
Episode 11500: Total Loss of tensor([[-1.8200]], grad_fn=<SubBackward0>)
[2022-11-05 13:51:26.634832] Process 0. Episode 11400, average_reward -0.071667
Episode 11400: Total Loss of tensor([[6.5006]], grad_fn=<SubBackward0>)
[2022-11-05 13:51:29.592583] Process 3. Episode 11300, average_reward -0.073540
Episode 11300: Total Loss of tensor([[-0.3191]], grad_fn=<SubBackward0>)
[2022-11-05 13:51:30.266881] Process 4. Episode 12100, average_reward -0.074463
Episode 12100: Total Loss of tensor([[9.4373]], grad_fn=<SubBackward0>)
[2022-11-05 13:52:22.150059] Process 1. Episode 11250, average_reward -0.069689
Episode 11250: Total Loss of tensor([[-1.4304]], grad_fn=<SubBackward0>)
[2022-11-05 13:52:39.898588] Process 2. Episode 11600, average_reward -0.076207
Episode 11600: Total Loss of tensor([[1.4464]], grad_fn=<SubBackward0>)
[2022-11-05 13:53:53.243778] Process 4. Episode 12150, average_reward -0.074486
Episode 12150: Total Loss of tensor([[3.4424]], grad_fn=<SubBackward0>)
[2022-11-05 13:54:03.391396] Process 3. Episode 11350, average_reward -0.073656
Episode 11350: Total Loss of tensor([[-6.7007]], grad_fn=<SubBackward0>)
[2022-11-05 13:54:04.575712] Process 0. Episode 11450, average_reward -0.071616
Episode 11450: Total Loss of tensor([[15.2596]], grad_fn=<SubBackward0>)
[2022-11-05 13:54:05.972409] Process 5. Episode 11550, average_reward -0.070216
Episode 11550: Total Loss of tensor([[-18.6189]], grad_fn=<SubBackward0>)
[2022-11-05 13:54:58.845662] Process 1. Episode 11300, average_reward -0.069558
Episode 11300: Total Loss of tensor([[-24.9960]], grad_fn=<SubBackward0>)
[2022-11-05 13:55:07.826354] Process 2. Episode 11650, average_reward -0.076137
Episode 11650: Total Loss of tensor([[-1.1523]], grad_fn=<SubBackward0>)
[2022-11-05 13:56:18.168026] Process 4. Episode 12200, average_reward -0.074508
Episode 12200: Total Loss of tensor([[8.7198]], grad_fn=<SubBackward0>)
[2022-11-05 13:56:26.730874] Process 3. Episode 11400, average_reward -0.073772
Episode 11400: Total Loss of tensor([[11.6123]], grad_fn=<SubBackward0>)
[2022-11-05 13:56:39.036335] Process 5. Episode 11600, average_reward -0.070086
Episode 11600: Total Loss of tensor([[0.9835]], grad_fn=<SubBackward0>)
[2022-11-05 13:56:44.245322] Process 0. Episode 11500, average_reward -0.071739
Episode 11500: Total Loss of tensor([[9.6435]], grad_fn=<SubBackward0>)
[2022-11-05 13:57:37.551782] Process 2. Episode 11700, average_reward -0.076068
Episode 11700: Total Loss of tensor([[13.5047]], grad_fn=<SubBackward0>)
[2022-11-05 13:57:44.542777] Process 1. Episode 11350, average_reward -0.069339
Episode 11350: Total Loss of tensor([[6.3558]], grad_fn=<SubBackward0>)
[2022-11-05 13:58:41.612166] Process 4. Episode 12250, average_reward -0.074531
Episode 12250: Total Loss of tensor([[9.5225]], grad_fn=<SubBackward0>)
[2022-11-05 13:58:59.849826] Process 3. Episode 11450, average_reward -0.073712
Episode 11450: Total Loss of tensor([[17.4456]], grad_fn=<SubBackward0>)
[2022-11-05 13:59:16.113001] Process 5. Episode 11650, average_reward -0.070215
Episode 11650: Total Loss of tensor([[-1.4712]], grad_fn=<SubBackward0>)
[2022-11-05 13:59:19.553857] Process 0. Episode 11550, average_reward -0.071688
Episode 11550: Total Loss of tensor([[-1.7919]], grad_fn=<SubBackward0>)
[2022-11-05 14:00:09.030587] Process 2. Episode 11750, average_reward -0.076085
Episode 11750: Total Loss of tensor([[-10.0813]], grad_fn=<SubBackward0>)
[2022-11-05 14:00:18.354040] Process 1. Episode 11400, average_reward -0.069123
Episode 11400: Total Loss of tensor([[2.2171]], grad_fn=<SubBackward0>)
[2022-11-05 14:01:02.779543] Process 4. Episode 12300, average_reward -0.074553
Episode 12300: Total Loss of tensor([[11.2806]], grad_fn=<SubBackward0>)
[2022-11-05 14:01:34.771913] Process 3. Episode 11500, average_reward -0.073913
Episode 11500: Total Loss of tensor([[18.0242]], grad_fn=<SubBackward0>)
[2022-11-05 14:01:48.183643] Process 5. Episode 11700, average_reward -0.070085
Episode 11700: Total Loss of tensor([[7.7150]], grad_fn=<SubBackward0>)
[2022-11-05 14:01:50.772647] Process 0. Episode 11600, average_reward -0.071638
Episode 11600: Total Loss of tensor([[13.0830]], grad_fn=<SubBackward0>)
[2022-11-05 14:02:38.651147] Process 2. Episode 11800, average_reward -0.075932
Episode 11800: Total Loss of tensor([[10.7486]], grad_fn=<SubBackward0>)
[2022-11-05 14:03:03.482427] Process 1. Episode 11450, average_reward -0.069083
Episode 11450: Total Loss of tensor([[-1.1084]], grad_fn=<SubBackward0>)
[2022-11-05 14:03:26.213429] Process 4. Episode 12350, average_reward -0.074737
Episode 12350: Total Loss of tensor([[0.7391]], grad_fn=<SubBackward0>)
[2022-11-05 14:04:09.652889] Process 3. Episode 11550, average_reward -0.073939
Episode 11550: Total Loss of tensor([[11.6261]], grad_fn=<SubBackward0>)
[2022-11-05 14:04:17.976376] Process 5. Episode 11750, average_reward -0.070298
Episode 11750: Total Loss of tensor([[0.0379]], grad_fn=<SubBackward0>)
[2022-11-05 14:04:27.017578] Process 0. Episode 11650, average_reward -0.071416
Episode 11650: Total Loss of tensor([[10.0003]], grad_fn=<SubBackward0>)
[2022-11-05 14:05:06.889834] Process 2. Episode 11850, average_reward -0.075865
Episode 11850: Total Loss of tensor([[-101.5960]], grad_fn=<SubBackward0>)
[2022-11-05 14:05:39.700961] Process 1. Episode 11500, average_reward -0.069130
Episode 11500: Total Loss of tensor([[22.4029]], grad_fn=<SubBackward0>)
[2022-11-05 14:05:52.662889] Process 4. Episode 12400, average_reward -0.074516
Episode 12400: Total Loss of tensor([[12.3123]], grad_fn=<SubBackward0>)
[2022-11-05 14:06:38.740181] Process 3. Episode 11600, average_reward -0.073966
Episode 11600: Total Loss of tensor([[4.0143]], grad_fn=<SubBackward0>)
[2022-11-05 14:06:56.936371] Process 5. Episode 11800, average_reward -0.070424
Episode 11800: Total Loss of tensor([[1.5895]], grad_fn=<SubBackward0>)
[2022-11-05 14:07:04.928671] Process 0. Episode 11700, average_reward -0.071111
Episode 11700: Total Loss of tensor([[-10.1311]], grad_fn=<SubBackward0>)
[2022-11-05 14:07:38.199681] Process 2. Episode 11900, average_reward -0.075630
Episode 11900: Total Loss of tensor([[1.6513]], grad_fn=<SubBackward0>)
[2022-11-05 14:08:14.793296] Process 1. Episode 11550, average_reward -0.068918
Episode 11550: Total Loss of tensor([[4.3731]], grad_fn=<SubBackward0>)
[2022-11-05 14:08:18.617166] Process 4. Episode 12450, average_reward -0.074458
Episode 12450: Total Loss of tensor([[3.0747]], grad_fn=<SubBackward0>)
[2022-11-05 14:09:18.507813] Process 3. Episode 11650, average_reward -0.073906
Episode 11650: Total Loss of tensor([[12.9407]], grad_fn=<SubBackward0>)
[2022-11-05 14:09:34.425182] Process 5. Episode 11850, average_reward -0.070211
Episode 11850: Total Loss of tensor([[4.0300]], grad_fn=<SubBackward0>)
[2022-11-05 14:09:42.412779] Process 0. Episode 11750, average_reward -0.071149
Episode 11750: Total Loss of tensor([[14.7947]], grad_fn=<SubBackward0>)
[2022-11-05 14:10:02.900862] Process 2. Episode 11950, average_reward -0.075732
Episode 11950: Total Loss of tensor([[-21.9636]], grad_fn=<SubBackward0>)
[2022-11-05 14:10:41.542299] Process 4. Episode 12500, average_reward -0.074480
Episode 12500: Total Loss of tensor([[16.4861]], grad_fn=<SubBackward0>)
[2022-11-05 14:11:00.063822] Process 1. Episode 11600, average_reward -0.068707
Episode 11600: Total Loss of tensor([[4.6092]], grad_fn=<SubBackward0>)
[2022-11-05 14:11:45.820083] Process 3. Episode 11700, average_reward -0.074017
Episode 11700: Total Loss of tensor([[-2.4884]], grad_fn=<SubBackward0>)
[2022-11-05 14:12:09.661532] Process 5. Episode 11900, average_reward -0.070084
Episode 11900: Total Loss of tensor([[-8.5778]], grad_fn=<SubBackward0>)
[2022-11-05 14:12:16.682853] Process 0. Episode 11800, average_reward -0.071102
Episode 11800: Total Loss of tensor([[-7.0135]], grad_fn=<SubBackward0>)
[2022-11-05 14:12:32.326244] Process 2. Episode 12000, average_reward -0.075833
Episode 12000: Total Loss of tensor([[5.7614]], grad_fn=<SubBackward0>)
[2022-11-05 14:13:05.393905] Process 4. Episode 12550, average_reward -0.074263
Episode 12550: Total Loss of tensor([[9.6439]], grad_fn=<SubBackward0>)
[2022-11-05 14:13:42.869194] Process 1. Episode 11650, average_reward -0.068498
Episode 11650: Total Loss of tensor([[-13.1352]], grad_fn=<SubBackward0>)
[2022-11-05 14:14:18.846711] Process 3. Episode 11750, average_reward -0.074213
Episode 11750: Total Loss of tensor([[19.9631]], grad_fn=<SubBackward0>)
[2022-11-05 14:14:42.322751] Process 5. Episode 11950, average_reward -0.070377
Episode 11950: Total Loss of tensor([[7.4767]], grad_fn=<SubBackward0>)
[2022-11-05 14:14:48.230952] Process 0. Episode 11850, average_reward -0.071055
Episode 11850: Total Loss of tensor([[8.3236]], grad_fn=<SubBackward0>)
[2022-11-05 14:14:58.941009] Process 2. Episode 12050, average_reward -0.075851
Episode 12050: Total Loss of tensor([[10.5871]], grad_fn=<SubBackward0>)
[2022-11-05 14:15:26.160620] Process 4. Episode 12600, average_reward -0.074286
Episode 12600: Total Loss of tensor([[4.9353]], grad_fn=<SubBackward0>)
[2022-11-05 14:16:20.697013] Process 1. Episode 11700, average_reward -0.068632
Episode 11700: Total Loss of tensor([[4.5377]], grad_fn=<SubBackward0>)
[2022-11-05 14:16:58.060460] Process 3. Episode 11800, average_reward -0.074237
Episode 11800: Total Loss of tensor([[5.5669]], grad_fn=<SubBackward0>)
[2022-11-05 14:17:16.794476] Process 5. Episode 12000, average_reward -0.070250
Episode 12000: Total Loss of tensor([[8.6421]], grad_fn=<SubBackward0>)
[2022-11-05 14:17:30.966284] Process 0. Episode 11900, average_reward -0.071008
Episode 11900: Total Loss of tensor([[10.9918]], grad_fn=<SubBackward0>)
[2022-11-05 14:17:32.696337] Process 2. Episode 12100, average_reward -0.075785
Episode 12100: Total Loss of tensor([[17.4927]], grad_fn=<SubBackward0>)
[2022-11-05 14:17:49.105094] Process 4. Episode 12650, average_reward -0.074466
Episode 12650: Total Loss of tensor([[11.6872]], grad_fn=<SubBackward0>)
[2022-11-05 14:18:54.045635] Process 1. Episode 11750, average_reward -0.068426
Episode 11750: Total Loss of tensor([[10.1556]], grad_fn=<SubBackward0>)
[2022-11-05 14:19:28.659213] Process 3. Episode 11850, average_reward -0.074346
Episode 11850: Total Loss of tensor([[-18.2111]], grad_fn=<SubBackward0>)
[2022-11-05 14:19:42.730027] Process 5. Episode 12050, average_reward -0.070124
Episode 12050: Total Loss of tensor([[-0.4008]], grad_fn=<SubBackward0>)
[2022-11-05 14:20:01.961992] Process 2. Episode 12150, average_reward -0.075967
Episode 12150: Total Loss of tensor([[5.9257]], grad_fn=<SubBackward0>)
[2022-11-05 14:20:13.024456] Process 4. Episode 12700, average_reward -0.074567
Episode 12700: Total Loss of tensor([[-35.3707]], grad_fn=<SubBackward0>)
[2022-11-05 14:20:20.139826] Process 0. Episode 11950, average_reward -0.071130
Episode 11950: Total Loss of tensor([[6.0276]], grad_fn=<SubBackward0>)
[2022-11-05 14:21:20.544076] Process 1. Episode 11800, average_reward -0.068559
Episode 11800: Total Loss of tensor([[17.8516]], grad_fn=<SubBackward0>)
[2022-11-05 14:21:54.698793] Process 3. Episode 11900, average_reward -0.074370
Episode 11900: Total Loss of tensor([[-1.8751]], grad_fn=<SubBackward0>)
[2022-11-05 14:22:15.948489] Process 5. Episode 12100, average_reward -0.070083
Episode 12100: Total Loss of tensor([[-0.6478]], grad_fn=<SubBackward0>)
[2022-11-05 14:22:35.982035] Process 4. Episode 12750, average_reward -0.074588
Episode 12750: Total Loss of tensor([[-83.7476]], grad_fn=<SubBackward0>)
[2022-11-05 14:22:37.282185] Process 2. Episode 12200, average_reward -0.075738
Episode 12200: Total Loss of tensor([[6.7682]], grad_fn=<SubBackward0>)
[2022-11-05 14:22:55.175649] Process 0. Episode 12000, average_reward -0.071417
Episode 12000: Total Loss of tensor([[6.4780]], grad_fn=<SubBackward0>)
[2022-11-05 14:24:08.429999] Process 1. Episode 11850, average_reward -0.068861
Episode 11850: Total Loss of tensor([[1.6949]], grad_fn=<SubBackward0>)
[2022-11-05 14:24:27.243332] Process 3. Episode 11950, average_reward -0.074310
Episode 11950: Total Loss of tensor([[27.0213]], grad_fn=<SubBackward0>)
[2022-11-05 14:24:42.723497] Process 5. Episode 12150, average_reward -0.070123
Episode 12150: Total Loss of tensor([[12.3855]], grad_fn=<SubBackward0>)
[2022-11-05 14:24:58.940396] Process 4. Episode 12800, average_reward -0.074531
Episode 12800: Total Loss of tensor([[-18.5044]], grad_fn=<SubBackward0>)
[2022-11-05 14:25:18.014689] Process 2. Episode 12250, average_reward -0.075592
Episode 12250: Total Loss of tensor([[2.5609]], grad_fn=<SubBackward0>)
[2022-11-05 14:25:28.028566] Process 0. Episode 12050, average_reward -0.071369
Episode 12050: Total Loss of tensor([[12.6883]], grad_fn=<SubBackward0>)
[2022-11-05 14:26:42.683700] Process 1. Episode 11900, average_reward -0.068739
Episode 11900: Total Loss of tensor([[9.7505]], grad_fn=<SubBackward0>)
[2022-11-05 14:27:09.539875] Process 5. Episode 12200, average_reward -0.070082
Episode 12200: Total Loss of tensor([[14.3383]], grad_fn=<SubBackward0>)
[2022-11-05 14:27:12.359439] Process 3. Episode 12000, average_reward -0.074583
Episode 12000: Total Loss of tensor([[7.7301]], grad_fn=<SubBackward0>)
[2022-11-05 14:27:23.080397] Process 4. Episode 12850, average_reward -0.074553
Episode 12850: Total Loss of tensor([[7.5756]], grad_fn=<SubBackward0>)
[2022-11-05 14:27:49.746236] Process 2. Episode 12300, average_reward -0.075691
Episode 12300: Total Loss of tensor([[6.7072]], grad_fn=<SubBackward0>)
[2022-11-05 14:28:03.103690] Process 0. Episode 12100, average_reward -0.071157
Episode 12100: Total Loss of tensor([[2.7735]], grad_fn=<SubBackward0>)
[2022-11-05 14:29:13.040234] Process 1. Episode 11950, average_reward -0.068536
Episode 11950: Total Loss of tensor([[11.0662]], grad_fn=<SubBackward0>)
[2022-11-05 14:29:37.039432] Process 5. Episode 12250, average_reward -0.070122
Episode 12250: Total Loss of tensor([[1.4970]], grad_fn=<SubBackward0>)
[2022-11-05 14:29:45.647567] Process 3. Episode 12050, average_reward -0.074523
Episode 12050: Total Loss of tensor([[-5.1241]], grad_fn=<SubBackward0>)
[2022-11-05 14:29:49.922345] Process 4. Episode 12900, average_reward -0.074574
Episode 12900: Total Loss of tensor([[5.5310]], grad_fn=<SubBackward0>)
[2022-11-05 14:30:22.633753] Process 2. Episode 12350, average_reward -0.075709
Episode 12350: Total Loss of tensor([[7.5288]], grad_fn=<SubBackward0>)
[2022-11-05 14:30:35.913199] Process 0. Episode 12150, average_reward -0.071193
Episode 12150: Total Loss of tensor([[2.0827]], grad_fn=<SubBackward0>)
[2022-11-05 14:31:45.535011] Process 1. Episode 12000, average_reward -0.068417
Episode 12000: Total Loss of tensor([[17.0215]], grad_fn=<SubBackward0>)
[2022-11-05 14:32:04.662037] Process 5. Episode 12300, average_reward -0.070081
Episode 12300: Total Loss of tensor([[1.8869]], grad_fn=<SubBackward0>)
[2022-11-05 14:32:17.907733] Process 4. Episode 12950, average_reward -0.074672
Episode 12950: Total Loss of tensor([[9.0865]], grad_fn=<SubBackward0>)
[2022-11-05 14:32:27.037952] Process 3. Episode 12100, average_reward -0.074628
Episode 12100: Total Loss of tensor([[-92.7529]], grad_fn=<SubBackward0>)
[2022-11-05 14:32:49.901906] Process 2. Episode 12400, average_reward -0.075887
Episode 12400: Total Loss of tensor([[14.0133]], grad_fn=<SubBackward0>)
[2022-11-05 14:33:13.514259] Process 0. Episode 12200, average_reward -0.070984
Episode 12200: Total Loss of tensor([[9.4866]], grad_fn=<SubBackward0>)
[2022-11-05 14:34:23.013425] Process 1. Episode 12050, average_reward -0.068216
Episode 12050: Total Loss of tensor([[18.6272]], grad_fn=<SubBackward0>)
[2022-11-05 14:34:36.239823] Process 5. Episode 12350, average_reward -0.070040
Episode 12350: Total Loss of tensor([[4.2406]], grad_fn=<SubBackward0>)
[2022-11-05 14:34:45.971101] Process 4. Episode 13000, average_reward -0.074692
Episode 13000: Total Loss of tensor([[-15.0393]], grad_fn=<SubBackward0>)
[2022-11-05 14:35:08.024494] Process 3. Episode 12150, average_reward -0.074486
Episode 12150: Total Loss of tensor([[1.6405]], grad_fn=<SubBackward0>)
[2022-11-05 14:35:15.552929] Process 2. Episode 12450, average_reward -0.075823
Episode 12450: Total Loss of tensor([[4.0698]], grad_fn=<SubBackward0>)
[2022-11-05 14:35:49.066345] Process 0. Episode 12250, average_reward -0.071102
Episode 12250: Total Loss of tensor([[4.9955]], grad_fn=<SubBackward0>)
[2022-11-05 14:37:06.043593] Process 5. Episode 12400, average_reward -0.070161
Episode 12400: Total Loss of tensor([[-4.9975]], grad_fn=<SubBackward0>)
[2022-11-05 14:37:07.204528] Process 1. Episode 12100, average_reward -0.068099
Episode 12100: Total Loss of tensor([[1.7742]], grad_fn=<SubBackward0>)
[2022-11-05 14:37:12.680315] Process 4. Episode 13050, average_reward -0.074483
Episode 13050: Total Loss of tensor([[12.6086]], grad_fn=<SubBackward0>)
[2022-11-05 14:37:41.980725] Process 2. Episode 12500, average_reward -0.075840
Episode 12500: Total Loss of tensor([[-114.5590]], grad_fn=<SubBackward0>)
[2022-11-05 14:37:44.838526] Process 3. Episode 12200, average_reward -0.074344
Episode 12200: Total Loss of tensor([[-1.6068]], grad_fn=<SubBackward0>)
[2022-11-05 14:38:21.222378] Process 0. Episode 12300, average_reward -0.071138
Episode 12300: Total Loss of tensor([[5.7996]], grad_fn=<SubBackward0>)
[2022-11-05 14:39:29.059288] Process 5. Episode 12450, average_reward -0.070201
Episode 12450: Total Loss of tensor([[4.3237]], grad_fn=<SubBackward0>)
[2022-11-05 14:39:38.918094] Process 4. Episode 13100, average_reward -0.074656
Episode 13100: Total Loss of tensor([[18.8822]], grad_fn=<SubBackward0>)
[2022-11-05 14:39:42.045232] Process 1. Episode 12150, average_reward -0.067984
Episode 12150: Total Loss of tensor([[-2.1253]], grad_fn=<SubBackward0>)
[2022-11-05 14:40:13.026073] Process 3. Episode 12250, average_reward -0.074286
Episode 12250: Total Loss of tensor([[1.7493]], grad_fn=<SubBackward0>)
[2022-11-05 14:40:13.841382] Process 2. Episode 12550, average_reward -0.075697
Episode 12550: Total Loss of tensor([[3.8054]], grad_fn=<SubBackward0>)
[2022-11-05 14:40:55.608465] Process 0. Episode 12350, average_reward -0.071174
Episode 12350: Total Loss of tensor([[3.7645]], grad_fn=<SubBackward0>)
[2022-11-05 14:41:56.704798] Process 5. Episode 12500, average_reward -0.070240
Episode 12500: Total Loss of tensor([[-4.4446]], grad_fn=<SubBackward0>)
[2022-11-05 14:42:01.184286] Process 4. Episode 13150, average_reward -0.074677
Episode 13150: Total Loss of tensor([[-2.8361]], grad_fn=<SubBackward0>)
[2022-11-05 14:42:12.261169] Process 1. Episode 12200, average_reward -0.067869
Episode 12200: Total Loss of tensor([[-1.1686]], grad_fn=<SubBackward0>)
[2022-11-05 14:42:36.422786] Process 3. Episode 12300, average_reward -0.074390
Episode 12300: Total Loss of tensor([[13.5710]], grad_fn=<SubBackward0>)
[2022-11-05 14:42:54.029047] Process 2. Episode 12600, average_reward -0.075635
Episode 12600: Total Loss of tensor([[10.3530]], grad_fn=<SubBackward0>)
[2022-11-05 14:43:29.018005] Process 0. Episode 12400, average_reward -0.071048
Episode 12400: Total Loss of tensor([[8.4261]], grad_fn=<SubBackward0>)
[2022-11-05 14:44:21.809445] Process 4. Episode 13200, average_reward -0.074621
Episode 13200: Total Loss of tensor([[11.9068]], grad_fn=<SubBackward0>)
[2022-11-05 14:44:22.695215] Process 5. Episode 12550, average_reward -0.070040
Episode 12550: Total Loss of tensor([[26.5365]], grad_fn=<SubBackward0>)
[2022-11-05 14:44:48.668035] Process 1. Episode 12250, average_reward -0.067755
Episode 12250: Total Loss of tensor([[-8.3724]], grad_fn=<SubBackward0>)
[2022-11-05 14:45:06.013982] Process 3. Episode 12350, average_reward -0.074089
Episode 12350: Total Loss of tensor([[-70.5951]], grad_fn=<SubBackward0>)
[2022-11-05 14:45:18.575898] Process 2. Episode 12650, average_reward -0.075573
Episode 12650: Total Loss of tensor([[4.4489]], grad_fn=<SubBackward0>)
[2022-11-05 14:46:04.513999] Process 0. Episode 12450, average_reward -0.071165
Episode 12450: Total Loss of tensor([[-48.2861]], grad_fn=<SubBackward0>)
[2022-11-05 14:46:45.383189] Process 4. Episode 13250, average_reward -0.074340
Episode 13250: Total Loss of tensor([[7.1004]], grad_fn=<SubBackward0>)
[2022-11-05 14:46:54.921831] Process 5. Episode 12600, average_reward -0.070159
Episode 12600: Total Loss of tensor([[3.1570]], grad_fn=<SubBackward0>)
[2022-11-05 14:47:29.014622] Process 1. Episode 12300, average_reward -0.067724
Episode 12300: Total Loss of tensor([[-4.5392]], grad_fn=<SubBackward0>)
[2022-11-05 14:47:47.214941] Process 2. Episode 12700, average_reward -0.075591
Episode 12700: Total Loss of tensor([[-7.1008]], grad_fn=<SubBackward0>)
[2022-11-05 14:47:48.573770] Process 3. Episode 12400, average_reward -0.074032
Episode 12400: Total Loss of tensor([[-134.0270]], grad_fn=<SubBackward0>)
[2022-11-05 14:48:29.541043] Process 0. Episode 12500, average_reward -0.071440
Episode 12500: Total Loss of tensor([[-4.3004]], grad_fn=<SubBackward0>)
[2022-11-05 14:49:10.374196] Process 4. Episode 13300, average_reward -0.074060
Episode 13300: Total Loss of tensor([[14.9656]], grad_fn=<SubBackward0>)
[2022-11-05 14:49:22.368843] Process 5. Episode 12650, average_reward -0.070198
Episode 12650: Total Loss of tensor([[15.9364]], grad_fn=<SubBackward0>)
[2022-11-05 14:50:15.544003] Process 1. Episode 12350, average_reward -0.067773
Episode 12350: Total Loss of tensor([[-3.4356]], grad_fn=<SubBackward0>)
[2022-11-05 14:50:17.140612] Process 3. Episode 12450, average_reward -0.074056
Episode 12450: Total Loss of tensor([[-96.0387]], grad_fn=<SubBackward0>)
[2022-11-05 14:50:24.448327] Process 2. Episode 12750, average_reward -0.075529
Episode 12750: Total Loss of tensor([[16.5193]], grad_fn=<SubBackward0>)
[2022-11-05 14:50:54.571129] Process 0. Episode 12550, average_reward -0.071554
Episode 12550: Total Loss of tensor([[10.0814]], grad_fn=<SubBackward0>)
[2022-11-05 14:51:36.002510] Process 4. Episode 13350, average_reward -0.074082
Episode 13350: Total Loss of tensor([[-11.0068]], grad_fn=<SubBackward0>)
[2022-11-05 14:51:51.513503] Process 5. Episode 12700, average_reward -0.070079
Episode 12700: Total Loss of tensor([[2.7136]], grad_fn=<SubBackward0>)
[2022-11-05 14:52:48.329702] Process 2. Episode 12800, average_reward -0.075703
Episode 12800: Total Loss of tensor([[11.2503]], grad_fn=<SubBackward0>)
[2022-11-05 14:52:51.325131] Process 1. Episode 12400, average_reward -0.067581
Episode 12400: Total Loss of tensor([[4.4344]], grad_fn=<SubBackward0>)
[2022-11-05 14:52:53.606897] Process 3. Episode 12500, average_reward -0.074000
Episode 12500: Total Loss of tensor([[10.3431]], grad_fn=<SubBackward0>)
[2022-11-05 14:53:20.294156] Process 0. Episode 12600, average_reward -0.071667
Episode 12600: Total Loss of tensor([[10.8202]], grad_fn=<SubBackward0>)
[2022-11-05 14:53:55.702453] Process 4. Episode 13400, average_reward -0.073881
Episode 13400: Total Loss of tensor([[9.8829]], grad_fn=<SubBackward0>)
[2022-11-05 14:54:20.932440] Process 5. Episode 12750, average_reward -0.070118
Episode 12750: Total Loss of tensor([[14.0965]], grad_fn=<SubBackward0>)
[2022-11-05 14:55:17.234072] Process 2. Episode 12850, average_reward -0.075564
Episode 12850: Total Loss of tensor([[-111.3190]], grad_fn=<SubBackward0>)
[2022-11-05 14:55:25.084377] Process 1. Episode 12450, average_reward -0.067631
Episode 12450: Total Loss of tensor([[-8.5146]], grad_fn=<SubBackward0>)
[2022-11-05 14:55:27.402499] Process 3. Episode 12550, average_reward -0.073865
Episode 12550: Total Loss of tensor([[7.6765]], grad_fn=<SubBackward0>)
[2022-11-05 14:55:53.827958] Process 0. Episode 12650, average_reward -0.071462
Episode 12650: Total Loss of tensor([[10.6406]], grad_fn=<SubBackward0>)
[2022-11-05 14:56:17.232793] Process 4. Episode 13450, average_reward -0.073978
Episode 13450: Total Loss of tensor([[13.7245]], grad_fn=<SubBackward0>)
[2022-11-05 14:56:47.189049] Process 5. Episode 12800, average_reward -0.070000
Episode 12800: Total Loss of tensor([[-108.6517]], grad_fn=<SubBackward0>)
[2022-11-05 14:57:44.715880] Process 2. Episode 12900, average_reward -0.075581
Episode 12900: Total Loss of tensor([[22.4156]], grad_fn=<SubBackward0>)
[2022-11-05 14:57:57.920575] Process 1. Episode 12500, average_reward -0.067920
Episode 12500: Total Loss of tensor([[-63.8859]], grad_fn=<SubBackward0>)
[2022-11-05 14:58:06.010342] Process 3. Episode 12600, average_reward -0.073968
Episode 12600: Total Loss of tensor([[17.0874]], grad_fn=<SubBackward0>)
[2022-11-05 14:58:21.239752] Process 0. Episode 12700, average_reward -0.071496
Episode 12700: Total Loss of tensor([[2.8241]], grad_fn=<SubBackward0>)
[2022-11-05 14:58:38.522833] Process 4. Episode 13500, average_reward -0.074148
Episode 13500: Total Loss of tensor([[17.9263]], grad_fn=<SubBackward0>)
[2022-11-05 14:59:18.491892] Process 5. Episode 12850, average_reward -0.069883
Episode 12850: Total Loss of tensor([[0.4611]], grad_fn=<SubBackward0>)
[2022-11-05 15:00:17.135965] Process 2. Episode 12950, average_reward -0.075521
Episode 12950: Total Loss of tensor([[5.8056]], grad_fn=<SubBackward0>)
[2022-11-05 15:00:36.007075] Process 3. Episode 12650, average_reward -0.073755
Episode 12650: Total Loss of tensor([[5.8066]], grad_fn=<SubBackward0>)
[2022-11-05 15:00:37.638844] Process 1. Episode 12550, average_reward -0.068048
Episode 12550: Total Loss of tensor([[-125.6405]], grad_fn=<SubBackward0>)
[2022-11-05 15:00:52.364202] Process 0. Episode 12750, average_reward -0.071529
Episode 12750: Total Loss of tensor([[-8.9008]], grad_fn=<SubBackward0>)
[2022-11-05 15:01:01.902059] Process 4. Episode 13550, average_reward -0.073948
Episode 13550: Total Loss of tensor([[9.3926]], grad_fn=<SubBackward0>)
[2022-11-05 15:01:43.750716] Process 5. Episode 12900, average_reward -0.069845
Episode 12900: Total Loss of tensor([[6.5987]], grad_fn=<SubBackward0>)
[2022-11-05 15:02:45.087810] Process 2. Episode 13000, average_reward -0.075769
Episode 13000: Total Loss of tensor([[7.2473]], grad_fn=<SubBackward0>)
[2022-11-05 15:03:12.323446] Process 1. Episode 12600, average_reward -0.067937
Episode 12600: Total Loss of tensor([[17.2684]], grad_fn=<SubBackward0>)
[2022-11-05 15:03:12.888961] Process 3. Episode 12700, average_reward -0.074094
Episode 12700: Total Loss of tensor([[6.1833]], grad_fn=<SubBackward0>)
[2022-11-05 15:03:14.706065] Process 0. Episode 12800, average_reward -0.071563
Episode 12800: Total Loss of tensor([[11.1319]], grad_fn=<SubBackward0>)
[2022-11-05 15:03:24.120121] Process 4. Episode 13600, average_reward -0.073897
Episode 13600: Total Loss of tensor([[20.0477]], grad_fn=<SubBackward0>)
[2022-11-05 15:04:14.065485] Process 5. Episode 12950, average_reward -0.070039
Episode 12950: Total Loss of tensor([[12.4302]], grad_fn=<SubBackward0>)
[2022-11-05 15:05:14.899295] Process 2. Episode 13050, average_reward -0.075632
Episode 13050: Total Loss of tensor([[-1.8086]], grad_fn=<SubBackward0>)
[2022-11-05 15:05:37.544519] Process 3. Episode 12750, average_reward -0.074039
Episode 12750: Total Loss of tensor([[9.3114]], grad_fn=<SubBackward0>)
[2022-11-05 15:05:42.525746] Process 0. Episode 12850, average_reward -0.071518
Episode 12850: Total Loss of tensor([[-68.6432]], grad_fn=<SubBackward0>)
[2022-11-05 15:05:48.950350] Process 4. Episode 13650, average_reward -0.073773
Episode 13650: Total Loss of tensor([[13.3566]], grad_fn=<SubBackward0>)
[2022-11-05 15:05:50.354039] Process 1. Episode 12650, average_reward -0.067984
Episode 12650: Total Loss of tensor([[12.4036]], grad_fn=<SubBackward0>)
[2022-11-05 15:06:40.911994] Process 5. Episode 13000, average_reward -0.070077
Episode 13000: Total Loss of tensor([[1.9720]], grad_fn=<SubBackward0>)
[2022-11-05 15:07:37.158841] Process 2. Episode 13100, average_reward -0.075649
Episode 13100: Total Loss of tensor([[3.3776]], grad_fn=<SubBackward0>)
[2022-11-05 15:08:06.880813] Process 3. Episode 12800, average_reward -0.073750
Episode 12800: Total Loss of tensor([[11.5746]], grad_fn=<SubBackward0>)
[2022-11-05 15:08:08.233315] Process 0. Episode 12900, average_reward -0.071550
Episode 12900: Total Loss of tensor([[7.8962]], grad_fn=<SubBackward0>)
[2022-11-05 15:08:11.627161] Process 4. Episode 13700, average_reward -0.073577
Episode 13700: Total Loss of tensor([[7.4188]], grad_fn=<SubBackward0>)
[2022-11-05 15:08:28.835111] Process 1. Episode 12700, average_reward -0.067953
Episode 12700: Total Loss of tensor([[2.3033]], grad_fn=<SubBackward0>)
[2022-11-05 15:09:16.516768] Process 5. Episode 13050, average_reward -0.070115
Episode 13050: Total Loss of tensor([[-12.3630]], grad_fn=<SubBackward0>)
[2022-11-05 15:10:05.191021] Process 2. Episode 13150, average_reward -0.075361
Episode 13150: Total Loss of tensor([[1.3999]], grad_fn=<SubBackward0>)
[2022-11-05 15:10:31.733326] Process 0. Episode 12950, average_reward -0.071506
Episode 12950: Total Loss of tensor([[1.5277]], grad_fn=<SubBackward0>)
[2022-11-05 15:10:32.894887] Process 4. Episode 13750, average_reward -0.073309
Episode 13750: Total Loss of tensor([[3.8327]], grad_fn=<SubBackward0>)
[2022-11-05 15:10:36.196976] Process 3. Episode 12850, average_reward -0.073541
Episode 12850: Total Loss of tensor([[1.1114]], grad_fn=<SubBackward0>)
[2022-11-05 15:11:00.145215] Process 1. Episode 12750, average_reward -0.067922
Episode 12750: Total Loss of tensor([[-4.4929]], grad_fn=<SubBackward0>)
[2022-11-05 15:11:45.744270] Process 5. Episode 13100, average_reward -0.070229
Episode 13100: Total Loss of tensor([[-75.6329]], grad_fn=<SubBackward0>)
[2022-11-05 15:12:27.043337] Process 2. Episode 13200, average_reward -0.075303
Episode 13200: Total Loss of tensor([[11.4073]], grad_fn=<SubBackward0>)
[2022-11-05 15:12:53.006565] Process 4. Episode 13800, average_reward -0.073333
Episode 13800: Total Loss of tensor([[10.6049]], grad_fn=<SubBackward0>)
[2022-11-05 15:13:01.235351] Process 0. Episode 13000, average_reward -0.071462
Episode 13000: Total Loss of tensor([[14.1060]], grad_fn=<SubBackward0>)
[2022-11-05 15:13:12.131846] Process 3. Episode 12900, average_reward -0.073488
Episode 12900: Total Loss of tensor([[8.0361]], grad_fn=<SubBackward0>)
[2022-11-05 15:13:41.257001] Process 1. Episode 12800, average_reward -0.067812
Episode 12800: Total Loss of tensor([[10.5116]], grad_fn=<SubBackward0>)
[2022-11-05 15:14:15.476778] Process 5. Episode 13150, average_reward -0.070114
Episode 13150: Total Loss of tensor([[4.1475]], grad_fn=<SubBackward0>)
[2022-11-05 15:14:50.653697] Process 2. Episode 13250, average_reward -0.075094
Episode 13250: Total Loss of tensor([[11.4005]], grad_fn=<SubBackward0>)
[2022-11-05 15:15:17.573099] Process 4. Episode 13850, average_reward -0.073141
Episode 13850: Total Loss of tensor([[10.5649]], grad_fn=<SubBackward0>)
[2022-11-05 15:15:33.765079] Process 0. Episode 13050, average_reward -0.071418
Episode 13050: Total Loss of tensor([[3.8707]], grad_fn=<SubBackward0>)
[2022-11-05 15:15:53.100615] Process 3. Episode 12950, average_reward -0.073436
Episode 12950: Total Loss of tensor([[-1.0206]], grad_fn=<SubBackward0>)
[2022-11-05 15:16:08.934568] Process 1. Episode 12850, average_reward -0.067626
Episode 12850: Total Loss of tensor([[2.6716]], grad_fn=<SubBackward0>)
[2022-11-05 15:16:39.808541] Process 5. Episode 13200, average_reward -0.070152
Episode 13200: Total Loss of tensor([[7.8197]], grad_fn=<SubBackward0>)
[2022-11-05 15:17:13.330973] Process 2. Episode 13300, average_reward -0.075338
Episode 13300: Total Loss of tensor([[4.8955]], grad_fn=<SubBackward0>)
[2022-11-05 15:17:46.901277] Process 4. Episode 13900, average_reward -0.073094
Episode 13900: Total Loss of tensor([[-39.4181]], grad_fn=<SubBackward0>)
[2022-11-05 15:18:12.675175] Process 0. Episode 13100, average_reward -0.071527
Episode 13100: Total Loss of tensor([[14.2306]], grad_fn=<SubBackward0>)
[2022-11-05 15:18:27.657371] Process 3. Episode 13000, average_reward -0.073385
Episode 13000: Total Loss of tensor([[3.5333]], grad_fn=<SubBackward0>)
[2022-11-05 15:18:37.887645] Process 1. Episode 12900, average_reward -0.067674
Episode 12900: Total Loss of tensor([[7.9081]], grad_fn=<SubBackward0>)
[2022-11-05 15:19:06.630296] Process 5. Episode 13250, average_reward -0.069887
Episode 13250: Total Loss of tensor([[-1.7954]], grad_fn=<SubBackward0>)
[2022-11-05 15:19:36.309999] Process 2. Episode 13350, average_reward -0.075206
Episode 13350: Total Loss of tensor([[-1.2825]], grad_fn=<SubBackward0>)
[2022-11-05 15:20:12.997390] Process 4. Episode 13950, average_reward -0.072903
Episode 13950: Total Loss of tensor([[-0.1010]], grad_fn=<SubBackward0>)
[2022-11-05 15:20:47.825117] Process 0. Episode 13150, average_reward -0.071559
Episode 13150: Total Loss of tensor([[-86.2302]], grad_fn=<SubBackward0>)
[2022-11-05 15:21:00.935367] Process 3. Episode 13050, average_reward -0.073410
Episode 13050: Total Loss of tensor([[11.1176]], grad_fn=<SubBackward0>)
[2022-11-05 15:21:07.602062] Process 1. Episode 12950, average_reward -0.067645
Episode 12950: Total Loss of tensor([[-125.7455]], grad_fn=<SubBackward0>)
[2022-11-05 15:21:45.680450] Process 5. Episode 13300, average_reward -0.070000
Episode 13300: Total Loss of tensor([[12.4953]], grad_fn=<SubBackward0>)
[2022-11-05 15:22:01.353743] Process 2. Episode 13400, average_reward -0.075224
Episode 13400: Total Loss of tensor([[-111.8237]], grad_fn=<SubBackward0>)
[2022-11-05 15:22:33.332308] Process 4. Episode 14000, average_reward -0.073000
Episode 14000: Total Loss of tensor([[12.5445]], grad_fn=<SubBackward0>)
[2022-11-05 15:23:17.412456] Process 0. Episode 13200, average_reward -0.071439
Episode 13200: Total Loss of tensor([[7.6199]], grad_fn=<SubBackward0>)
[2022-11-05 15:23:29.548123] Process 3. Episode 13100, average_reward -0.073282
Episode 13100: Total Loss of tensor([[4.1199]], grad_fn=<SubBackward0>)
[2022-11-05 15:23:51.995408] Process 1. Episode 13000, average_reward -0.067846
Episode 13000: Total Loss of tensor([[2.6903]], grad_fn=<SubBackward0>)
[2022-11-05 15:24:18.427083] Process 5. Episode 13350, average_reward -0.070037
Episode 13350: Total Loss of tensor([[1.3686]], grad_fn=<SubBackward0>)
[2022-11-05 15:24:20.576534] Process 2. Episode 13450, average_reward -0.075019
Episode 13450: Total Loss of tensor([[8.0379]], grad_fn=<SubBackward0>)
[2022-11-05 15:24:55.705054] Process 4. Episode 14050, average_reward -0.072811
Episode 14050: Total Loss of tensor([[5.9979]], grad_fn=<SubBackward0>)
[2022-11-05 15:25:42.001951] Process 0. Episode 13250, average_reward -0.071623
Episode 13250: Total Loss of tensor([[8.3181]], grad_fn=<SubBackward0>)
[2022-11-05 15:25:55.095732] Process 3. Episode 13150, average_reward -0.073080
Episode 13150: Total Loss of tensor([[-4.4635]], grad_fn=<SubBackward0>)
[2022-11-05 15:26:38.713377] Process 1. Episode 13050, average_reward -0.067739
Episode 13050: Total Loss of tensor([[13.6365]], grad_fn=<SubBackward0>)
[2022-11-05 15:26:40.602804] Process 2. Episode 13500, average_reward -0.075630
Episode 13500: Total Loss of tensor([[17.5367]], grad_fn=<SubBackward0>)
[2022-11-05 15:26:49.903328] Process 5. Episode 13400, average_reward -0.069851
Episode 13400: Total Loss of tensor([[16.8255]], grad_fn=<SubBackward0>)
[2022-11-05 15:27:15.578541] Process 4. Episode 14100, average_reward -0.072695
Episode 14100: Total Loss of tensor([[8.9783]], grad_fn=<SubBackward0>)
[2022-11-05 15:28:14.966438] Process 0. Episode 13300, average_reward -0.071504
Episode 13300: Total Loss of tensor([[0.9139]], grad_fn=<SubBackward0>)
[2022-11-05 15:28:36.115118] Process 3. Episode 13200, average_reward -0.073030
Episode 13200: Total Loss of tensor([[7.2480]], grad_fn=<SubBackward0>)
[2022-11-05 15:29:00.050707] Process 2. Episode 13550, average_reward -0.075498
Episode 13550: Total Loss of tensor([[-0.7287]], grad_fn=<SubBackward0>)
[2022-11-05 15:29:14.628231] Process 5. Episode 13450, average_reward -0.069740
Episode 13450: Total Loss of tensor([[7.1296]], grad_fn=<SubBackward0>)
[2022-11-05 15:29:15.008809] Process 1. Episode 13100, average_reward -0.067786
Episode 13100: Total Loss of tensor([[-73.2262]], grad_fn=<SubBackward0>)
[2022-11-05 15:29:35.996698] Process 4. Episode 14150, average_reward -0.072721
Episode 14150: Total Loss of tensor([[3.7338]], grad_fn=<SubBackward0>)
[2022-11-05 15:30:47.481833] Process 0. Episode 13350, average_reward -0.071610
Episode 13350: Total Loss of tensor([[2.1510]], grad_fn=<SubBackward0>)
[2022-11-05 15:31:17.681908] Process 3. Episode 13250, average_reward -0.073057
Episode 13250: Total Loss of tensor([[31.7556]], grad_fn=<SubBackward0>)
[2022-11-05 15:31:23.038330] Process 2. Episode 13600, average_reward -0.075662
Episode 13600: Total Loss of tensor([[3.5825]], grad_fn=<SubBackward0>)
[2022-11-05 15:31:45.058939] Process 5. Episode 13500, average_reward -0.069481
Episode 13500: Total Loss of tensor([[7.1314]], grad_fn=<SubBackward0>)
[2022-11-05 15:31:45.168003] Process 1. Episode 13150, average_reward -0.067605
Episode 13150: Total Loss of tensor([[13.5491]], grad_fn=<SubBackward0>)
[2022-11-05 15:31:58.743021] Process 4. Episode 14200, average_reward -0.072746
Episode 14200: Total Loss of tensor([[0.8621]], grad_fn=<SubBackward0>)
[2022-11-05 15:33:21.126311] Process 0. Episode 13400, average_reward -0.071567
Episode 13400: Total Loss of tensor([[-13.6165]], grad_fn=<SubBackward0>)
[2022-11-05 15:33:42.328652] Process 3. Episode 13300, average_reward -0.072857
Episode 13300: Total Loss of tensor([[4.7530]], grad_fn=<SubBackward0>)
[2022-11-05 15:33:46.292350] Process 2. Episode 13650, average_reward -0.075604
Episode 13650: Total Loss of tensor([[-107.0636]], grad_fn=<SubBackward0>)
[2022-11-05 15:34:09.830096] Process 1. Episode 13200, average_reward -0.067803
Episode 13200: Total Loss of tensor([[-17.4110]], grad_fn=<SubBackward0>)
[2022-11-05 15:34:14.434413] Process 5. Episode 13550, average_reward -0.069299
Episode 13550: Total Loss of tensor([[6.1482]], grad_fn=<SubBackward0>)
[2022-11-05 15:34:20.804396] Process 4. Episode 14250, average_reward -0.072702
Episode 14250: Total Loss of tensor([[15.0367]], grad_fn=<SubBackward0>)
[2022-11-05 15:35:56.035257] Process 0. Episode 13450, average_reward -0.071524
Episode 13450: Total Loss of tensor([[9.3903]], grad_fn=<SubBackward0>)
[2022-11-05 15:36:11.172486] Process 3. Episode 13350, average_reward -0.073034
Episode 13350: Total Loss of tensor([[-107.8834]], grad_fn=<SubBackward0>)
[2022-11-05 15:36:13.023729] Process 2. Episode 13700, average_reward -0.075474
Episode 13700: Total Loss of tensor([[4.8490]], grad_fn=<SubBackward0>)
[2022-11-05 15:36:40.403323] Process 5. Episode 13600, average_reward -0.069412
Episode 13600: Total Loss of tensor([[2.9423]], grad_fn=<SubBackward0>)
[2022-11-05 15:36:40.419587] Process 1. Episode 13250, average_reward -0.067623
Episode 13250: Total Loss of tensor([[7.6105]], grad_fn=<SubBackward0>)
[2022-11-05 15:36:44.413783] Process 4. Episode 14300, average_reward -0.072727
Episode 14300: Total Loss of tensor([[4.3453]], grad_fn=<SubBackward0>)
[2022-11-05 15:38:19.156791] Process 0. Episode 13500, average_reward -0.071259
Episode 13500: Total Loss of tensor([[7.1363]], grad_fn=<SubBackward0>)
[2022-11-05 15:38:33.449941] Process 2. Episode 13750, average_reward -0.075418
Episode 13750: Total Loss of tensor([[7.6796]], grad_fn=<SubBackward0>)
[2022-11-05 15:38:49.861149] Process 3. Episode 13400, average_reward -0.073134
Episode 13400: Total Loss of tensor([[-54.9889]], grad_fn=<SubBackward0>)
[2022-11-05 15:39:06.062480] Process 4. Episode 14350, average_reward -0.072892
Episode 14350: Total Loss of tensor([[-2.5443]], grad_fn=<SubBackward0>)
[2022-11-05 15:39:12.036737] Process 5. Episode 13650, average_reward -0.069231
Episode 13650: Total Loss of tensor([[-14.4647]], grad_fn=<SubBackward0>)
[2022-11-05 15:39:14.958210] Process 1. Episode 13300, average_reward -0.067895
Episode 13300: Total Loss of tensor([[14.0180]], grad_fn=<SubBackward0>)
[2022-11-05 15:40:50.541806] Process 0. Episode 13550, average_reward -0.071070
Episode 13550: Total Loss of tensor([[11.8643]], grad_fn=<SubBackward0>)
[2022-11-05 15:40:55.827225] Process 2. Episode 13800, average_reward -0.075435
Episode 13800: Total Loss of tensor([[15.2986]], grad_fn=<SubBackward0>)
[2022-11-05 15:41:19.134545] Process 3. Episode 13450, average_reward -0.073086
Episode 13450: Total Loss of tensor([[2.7601]], grad_fn=<SubBackward0>)
[2022-11-05 15:41:22.937187] Process 4. Episode 14400, average_reward -0.072917
Episode 14400: Total Loss of tensor([[5.2928]], grad_fn=<SubBackward0>)
[2022-11-05 15:41:46.519929] Process 5. Episode 13700, average_reward -0.069270
Episode 13700: Total Loss of tensor([[-69.5115]], grad_fn=<SubBackward0>)
[2022-11-05 15:41:57.482351] Process 1. Episode 13350, average_reward -0.067940
Episode 13350: Total Loss of tensor([[-123.7394]], grad_fn=<SubBackward0>)
[2022-11-05 15:43:18.405452] Process 2. Episode 13850, average_reward -0.075235
Episode 13850: Total Loss of tensor([[-29.8381]], grad_fn=<SubBackward0>)
[2022-11-05 15:43:24.767594] Process 0. Episode 13600, average_reward -0.071029
Episode 13600: Total Loss of tensor([[14.3346]], grad_fn=<SubBackward0>)
[2022-11-05 15:43:44.098781] Process 4. Episode 14450, average_reward -0.073010
Episode 14450: Total Loss of tensor([[4.6758]], grad_fn=<SubBackward0>)
[2022-11-05 15:43:53.078343] Process 3. Episode 13500, average_reward -0.073185
Episode 13500: Total Loss of tensor([[-9.5245]], grad_fn=<SubBackward0>)
[2022-11-05 15:44:23.121097] Process 5. Episode 13750, average_reward -0.069527
Episode 13750: Total Loss of tensor([[8.2536]], grad_fn=<SubBackward0>)
[2022-11-05 15:44:29.130661] Process 1. Episode 13400, average_reward -0.068134
Episode 13400: Total Loss of tensor([[-46.3522]], grad_fn=<SubBackward0>)
[2022-11-05 15:45:46.400053] Process 2. Episode 13900, average_reward -0.075180
Episode 13900: Total Loss of tensor([[5.6745]], grad_fn=<SubBackward0>)
[2022-11-05 15:46:01.516405] Process 0. Episode 13650, average_reward -0.071062
Episode 13650: Total Loss of tensor([[17.3636]], grad_fn=<SubBackward0>)
[2022-11-05 15:46:15.985010] Process 4. Episode 14500, average_reward -0.073172
Episode 14500: Total Loss of tensor([[-1.4960]], grad_fn=<SubBackward0>)
[2022-11-05 15:46:25.644596] Process 3. Episode 13550, average_reward -0.073210
Episode 13550: Total Loss of tensor([[1.0017]], grad_fn=<SubBackward0>)
[2022-11-05 15:46:55.929797] Process 1. Episode 13450, average_reward -0.068104
Episode 13450: Total Loss of tensor([[15.5992]], grad_fn=<SubBackward0>)
[2022-11-05 15:46:56.301708] Process 5. Episode 13800, average_reward -0.069420
Episode 13800: Total Loss of tensor([[9.1456]], grad_fn=<SubBackward0>)
[2022-11-05 15:48:13.924904] Process 2. Episode 13950, average_reward -0.075269
Episode 13950: Total Loss of tensor([[-86.3737]], grad_fn=<SubBackward0>)
[2022-11-05 15:48:29.230829] Process 0. Episode 13700, average_reward -0.070949
Episode 13700: Total Loss of tensor([[-0.1515]], grad_fn=<SubBackward0>)
[2022-11-05 15:48:43.602540] Process 4. Episode 14550, average_reward -0.072921
Episode 14550: Total Loss of tensor([[9.6265]], grad_fn=<SubBackward0>)
[2022-11-05 15:49:05.249813] Process 3. Episode 13600, average_reward -0.073162
Episode 13600: Total Loss of tensor([[13.9365]], grad_fn=<SubBackward0>)
[2022-11-05 15:49:35.055562] Process 5. Episode 13850, average_reward -0.069386
Episode 13850: Total Loss of tensor([[7.9214]], grad_fn=<SubBackward0>)
[2022-11-05 15:49:38.674487] Process 1. Episode 13500, average_reward -0.068296
Episode 13500: Total Loss of tensor([[-7.5573]], grad_fn=<SubBackward0>)
[2022-11-05 15:50:39.204918] Process 2. Episode 14000, average_reward -0.075214
Episode 14000: Total Loss of tensor([[0.5508]], grad_fn=<SubBackward0>)
[2022-11-05 15:51:00.998315] Process 0. Episode 13750, average_reward -0.070836
Episode 13750: Total Loss of tensor([[-110.1088]], grad_fn=<SubBackward0>)
[2022-11-05 15:51:12.157961] Process 4. Episode 14600, average_reward -0.072808
Episode 14600: Total Loss of tensor([[7.6549]], grad_fn=<SubBackward0>)
[2022-11-05 15:51:34.479963] Process 3. Episode 13650, average_reward -0.073553
Episode 13650: Total Loss of tensor([[13.5607]], grad_fn=<SubBackward0>)
[2022-11-05 15:52:07.236756] Process 5. Episode 13900, average_reward -0.069640
Episode 13900: Total Loss of tensor([[-8.6751]], grad_fn=<SubBackward0>)
[2022-11-05 15:52:16.728097] Process 1. Episode 13550, average_reward -0.068339
Episode 13550: Total Loss of tensor([[-1.4196]], grad_fn=<SubBackward0>)
[2022-11-05 15:53:04.703431] Process 2. Episode 14050, average_reward -0.075374
Episode 14050: Total Loss of tensor([[12.4285]], grad_fn=<SubBackward0>)
[2022-11-05 15:53:29.329549] Process 0. Episode 13800, average_reward -0.070797
Episode 13800: Total Loss of tensor([[10.9453]], grad_fn=<SubBackward0>)
[2022-11-05 15:53:38.451261] Process 4. Episode 14650, average_reward -0.072696
Episode 14650: Total Loss of tensor([[6.9979]], grad_fn=<SubBackward0>)
[2022-11-05 15:54:11.567675] Process 3. Episode 13700, average_reward -0.073431
Episode 13700: Total Loss of tensor([[-9.1673]], grad_fn=<SubBackward0>)
[2022-11-05 15:54:38.741044] Process 5. Episode 13950, average_reward -0.069606
Episode 13950: Total Loss of tensor([[6.8797]], grad_fn=<SubBackward0>)
[2022-11-05 15:54:48.520575] Process 1. Episode 13600, average_reward -0.068235
Episode 13600: Total Loss of tensor([[-12.6596]], grad_fn=<SubBackward0>)
[2022-11-05 15:55:25.162490] Process 2. Episode 14100, average_reward -0.075319
Episode 14100: Total Loss of tensor([[7.1237]], grad_fn=<SubBackward0>)
[2022-11-05 15:55:59.841470] Process 4. Episode 14700, average_reward -0.072721
Episode 14700: Total Loss of tensor([[0.9573]], grad_fn=<SubBackward0>)
[2022-11-05 15:56:01.986250] Process 0. Episode 13850, average_reward -0.070903
Episode 13850: Total Loss of tensor([[-36.0328]], grad_fn=<SubBackward0>)
[2022-11-05 15:56:45.109807] Process 3. Episode 13750, average_reward -0.073455
Episode 13750: Total Loss of tensor([[-5.7215]], grad_fn=<SubBackward0>)
[2022-11-05 15:57:01.083418] Process 5. Episode 14000, average_reward -0.069786
Episode 14000: Total Loss of tensor([[2.5642]], grad_fn=<SubBackward0>)
[2022-11-05 15:57:23.775553] Process 1. Episode 13650, average_reward -0.068205
Episode 13650: Total Loss of tensor([[-129.3231]], grad_fn=<SubBackward0>)
[2022-11-05 15:57:50.586993] Process 2. Episode 14150, average_reward -0.075265
Episode 14150: Total Loss of tensor([[-4.3030]], grad_fn=<SubBackward0>)
[2022-11-05 15:58:20.381460] Process 4. Episode 14750, average_reward -0.072746
Episode 14750: Total Loss of tensor([[7.3124]], grad_fn=<SubBackward0>)
[2022-11-05 15:58:35.827640] Process 0. Episode 13900, average_reward -0.070791
Episode 13900: Total Loss of tensor([[14.2912]], grad_fn=<SubBackward0>)
[2022-11-05 15:59:07.934419] Process 3. Episode 13800, average_reward -0.073261
Episode 13800: Total Loss of tensor([[5.1104]], grad_fn=<SubBackward0>)
[2022-11-05 15:59:27.359468] Process 5. Episode 14050, average_reward -0.069751
Episode 14050: Total Loss of tensor([[12.2990]], grad_fn=<SubBackward0>)
[2022-11-05 16:00:05.723922] Process 1. Episode 13700, average_reward -0.068102
Episode 13700: Total Loss of tensor([[-1.7561]], grad_fn=<SubBackward0>)
[2022-11-05 16:00:18.247299] Process 2. Episode 14200, average_reward -0.075211
Episode 14200: Total Loss of tensor([[-1.8723]], grad_fn=<SubBackward0>)
[2022-11-05 16:00:40.186874] Process 4. Episode 14800, average_reward -0.072635
Episode 14800: Total Loss of tensor([[0.1377]], grad_fn=<SubBackward0>)
[2022-11-05 16:01:08.237638] Process 0. Episode 13950, average_reward -0.070753
Episode 13950: Total Loss of tensor([[1.7175]], grad_fn=<SubBackward0>)
[2022-11-05 16:01:39.608780] Process 3. Episode 13850, average_reward -0.072996
Episode 13850: Total Loss of tensor([[9.1851]], grad_fn=<SubBackward0>)
[2022-11-05 16:01:57.078235] Process 5. Episode 14100, average_reward -0.069574
Episode 14100: Total Loss of tensor([[2.2108]], grad_fn=<SubBackward0>)
[2022-11-05 16:02:39.219315] Process 2. Episode 14250, average_reward -0.075158
Episode 14250: Total Loss of tensor([[4.0574]], grad_fn=<SubBackward0>)
[2022-11-05 16:02:49.642270] Process 1. Episode 13750, average_reward -0.068364
Episode 13750: Total Loss of tensor([[-51.6555]], grad_fn=<SubBackward0>)
[2022-11-05 16:02:59.814057] Process 4. Episode 14850, average_reward -0.072660
Episode 14850: Total Loss of tensor([[13.8092]], grad_fn=<SubBackward0>)
[2022-11-05 16:03:35.284190] Process 0. Episode 14000, average_reward -0.070571
Episode 14000: Total Loss of tensor([[0.6722]], grad_fn=<SubBackward0>)
[2022-11-05 16:04:05.493949] Process 3. Episode 13900, average_reward -0.073094
Episode 13900: Total Loss of tensor([[7.1772]], grad_fn=<SubBackward0>)
[2022-11-05 16:04:22.472930] Process 5. Episode 14150, average_reward -0.069753
Episode 14150: Total Loss of tensor([[5.4987]], grad_fn=<SubBackward0>)
[2022-11-05 16:05:13.137397] Process 2. Episode 14300, average_reward -0.075035
Episode 14300: Total Loss of tensor([[14.4738]], grad_fn=<SubBackward0>)
[2022-11-05 16:05:16.734321] Process 1. Episode 13800, average_reward -0.068551
Episode 13800: Total Loss of tensor([[7.9320]], grad_fn=<SubBackward0>)
[2022-11-05 16:05:20.733814] Process 4. Episode 14900, average_reward -0.072550
Episode 14900: Total Loss of tensor([[4.6136]], grad_fn=<SubBackward0>)
[2022-11-05 16:06:08.672619] Process 0. Episode 14050, average_reward -0.070676
Episode 14050: Total Loss of tensor([[12.5710]], grad_fn=<SubBackward0>)
[2022-11-05 16:06:34.407390] Process 3. Episode 13950, average_reward -0.073477
Episode 13950: Total Loss of tensor([[-8.5295]], grad_fn=<SubBackward0>)
[2022-11-05 16:06:49.429091] Process 5. Episode 14200, average_reward -0.069648
Episode 14200: Total Loss of tensor([[5.1126]], grad_fn=<SubBackward0>)
[2022-11-05 16:07:41.771740] Process 4. Episode 14950, average_reward -0.072441
Episode 14950: Total Loss of tensor([[12.4843]], grad_fn=<SubBackward0>)
[2022-11-05 16:07:45.959709] Process 2. Episode 14350, average_reward -0.075261
Episode 14350: Total Loss of tensor([[-71.5955]], grad_fn=<SubBackward0>)
[2022-11-05 16:07:57.653418] Process 1. Episode 13850, average_reward -0.068664
Episode 13850: Total Loss of tensor([[12.2717]], grad_fn=<SubBackward0>)
[2022-11-05 16:08:47.541651] Process 0. Episode 14100, average_reward -0.070709
Episode 14100: Total Loss of tensor([[6.0056]], grad_fn=<SubBackward0>)
[2022-11-05 16:09:06.043886] Process 3. Episode 14000, average_reward -0.073571
Episode 14000: Total Loss of tensor([[-17.7468]], grad_fn=<SubBackward0>)
[2022-11-05 16:09:20.029398] Process 5. Episode 14250, average_reward -0.069684
Episode 14250: Total Loss of tensor([[22.8547]], grad_fn=<SubBackward0>)
[2022-11-05 16:10:00.681268] Process 4. Episode 15000, average_reward -0.072467
Episode 15000: Total Loss of tensor([[13.7298]], grad_fn=<SubBackward0>)
[2022-11-05 16:10:07.161351] Process 2. Episode 14400, average_reward -0.075278
Episode 14400: Total Loss of tensor([[12.5169]], grad_fn=<SubBackward0>)
[2022-11-05 16:10:27.664034] Process 1. Episode 13900, average_reward -0.068705
Episode 13900: Total Loss of tensor([[-17.9584]], grad_fn=<SubBackward0>)
[2022-11-05 16:11:21.303238] Process 0. Episode 14150, average_reward -0.070601
Episode 14150: Total Loss of tensor([[-0.1884]], grad_fn=<SubBackward0>)
[2022-11-05 16:11:30.746475] Process 3. Episode 14050, average_reward -0.073523
Episode 14050: Total Loss of tensor([[-1.9232]], grad_fn=<SubBackward0>)
[2022-11-05 16:11:55.785497] Process 5. Episode 14300, average_reward -0.069510
Episode 14300: Total Loss of tensor([[-1.2029]], grad_fn=<SubBackward0>)
[2022-11-05 16:12:28.767994] Process 4. Episode 15050, average_reward -0.072359
Episode 15050: Total Loss of tensor([[-2.3717]], grad_fn=<SubBackward0>)
[2022-11-05 16:12:32.207978] Process 2. Episode 14450, average_reward -0.075363
Episode 14450: Total Loss of tensor([[-3.8658]], grad_fn=<SubBackward0>)
[2022-11-05 16:12:57.274664] Process 1. Episode 13950, average_reward -0.068602
Episode 13950: Total Loss of tensor([[13.6715]], grad_fn=<SubBackward0>)
[2022-11-05 16:13:47.442320] Process 0. Episode 14200, average_reward -0.070493
Episode 14200: Total Loss of tensor([[4.3524]], grad_fn=<SubBackward0>)
[2022-11-05 16:14:00.562479] Process 3. Episode 14100, average_reward -0.073333
Episode 14100: Total Loss of tensor([[6.9510]], grad_fn=<SubBackward0>)
[2022-11-05 16:14:22.011894] Process 5. Episode 14350, average_reward -0.069477
Episode 14350: Total Loss of tensor([[4.9781]], grad_fn=<SubBackward0>)
[2022-11-05 16:14:52.295197] Process 4. Episode 15100, average_reward -0.072318
Episode 15100: Total Loss of tensor([[6.8731]], grad_fn=<SubBackward0>)
[2022-11-05 16:14:57.173949] Process 2. Episode 14500, average_reward -0.075379
Episode 14500: Total Loss of tensor([[-0.2389]], grad_fn=<SubBackward0>)
[2022-11-05 16:15:29.319557] Process 1. Episode 14000, average_reward -0.068571
Episode 14000: Total Loss of tensor([[8.2760]], grad_fn=<SubBackward0>)
[2022-11-05 16:16:14.514921] Process 0. Episode 14250, average_reward -0.070456
Episode 14250: Total Loss of tensor([[3.1856]], grad_fn=<SubBackward0>)
[2022-11-05 16:16:27.104467] Process 3. Episode 14150, average_reward -0.073216
Episode 14150: Total Loss of tensor([[-18.1978]], grad_fn=<SubBackward0>)
[2022-11-05 16:16:49.133146] Process 5. Episode 14400, average_reward -0.069444
Episode 14400: Total Loss of tensor([[11.1792]], grad_fn=<SubBackward0>)
[2022-11-05 16:17:19.794034] Process 4. Episode 15150, average_reward -0.072277
Episode 15150: Total Loss of tensor([[4.0277]], grad_fn=<SubBackward0>)
[2022-11-05 16:17:22.470886] Process 2. Episode 14550, average_reward -0.075533
Episode 14550: Total Loss of tensor([[14.4816]], grad_fn=<SubBackward0>)
[2022-11-05 16:18:00.913065] Process 1. Episode 14050, average_reward -0.068470
Episode 14050: Total Loss of tensor([[2.9456]], grad_fn=<SubBackward0>)
[2022-11-05 16:18:53.892061] Process 0. Episode 14300, average_reward -0.070629
Episode 14300: Total Loss of tensor([[10.8712]], grad_fn=<SubBackward0>)
[2022-11-05 16:19:03.179329] Process 3. Episode 14200, average_reward -0.073380
Episode 14200: Total Loss of tensor([[7.3083]], grad_fn=<SubBackward0>)
[2022-11-05 16:19:17.814700] Process 5. Episode 14450, average_reward -0.069273
Episode 14450: Total Loss of tensor([[1.6062]], grad_fn=<SubBackward0>)
[2022-11-05 16:19:41.990094] Process 4. Episode 15200, average_reward -0.072171
Episode 15200: Total Loss of tensor([[7.1946]], grad_fn=<SubBackward0>)
[2022-11-05 16:19:49.456401] Process 2. Episode 14600, average_reward -0.075411
Episode 14600: Total Loss of tensor([[0.7133]], grad_fn=<SubBackward0>)
[2022-11-05 16:20:32.376635] Process 1. Episode 14100, average_reward -0.068652
Episode 14100: Total Loss of tensor([[3.5625]], grad_fn=<SubBackward0>)
[2022-11-05 16:21:26.312752] Process 3. Episode 14250, average_reward -0.073614
Episode 14250: Total Loss of tensor([[17.6500]], grad_fn=<SubBackward0>)
[2022-11-05 16:21:26.573994] Process 0. Episode 14350, average_reward -0.070732
Episode 14350: Total Loss of tensor([[7.0515]], grad_fn=<SubBackward0>)
[2022-11-05 16:21:45.087503] Process 5. Episode 14500, average_reward -0.069034
Episode 14500: Total Loss of tensor([[12.1403]], grad_fn=<SubBackward0>)
[2022-11-05 16:22:03.932990] Process 4. Episode 15250, average_reward -0.072197
Episode 15250: Total Loss of tensor([[9.9763]], grad_fn=<SubBackward0>)
[2022-11-05 16:22:15.161079] Process 2. Episode 14650, average_reward -0.075495
Episode 14650: Total Loss of tensor([[2.5168]], grad_fn=<SubBackward0>)
[2022-11-05 16:23:11.386147] Process 1. Episode 14150, average_reward -0.068622
Episode 14150: Total Loss of tensor([[-0.5585]], grad_fn=<SubBackward0>)
[2022-11-05 16:23:51.706012] Process 0. Episode 14400, average_reward -0.070694
Episode 14400: Total Loss of tensor([[5.2130]], grad_fn=<SubBackward0>)
[2022-11-05 16:23:56.124733] Process 3. Episode 14300, average_reward -0.073566
Episode 14300: Total Loss of tensor([[-16.3638]], grad_fn=<SubBackward0>)
[2022-11-05 16:24:10.659815] Process 5. Episode 14550, average_reward -0.068866
Episode 14550: Total Loss of tensor([[7.1361]], grad_fn=<SubBackward0>)
[2022-11-05 16:24:26.964624] Process 4. Episode 15300, average_reward -0.072157
Episode 15300: Total Loss of tensor([[4.8925]], grad_fn=<SubBackward0>)
[2022-11-05 16:24:40.305962] Process 2. Episode 14700, average_reward -0.075442
Episode 14700: Total Loss of tensor([[-1.2058]], grad_fn=<SubBackward0>)
[2022-11-05 16:25:41.855578] Process 1. Episode 14200, average_reward -0.068521
Episode 14200: Total Loss of tensor([[13.9844]], grad_fn=<SubBackward0>)
[2022-11-05 16:26:22.273170] Process 3. Episode 14350, average_reward -0.073728
Episode 14350: Total Loss of tensor([[10.2379]], grad_fn=<SubBackward0>)
[2022-11-05 16:26:22.386896] Process 0. Episode 14450, average_reward -0.070796
Episode 14450: Total Loss of tensor([[-100.4178]], grad_fn=<SubBackward0>)
[2022-11-05 16:26:53.922571] Process 5. Episode 14600, average_reward -0.068767
Episode 14600: Total Loss of tensor([[10.0213]], grad_fn=<SubBackward0>)
[2022-11-05 16:26:57.249907] Process 4. Episode 15350, average_reward -0.072052
Episode 15350: Total Loss of tensor([[8.7391]], grad_fn=<SubBackward0>)
[2022-11-05 16:27:06.187132] Process 2. Episode 14750, average_reward -0.075525
Episode 14750: Total Loss of tensor([[-15.9530]], grad_fn=<SubBackward0>)
[2022-11-05 16:28:20.499745] Process 1. Episode 14250, average_reward -0.068491
Episode 14250: Total Loss of tensor([[2.2579]], grad_fn=<SubBackward0>)
[2022-11-05 16:28:47.609640] Process 3. Episode 14400, average_reward -0.073611
Episode 14400: Total Loss of tensor([[4.7641]], grad_fn=<SubBackward0>)
[2022-11-05 16:28:48.143715] Process 0. Episode 14500, average_reward -0.070690
Episode 14500: Total Loss of tensor([[3.4424]], grad_fn=<SubBackward0>)
[2022-11-05 16:29:18.905438] Process 4. Episode 15400, average_reward -0.072208
Episode 15400: Total Loss of tensor([[3.4938]], grad_fn=<SubBackward0>)
[2022-11-05 16:29:25.411106] Process 5. Episode 14650, average_reward -0.068737
Episode 14650: Total Loss of tensor([[11.3920]], grad_fn=<SubBackward0>)
[2022-11-05 16:29:31.000833] Process 2. Episode 14800, average_reward -0.075473
Episode 14800: Total Loss of tensor([[-31.7746]], grad_fn=<SubBackward0>)
[2022-11-05 16:30:53.272238] Process 1. Episode 14300, average_reward -0.068531
Episode 14300: Total Loss of tensor([[-119.1348]], grad_fn=<SubBackward0>)
[2022-11-05 16:31:14.803683] Process 3. Episode 14450, average_reward -0.073564
Episode 14450: Total Loss of tensor([[0.8190]], grad_fn=<SubBackward0>)
[2022-11-05 16:31:19.606204] Process 0. Episode 14550, average_reward -0.070653
Episode 14550: Total Loss of tensor([[19.5254]], grad_fn=<SubBackward0>)
[2022-11-05 16:31:42.425758] Process 4. Episode 15450, average_reward -0.072492
Episode 15450: Total Loss of tensor([[8.4677]], grad_fn=<SubBackward0>)
[2022-11-05 16:31:54.116736] Process 5. Episode 14700, average_reward -0.068707
Episode 14700: Total Loss of tensor([[1.2109]], grad_fn=<SubBackward0>)
[2022-11-05 16:31:55.882268] Process 2. Episode 14850, average_reward -0.075488
Episode 14850: Total Loss of tensor([[16.9957]], grad_fn=<SubBackward0>)
[2022-11-05 16:33:24.066572] Process 1. Episode 14350, average_reward -0.068641
Episode 14350: Total Loss of tensor([[12.0325]], grad_fn=<SubBackward0>)
[2022-11-05 16:33:43.590005] Process 3. Episode 14500, average_reward -0.073517
Episode 14500: Total Loss of tensor([[-8.0949]], grad_fn=<SubBackward0>)
[2022-11-05 16:33:48.195628] Process 0. Episode 14600, average_reward -0.070753
Episode 14600: Total Loss of tensor([[-10.6130]], grad_fn=<SubBackward0>)
[2022-11-05 16:34:07.194066] Process 4. Episode 15500, average_reward -0.072516
Episode 15500: Total Loss of tensor([[15.3820]], grad_fn=<SubBackward0>)
[2022-11-05 16:34:22.434264] Process 2. Episode 14900, average_reward -0.075436
Episode 14900: Total Loss of tensor([[0.7858]], grad_fn=<SubBackward0>)
[2022-11-05 16:34:23.669882] Process 5. Episode 14750, average_reward -0.068881
Episode 14750: Total Loss of tensor([[12.4001]], grad_fn=<SubBackward0>)
[2022-11-05 16:35:54.173695] Process 1. Episode 14400, average_reward -0.068542
Episode 14400: Total Loss of tensor([[10.0250]], grad_fn=<SubBackward0>)
[2022-11-05 16:36:08.762873] Process 3. Episode 14550, average_reward -0.073333
Episode 14550: Total Loss of tensor([[4.9410]], grad_fn=<SubBackward0>)
[2022-11-05 16:36:15.396515] Process 0. Episode 14650, average_reward -0.071058
Episode 14650: Total Loss of tensor([[-9.9040]], grad_fn=<SubBackward0>)
[2022-11-05 16:36:28.656008] Process 4. Episode 15550, average_reward -0.072540
Episode 15550: Total Loss of tensor([[9.6158]], grad_fn=<SubBackward0>)
[2022-11-05 16:36:48.637352] Process 5. Episode 14800, average_reward -0.068919
Episode 14800: Total Loss of tensor([[20.2938]], grad_fn=<SubBackward0>)
[2022-11-05 16:36:56.465478] Process 2. Episode 14950, average_reward -0.075585
Episode 14950: Total Loss of tensor([[-1.6493]], grad_fn=<SubBackward0>)
[2022-11-05 16:38:24.152723] Process 1. Episode 14450, average_reward -0.068858
Episode 14450: Total Loss of tensor([[11.4606]], grad_fn=<SubBackward0>)
[2022-11-05 16:38:38.214907] Process 0. Episode 14700, average_reward -0.071020
Episode 14700: Total Loss of tensor([[-1.6490]], grad_fn=<SubBackward0>)
[2022-11-05 16:38:44.393552] Process 3. Episode 14600, average_reward -0.073356
Episode 14600: Total Loss of tensor([[12.5939]], grad_fn=<SubBackward0>)
[2022-11-05 16:38:48.591968] Process 4. Episode 15600, average_reward -0.072564
Episode 15600: Total Loss of tensor([[5.1770]], grad_fn=<SubBackward0>)
[2022-11-05 16:39:10.181905] Process 5. Episode 14850, average_reward -0.068889
Episode 14850: Total Loss of tensor([[-26.2299]], grad_fn=<SubBackward0>)
[2022-11-05 16:39:22.772002] Process 2. Episode 15000, average_reward -0.075800
Episode 15000: Total Loss of tensor([[11.1962]], grad_fn=<SubBackward0>)
[2022-11-05 16:40:58.888022] Process 1. Episode 14500, average_reward -0.068897
Episode 14500: Total Loss of tensor([[21.8302]], grad_fn=<SubBackward0>)
[2022-11-05 16:41:05.950177] Process 0. Episode 14750, average_reward -0.071186
Episode 14750: Total Loss of tensor([[20.0463]], grad_fn=<SubBackward0>)
[2022-11-05 16:41:10.856604] Process 4. Episode 15650, average_reward -0.072716
Episode 15650: Total Loss of tensor([[19.4762]], grad_fn=<SubBackward0>)
[2022-11-05 16:41:16.376281] Process 3. Episode 14650, average_reward -0.073447
Episode 14650: Total Loss of tensor([[18.3662]], grad_fn=<SubBackward0>)
[2022-11-05 16:41:37.586647] Process 5. Episode 14900, average_reward -0.068926
Episode 14900: Total Loss of tensor([[27.2255]], grad_fn=<SubBackward0>)
[2022-11-05 16:41:55.546587] Process 2. Episode 15050, average_reward -0.075880
Episode 15050: Total Loss of tensor([[5.3579]], grad_fn=<SubBackward0>)
[2022-11-05 16:43:31.548075] Process 0. Episode 14800, average_reward -0.071351
Episode 14800: Total Loss of tensor([[17.4765]], grad_fn=<SubBackward0>)
[2022-11-05 16:43:34.313753] Process 1. Episode 14550, average_reward -0.068729
Episode 14550: Total Loss of tensor([[21.5742]], grad_fn=<SubBackward0>)
[2022-11-05 16:43:38.704216] Process 4. Episode 15700, average_reward -0.072739
Episode 15700: Total Loss of tensor([[25.2852]], grad_fn=<SubBackward0>)
[2022-11-05 16:43:44.058932] Process 3. Episode 14700, average_reward -0.073469
Episode 14700: Total Loss of tensor([[-7.7622]], grad_fn=<SubBackward0>)
[2022-11-05 16:44:04.206458] Process 5. Episode 14950, average_reward -0.069097
Episode 14950: Total Loss of tensor([[11.9553]], grad_fn=<SubBackward0>)
[2022-11-05 16:44:19.034141] Process 2. Episode 15100, average_reward -0.076026
Episode 15100: Total Loss of tensor([[7.3360]], grad_fn=<SubBackward0>)
[2022-11-05 16:45:59.209827] Process 0. Episode 14850, average_reward -0.071380
Episode 14850: Total Loss of tensor([[5.5563]], grad_fn=<SubBackward0>)
[2022-11-05 16:46:02.706206] Process 4. Episode 15750, average_reward -0.072762
Episode 15750: Total Loss of tensor([[-66.5694]], grad_fn=<SubBackward0>)
[2022-11-05 16:46:04.619161] Process 1. Episode 14600, average_reward -0.068630
Episode 14600: Total Loss of tensor([[3.0631]], grad_fn=<SubBackward0>)
[2022-11-05 16:46:25.216859] Process 3. Episode 14750, average_reward -0.073356
Episode 14750: Total Loss of tensor([[-14.3598]], grad_fn=<SubBackward0>)
[2022-11-05 16:46:41.672542] Process 2. Episode 15150, average_reward -0.076106
Episode 15150: Total Loss of tensor([[6.7727]], grad_fn=<SubBackward0>)
[2022-11-05 16:46:42.345691] Process 5. Episode 15000, average_reward -0.069200
Episode 15000: Total Loss of tensor([[15.1891]], grad_fn=<SubBackward0>)
[2022-11-05 16:48:23.281434] Process 4. Episode 15800, average_reward -0.072658
Episode 15800: Total Loss of tensor([[2.7500]], grad_fn=<SubBackward0>)
[2022-11-05 16:48:30.609916] Process 0. Episode 14900, average_reward -0.071275
Episode 14900: Total Loss of tensor([[-0.7331]], grad_fn=<SubBackward0>)
[2022-11-05 16:48:44.035324] Process 1. Episode 14650, average_reward -0.068601
Episode 14650: Total Loss of tensor([[5.4284]], grad_fn=<SubBackward0>)
[2022-11-05 16:49:01.118748] Process 2. Episode 15200, average_reward -0.076053
Episode 15200: Total Loss of tensor([[11.2782]], grad_fn=<SubBackward0>)
[2022-11-05 16:49:05.179115] Process 3. Episode 14800, average_reward -0.073311
Episode 14800: Total Loss of tensor([[7.3332]], grad_fn=<SubBackward0>)
[2022-11-05 16:49:05.721645] Process 5. Episode 15050, average_reward -0.069103
Episode 15050: Total Loss of tensor([[7.9202]], grad_fn=<SubBackward0>)
[2022-11-05 16:50:41.087641] Process 4. Episode 15850, average_reward -0.072555
Episode 15850: Total Loss of tensor([[1.4890]], grad_fn=<SubBackward0>)
[2022-11-05 16:51:10.210570] Process 0. Episode 14950, average_reward -0.071371
Episode 14950: Total Loss of tensor([[11.8583]], grad_fn=<SubBackward0>)
[2022-11-05 16:51:20.433562] Process 2. Episode 15250, average_reward -0.076197
Episode 15250: Total Loss of tensor([[9.2544]], grad_fn=<SubBackward0>)
[2022-11-05 16:51:27.522260] Process 1. Episode 14700, average_reward -0.068707
Episode 14700: Total Loss of tensor([[4.3996]], grad_fn=<SubBackward0>)
[2022-11-05 16:51:30.776410] Process 5. Episode 15100, average_reward -0.069073
Episode 15100: Total Loss of tensor([[5.0902]], grad_fn=<SubBackward0>)
[2022-11-05 16:51:42.880972] Process 3. Episode 14850, average_reward -0.073333
Episode 14850: Total Loss of tensor([[-91.8495]], grad_fn=<SubBackward0>)
[2022-11-05 16:52:58.536665] Process 4. Episode 15900, average_reward -0.072579
Episode 15900: Total Loss of tensor([[-21.3202]], grad_fn=<SubBackward0>)
[2022-11-05 16:53:36.338941] Process 0. Episode 15000, average_reward -0.071467
Episode 15000: Total Loss of tensor([[11.0266]], grad_fn=<SubBackward0>)
[2022-11-05 16:53:44.050117] Process 2. Episode 15300, average_reward -0.076209
Episode 15300: Total Loss of tensor([[-97.0746]], grad_fn=<SubBackward0>)
[2022-11-05 16:53:55.891670] Process 5. Episode 15150, average_reward -0.069175
Episode 15150: Total Loss of tensor([[14.6635]], grad_fn=<SubBackward0>)
[2022-11-05 16:53:59.776905] Process 1. Episode 14750, average_reward -0.068610
Episode 14750: Total Loss of tensor([[3.5137]], grad_fn=<SubBackward0>)
[2022-11-05 16:54:23.648482] Process 3. Episode 14900, average_reward -0.073289
Episode 14900: Total Loss of tensor([[7.0712]], grad_fn=<SubBackward0>)
[2022-11-05 16:55:20.844287] Process 4. Episode 15950, average_reward -0.072539
Episode 15950: Total Loss of tensor([[1.2591]], grad_fn=<SubBackward0>)
[2022-11-05 16:56:07.918185] Process 0. Episode 15050, average_reward -0.071362
Episode 15050: Total Loss of tensor([[9.1869]], grad_fn=<SubBackward0>)
[2022-11-05 16:56:11.035192] Process 2. Episode 15350, average_reward -0.076091
Episode 15350: Total Loss of tensor([[17.1543]], grad_fn=<SubBackward0>)
[2022-11-05 16:56:20.421891] Process 5. Episode 15200, average_reward -0.069276
Episode 15200: Total Loss of tensor([[10.2792]], grad_fn=<SubBackward0>)
[2022-11-05 16:56:28.279056] Process 1. Episode 14800, average_reward -0.068446
Episode 14800: Total Loss of tensor([[9.0480]], grad_fn=<SubBackward0>)
[2022-11-05 16:56:58.540175] Process 3. Episode 14950, average_reward -0.073244
Episode 14950: Total Loss of tensor([[8.9067]], grad_fn=<SubBackward0>)
[2022-11-05 16:57:46.889421] Process 4. Episode 16000, average_reward -0.072563
Episode 16000: Total Loss of tensor([[25.8976]], grad_fn=<SubBackward0>)
[2022-11-05 16:58:37.586676] Process 0. Episode 15100, average_reward -0.071391
Episode 15100: Total Loss of tensor([[13.3343]], grad_fn=<SubBackward0>)
[2022-11-05 16:58:38.341769] Process 2. Episode 15400, average_reward -0.075844
Episode 15400: Total Loss of tensor([[5.5371]], grad_fn=<SubBackward0>)
[2022-11-05 16:58:45.004453] Process 5. Episode 15250, average_reward -0.069311
Episode 15250: Total Loss of tensor([[-5.8851]], grad_fn=<SubBackward0>)
[2022-11-05 16:59:09.276088] Process 1. Episode 14850, average_reward -0.068418
Episode 14850: Total Loss of tensor([[11.7794]], grad_fn=<SubBackward0>)
[2022-11-05 16:59:32.079939] Process 3. Episode 15000, average_reward -0.073200
Episode 15000: Total Loss of tensor([[7.5250]], grad_fn=<SubBackward0>)
[2022-11-05 17:00:06.701369] Process 4. Episode 16050, average_reward -0.072523
Episode 16050: Total Loss of tensor([[1.1904]], grad_fn=<SubBackward0>)
[2022-11-05 17:00:58.649840] Process 2. Episode 15450, average_reward -0.075793
Episode 15450: Total Loss of tensor([[5.1949]], grad_fn=<SubBackward0>)
[2022-11-05 17:01:12.508946] Process 0. Episode 15150, average_reward -0.071485
Episode 15150: Total Loss of tensor([[1.9279]], grad_fn=<SubBackward0>)
[2022-11-05 17:01:15.295999] Process 5. Episode 15300, average_reward -0.069346
Episode 15300: Total Loss of tensor([[5.8923]], grad_fn=<SubBackward0>)
[2022-11-05 17:01:49.063479] Process 1. Episode 14900, average_reward -0.068188
Episode 14900: Total Loss of tensor([[13.1885]], grad_fn=<SubBackward0>)
[2022-11-05 17:02:04.210415] Process 3. Episode 15050, average_reward -0.073156
Episode 15050: Total Loss of tensor([[10.2455]], grad_fn=<SubBackward0>)
[2022-11-05 17:02:27.991832] Process 4. Episode 16100, average_reward -0.072609
Episode 16100: Total Loss of tensor([[23.8487]], grad_fn=<SubBackward0>)
[2022-11-05 17:03:27.875990] Process 2. Episode 15500, average_reward -0.075806
Episode 15500: Total Loss of tensor([[3.3549]], grad_fn=<SubBackward0>)
[2022-11-05 17:03:40.862811] Process 0. Episode 15200, average_reward -0.071382
Episode 15200: Total Loss of tensor([[5.2942]], grad_fn=<SubBackward0>)
[2022-11-05 17:03:45.841338] Process 5. Episode 15350, average_reward -0.069446
Episode 15350: Total Loss of tensor([[-1.0015]], grad_fn=<SubBackward0>)
[2022-11-05 17:04:26.530342] Process 1. Episode 14950, average_reward -0.068161
Episode 14950: Total Loss of tensor([[-117.5501]], grad_fn=<SubBackward0>)
[2022-11-05 17:04:35.385163] Process 3. Episode 15100, average_reward -0.073046
Episode 15100: Total Loss of tensor([[15.4774]], grad_fn=<SubBackward0>)
[2022-11-05 17:04:48.899059] Process 4. Episode 16150, average_reward -0.072755
Episode 16150: Total Loss of tensor([[-6.1291]], grad_fn=<SubBackward0>)
[2022-11-05 17:05:57.232804] Process 2. Episode 15550, average_reward -0.075884
Episode 15550: Total Loss of tensor([[10.3539]], grad_fn=<SubBackward0>)
[2022-11-05 17:06:09.959887] Process 5. Episode 15400, average_reward -0.069286
Episode 15400: Total Loss of tensor([[2.5795]], grad_fn=<SubBackward0>)
[2022-11-05 17:06:15.488118] Process 0. Episode 15250, average_reward -0.071344
Episode 15250: Total Loss of tensor([[14.2760]], grad_fn=<SubBackward0>)
[2022-11-05 17:06:54.625106] Process 1. Episode 15000, average_reward -0.068267
Episode 15000: Total Loss of tensor([[3.7715]], grad_fn=<SubBackward0>)
[2022-11-05 17:07:04.190043] Process 3. Episode 15150, average_reward -0.073201
Episode 15150: Total Loss of tensor([[3.5454]], grad_fn=<SubBackward0>)
[2022-11-05 17:07:10.104178] Process 4. Episode 16200, average_reward -0.072716
Episode 16200: Total Loss of tensor([[8.2772]], grad_fn=<SubBackward0>)
[2022-11-05 17:08:34.096161] Process 2. Episode 15600, average_reward -0.075962
Episode 15600: Total Loss of tensor([[5.7450]], grad_fn=<SubBackward0>)
[2022-11-05 17:08:34.425887] Process 5. Episode 15450, average_reward -0.069320
Episode 15450: Total Loss of tensor([[15.2446]], grad_fn=<SubBackward0>)
[2022-11-05 17:08:41.237938] Process 0. Episode 15300, average_reward -0.071307
Episode 15300: Total Loss of tensor([[5.9650]], grad_fn=<SubBackward0>)
[2022-11-05 17:09:29.665354] Process 4. Episode 16250, average_reward -0.072738
Episode 16250: Total Loss of tensor([[8.6387]], grad_fn=<SubBackward0>)
[2022-11-05 17:09:38.025138] Process 3. Episode 15200, average_reward -0.072961
Episode 15200: Total Loss of tensor([[6.7353]], grad_fn=<SubBackward0>)
[2022-11-05 17:09:39.628574] Process 1. Episode 15050, average_reward -0.068173
Episode 15050: Total Loss of tensor([[2.0583]], grad_fn=<SubBackward0>)
[2022-11-05 17:10:59.304095] Process 5. Episode 15500, average_reward -0.069419
Episode 15500: Total Loss of tensor([[4.7504]], grad_fn=<SubBackward0>)
[2022-11-05 17:11:04.581782] Process 0. Episode 15350, average_reward -0.071270
Episode 15350: Total Loss of tensor([[19.4476]], grad_fn=<SubBackward0>)
[2022-11-05 17:11:11.294782] Process 2. Episode 15650, average_reward -0.075847
Episode 15650: Total Loss of tensor([[-0.8963]], grad_fn=<SubBackward0>)
[2022-11-05 17:11:48.562687] Process 4. Episode 16300, average_reward -0.072945
Episode 16300: Total Loss of tensor([[2.8896]], grad_fn=<SubBackward0>)
[2022-11-05 17:12:10.591022] Process 3. Episode 15250, average_reward -0.073115
Episode 15250: Total Loss of tensor([[-6.5224]], grad_fn=<SubBackward0>)
[2022-11-05 17:12:16.511768] Process 1. Episode 15100, average_reward -0.068344
Episode 15100: Total Loss of tensor([[-20.4216]], grad_fn=<SubBackward0>)
[2022-11-05 17:13:26.374800] Process 0. Episode 15400, average_reward -0.071234
Episode 15400: Total Loss of tensor([[10.7084]], grad_fn=<SubBackward0>)
[2022-11-05 17:13:36.150067] Process 2. Episode 15700, average_reward -0.075860
Episode 15700: Total Loss of tensor([[2.2414]], grad_fn=<SubBackward0>)
[2022-11-05 17:13:42.259740] Process 5. Episode 15550, average_reward -0.069518
Episode 15550: Total Loss of tensor([[8.9282]], grad_fn=<SubBackward0>)
[2022-11-05 17:14:06.461421] Process 4. Episode 16350, average_reward -0.072722
Episode 16350: Total Loss of tensor([[-3.5590]], grad_fn=<SubBackward0>)
[2022-11-05 17:14:50.878566] Process 3. Episode 15300, average_reward -0.073007
Episode 15300: Total Loss of tensor([[-2.3960]], grad_fn=<SubBackward0>)
[2022-11-05 17:14:59.601468] Process 1. Episode 15150, average_reward -0.068119
Episode 15150: Total Loss of tensor([[9.2166]], grad_fn=<SubBackward0>)
[2022-11-05 17:15:53.017155] Process 0. Episode 15450, average_reward -0.071133
Episode 15450: Total Loss of tensor([[-0.8827]], grad_fn=<SubBackward0>)
[2022-11-05 17:15:57.900341] Process 2. Episode 15750, average_reward -0.075746
Episode 15750: Total Loss of tensor([[3.3364]], grad_fn=<SubBackward0>)
[2022-11-05 17:16:09.704186] Process 5. Episode 15600, average_reward -0.069615
Episode 15600: Total Loss of tensor([[1.4876]], grad_fn=<SubBackward0>)
[2022-11-05 17:16:24.258882] Process 4. Episode 16400, average_reward -0.072622
Episode 16400: Total Loss of tensor([[-3.1837]], grad_fn=<SubBackward0>)
[2022-11-05 17:17:33.935231] Process 1. Episode 15200, average_reward -0.068224
Episode 15200: Total Loss of tensor([[112.2371]], grad_fn=<SubBackward0>)
[2022-11-05 17:17:34.914333] Process 3. Episode 15350, average_reward -0.073029
Episode 15350: Total Loss of tensor([[11.2894]], grad_fn=<SubBackward0>)
[2022-11-05 17:18:20.069334] Process 0. Episode 15500, average_reward -0.071032
Episode 15500: Total Loss of tensor([[7.3832]], grad_fn=<SubBackward0>)
[2022-11-05 17:18:20.720854] Process 2. Episode 15800, average_reward -0.075759
Episode 15800: Total Loss of tensor([[9.8564]], grad_fn=<SubBackward0>)
[2022-11-05 17:18:44.484865] Process 4. Episode 16450, average_reward -0.072584
Episode 16450: Total Loss of tensor([[2.9066]], grad_fn=<SubBackward0>)
[2022-11-05 17:18:52.116791] Process 5. Episode 15650, average_reward -0.069521
Episode 15650: Total Loss of tensor([[1.6931]], grad_fn=<SubBackward0>)
[2022-11-05 17:20:10.232364] Process 1. Episode 15250, average_reward -0.068328
Episode 15250: Total Loss of tensor([[2.4189]], grad_fn=<SubBackward0>)
[2022-11-05 17:20:11.037561] Process 3. Episode 15400, average_reward -0.073052
Episode 15400: Total Loss of tensor([[-2.9959]], grad_fn=<SubBackward0>)
[2022-11-05 17:20:47.191510] Process 2. Episode 15850, average_reward -0.075773
Episode 15850: Total Loss of tensor([[-9.9897]], grad_fn=<SubBackward0>)
[2022-11-05 17:20:48.755114] Process 0. Episode 15550, average_reward -0.071318
Episode 15550: Total Loss of tensor([[-75.2611]], grad_fn=<SubBackward0>)
[2022-11-05 17:21:02.634685] Process 4. Episode 16500, average_reward -0.072485
Episode 16500: Total Loss of tensor([[4.3770]], grad_fn=<SubBackward0>)
[2022-11-05 17:21:32.014249] Process 5. Episode 15700, average_reward -0.069554
Episode 15700: Total Loss of tensor([[6.5599]], grad_fn=<SubBackward0>)
[2022-11-05 17:22:39.592074] Process 3. Episode 15450, average_reward -0.073010
Episode 15450: Total Loss of tensor([[-6.9069]], grad_fn=<SubBackward0>)
[2022-11-05 17:22:48.652874] Process 1. Episode 15300, average_reward -0.068366
Episode 15300: Total Loss of tensor([[22.1692]], grad_fn=<SubBackward0>)
[2022-11-05 17:23:12.077784] Process 2. Episode 15900, average_reward -0.075723
Episode 15900: Total Loss of tensor([[13.1516]], grad_fn=<SubBackward0>)
[2022-11-05 17:23:16.021856] Process 0. Episode 15600, average_reward -0.071218
Episode 15600: Total Loss of tensor([[11.7042]], grad_fn=<SubBackward0>)
[2022-11-05 17:23:25.396007] Process 4. Episode 16550, average_reward -0.072508
Episode 16550: Total Loss of tensor([[-2.7761]], grad_fn=<SubBackward0>)
[2022-11-05 17:24:04.654566] Process 5. Episode 15750, average_reward -0.069460
Episode 15750: Total Loss of tensor([[10.2622]], grad_fn=<SubBackward0>)
[2022-11-05 17:25:06.081417] Process 3. Episode 15500, average_reward -0.072968
Episode 15500: Total Loss of tensor([[1.2574]], grad_fn=<SubBackward0>)
[2022-11-05 17:25:20.530100] Process 1. Episode 15350, average_reward -0.068469
Episode 15350: Total Loss of tensor([[8.4071]], grad_fn=<SubBackward0>)
[2022-11-05 17:25:41.451373] Process 2. Episode 15950, average_reward -0.075674
Episode 15950: Total Loss of tensor([[15.1020]], grad_fn=<SubBackward0>)
[2022-11-05 17:25:47.050636] Process 4. Episode 16600, average_reward -0.072530
Episode 16600: Total Loss of tensor([[7.4889]], grad_fn=<SubBackward0>)
[2022-11-05 17:25:53.455338] Process 0. Episode 15650, average_reward -0.071054
Episode 15650: Total Loss of tensor([[0.3617]], grad_fn=<SubBackward0>)
[2022-11-05 17:26:28.683243] Process 5. Episode 15800, average_reward -0.069367
Episode 15800: Total Loss of tensor([[12.6952]], grad_fn=<SubBackward0>)
[2022-11-05 17:27:34.422174] Process 3. Episode 15550, average_reward -0.072862
Episode 15550: Total Loss of tensor([[0.0460]], grad_fn=<SubBackward0>)
[2022-11-05 17:27:52.910874] Process 1. Episode 15400, average_reward -0.068247
Episode 15400: Total Loss of tensor([[4.1311]], grad_fn=<SubBackward0>)
[2022-11-05 17:28:05.183158] Process 4. Episode 16650, average_reward -0.072553
Episode 16650: Total Loss of tensor([[2.9647]], grad_fn=<SubBackward0>)
[2022-11-05 17:28:12.983460] Process 2. Episode 16000, average_reward -0.075813
Episode 16000: Total Loss of tensor([[-50.3097]], grad_fn=<SubBackward0>)
[2022-11-05 17:28:34.457647] Process 0. Episode 15700, average_reward -0.071083
Episode 15700: Total Loss of tensor([[10.0492]], grad_fn=<SubBackward0>)
[2022-11-05 17:29:07.031337] Process 5. Episode 15850, average_reward -0.069274
Episode 15850: Total Loss of tensor([[2.3399]], grad_fn=<SubBackward0>)
[2022-11-05 17:30:06.227769] Process 3. Episode 15600, average_reward -0.073205
Episode 15600: Total Loss of tensor([[11.1632]], grad_fn=<SubBackward0>)
[2022-11-05 17:30:15.849157] Process 1. Episode 15450, average_reward -0.068414
Episode 15450: Total Loss of tensor([[6.0779]], grad_fn=<SubBackward0>)
[2022-11-05 17:30:25.772663] Process 4. Episode 16700, average_reward -0.072635
Episode 16700: Total Loss of tensor([[4.4479]], grad_fn=<SubBackward0>)
[2022-11-05 17:30:38.970035] Process 2. Episode 16050, average_reward -0.075826
Episode 16050: Total Loss of tensor([[-11.1916]], grad_fn=<SubBackward0>)
[2022-11-05 17:31:01.140396] Process 0. Episode 15750, average_reward -0.071048
Episode 15750: Total Loss of tensor([[9.8138]], grad_fn=<SubBackward0>)
[2022-11-05 17:31:40.866728] Process 5. Episode 15900, average_reward -0.069245
Episode 15900: Total Loss of tensor([[14.6856]], grad_fn=<SubBackward0>)
[2022-11-05 17:32:34.634331] Process 3. Episode 15650, average_reward -0.073099
Episode 15650: Total Loss of tensor([[-1.0853]], grad_fn=<SubBackward0>)
[2022-11-05 17:32:43.112051] Process 1. Episode 15500, average_reward -0.068194
Episode 15500: Total Loss of tensor([[1.1849]], grad_fn=<SubBackward0>)
[2022-11-05 17:32:48.828660] Process 4. Episode 16750, average_reward -0.072597
Episode 16750: Total Loss of tensor([[11.4432]], grad_fn=<SubBackward0>)
[2022-11-05 17:33:14.975199] Process 2. Episode 16100, average_reward -0.075839
Episode 16100: Total Loss of tensor([[6.4230]], grad_fn=<SubBackward0>)
[2022-11-05 17:33:32.563763] Process 0. Episode 15800, average_reward -0.071203
Episode 15800: Total Loss of tensor([[3.9027]], grad_fn=<SubBackward0>)
[2022-11-05 17:34:06.075023] Process 5. Episode 15950, average_reward -0.069154
Episode 15950: Total Loss of tensor([[12.4151]], grad_fn=<SubBackward0>)
[2022-11-05 17:35:03.372029] Process 3. Episode 15700, average_reward -0.072994
Episode 15700: Total Loss of tensor([[-13.1484]], grad_fn=<SubBackward0>)
[2022-11-05 17:35:07.275641] Process 4. Episode 16800, average_reward -0.072679
Episode 16800: Total Loss of tensor([[8.3529]], grad_fn=<SubBackward0>)
[2022-11-05 17:35:12.388013] Process 1. Episode 15550, average_reward -0.068232
Episode 15550: Total Loss of tensor([[4.7157]], grad_fn=<SubBackward0>)
[2022-11-05 17:35:55.501474] Process 2. Episode 16150, average_reward -0.075851
Episode 16150: Total Loss of tensor([[15.0218]], grad_fn=<SubBackward0>)
[2022-11-05 17:36:06.068127] Process 0. Episode 15850, average_reward -0.071230
Episode 15850: Total Loss of tensor([[17.0243]], grad_fn=<SubBackward0>)
[2022-11-05 17:36:31.342527] Process 5. Episode 16000, average_reward -0.069062
Episode 16000: Total Loss of tensor([[15.1874]], grad_fn=<SubBackward0>)
[2022-11-05 17:37:28.222389] Process 3. Episode 15750, average_reward -0.073079
Episode 15750: Total Loss of tensor([[15.0847]], grad_fn=<SubBackward0>)
[2022-11-05 17:37:31.945416] Process 4. Episode 16850, average_reward -0.072819
Episode 16850: Total Loss of tensor([[8.6497]], grad_fn=<SubBackward0>)
[2022-11-05 17:37:43.034691] Process 1. Episode 15600, average_reward -0.068205
Episode 15600: Total Loss of tensor([[8.4931]], grad_fn=<SubBackward0>)
[2022-11-05 17:38:21.093742] Process 2. Episode 16200, average_reward -0.075741
Episode 16200: Total Loss of tensor([[1.6160]], grad_fn=<SubBackward0>)
[2022-11-05 17:38:36.576736] Process 0. Episode 15900, average_reward -0.071195
Episode 15900: Total Loss of tensor([[-14.5995]], grad_fn=<SubBackward0>)
[2022-11-05 17:38:58.725466] Process 5. Episode 16050, average_reward -0.069159
Episode 16050: Total Loss of tensor([[-0.6134]], grad_fn=<SubBackward0>)
[2022-11-05 17:39:55.281376] Process 3. Episode 15800, average_reward -0.072975
Episode 15800: Total Loss of tensor([[11.4517]], grad_fn=<SubBackward0>)
[2022-11-05 17:39:56.535276] Process 4. Episode 16900, average_reward -0.072722
Episode 16900: Total Loss of tensor([[3.3302]], grad_fn=<SubBackward0>)
[2022-11-05 17:40:12.231993] Process 1. Episode 15650, average_reward -0.068051
Episode 15650: Total Loss of tensor([[5.0452]], grad_fn=<SubBackward0>)
[2022-11-05 17:40:50.855462] Process 2. Episode 16250, average_reward -0.075754
Episode 16250: Total Loss of tensor([[9.1973]], grad_fn=<SubBackward0>)
[2022-11-05 17:41:10.508652] Process 0. Episode 15950, average_reward -0.071160
Episode 15950: Total Loss of tensor([[9.1248]], grad_fn=<SubBackward0>)
[2022-11-05 17:41:24.086202] Process 5. Episode 16100, average_reward -0.069317
Episode 16100: Total Loss of tensor([[0.4144]], grad_fn=<SubBackward0>)
[2022-11-05 17:42:19.792885] Process 4. Episode 16950, average_reward -0.072861
Episode 16950: Total Loss of tensor([[22.1933]], grad_fn=<SubBackward0>)
[2022-11-05 17:42:20.443211] Process 3. Episode 15850, average_reward -0.073186
Episode 15850: Total Loss of tensor([[19.4639]], grad_fn=<SubBackward0>)
[2022-11-05 17:42:40.243176] Process 1. Episode 15700, average_reward -0.068089
Episode 15700: Total Loss of tensor([[9.1507]], grad_fn=<SubBackward0>)
[2022-11-05 17:43:27.283736] Process 2. Episode 16300, average_reward -0.075767
Episode 16300: Total Loss of tensor([[21.9438]], grad_fn=<SubBackward0>)
[2022-11-05 17:43:35.553826] Process 0. Episode 16000, average_reward -0.071375
Episode 16000: Total Loss of tensor([[15.4303]], grad_fn=<SubBackward0>)
[2022-11-05 17:43:49.345764] Process 5. Episode 16150, average_reward -0.069474
Episode 16150: Total Loss of tensor([[7.9338]], grad_fn=<SubBackward0>)
[2022-11-05 17:44:42.571031] Process 4. Episode 17000, average_reward -0.072824
Episode 17000: Total Loss of tensor([[-107.4983]], grad_fn=<SubBackward0>)
[2022-11-05 17:44:54.083537] Process 3. Episode 15900, average_reward -0.073208
Episode 15900: Total Loss of tensor([[3.1581]], grad_fn=<SubBackward0>)
[2022-11-05 17:45:08.923574] Process 1. Episode 15750, average_reward -0.068063
Episode 15750: Total Loss of tensor([[-13.8536]], grad_fn=<SubBackward0>)
[2022-11-05 17:45:55.956103] Process 0. Episode 16050, average_reward -0.071402
Episode 16050: Total Loss of tensor([[19.5305]], grad_fn=<SubBackward0>)
[2022-11-05 17:46:09.398344] Process 2. Episode 16350, average_reward -0.076147
Episode 16350: Total Loss of tensor([[4.4553]], grad_fn=<SubBackward0>)
[2022-11-05 17:46:23.990878] Process 5. Episode 16200, average_reward -0.069506
Episode 16200: Total Loss of tensor([[11.2396]], grad_fn=<SubBackward0>)
[2022-11-05 17:47:02.121216] Process 4. Episode 17050, average_reward -0.072786
Episode 17050: Total Loss of tensor([[17.4888]], grad_fn=<SubBackward0>)
[2022-11-05 17:47:26.109859] Process 3. Episode 15950, average_reward -0.073166
Episode 15950: Total Loss of tensor([[4.5891]], grad_fn=<SubBackward0>)
[2022-11-05 17:47:39.467791] Process 1. Episode 15800, average_reward -0.068038
Episode 15800: Total Loss of tensor([[-123.1575]], grad_fn=<SubBackward0>)
[2022-11-05 17:48:23.321264] Process 0. Episode 16100, average_reward -0.071304
Episode 16100: Total Loss of tensor([[7.2059]], grad_fn=<SubBackward0>)
[2022-11-05 17:48:48.945587] Process 2. Episode 16400, average_reward -0.076159
Episode 16400: Total Loss of tensor([[10.8584]], grad_fn=<SubBackward0>)
[2022-11-05 17:48:53.483756] Process 5. Episode 16250, average_reward -0.069600
Episode 16250: Total Loss of tensor([[18.1156]], grad_fn=<SubBackward0>)
[2022-11-05 17:49:22.796700] Process 4. Episode 17100, average_reward -0.072982
Episode 17100: Total Loss of tensor([[13.4631]], grad_fn=<SubBackward0>)
[2022-11-05 17:49:47.273930] Process 3. Episode 16000, average_reward -0.073188
Episode 16000: Total Loss of tensor([[-10.4929]], grad_fn=<SubBackward0>)
[2022-11-05 17:50:14.663068] Process 1. Episode 15850, average_reward -0.068076
Episode 15850: Total Loss of tensor([[-3.4567]], grad_fn=<SubBackward0>)
[2022-11-05 17:50:49.130199] Process 0. Episode 16150, average_reward -0.071393
Episode 16150: Total Loss of tensor([[13.0734]], grad_fn=<SubBackward0>)
[2022-11-05 17:51:25.400287] Process 5. Episode 16300, average_reward -0.069571
Episode 16300: Total Loss of tensor([[11.5806]], grad_fn=<SubBackward0>)
[2022-11-05 17:51:26.699472] Process 2. Episode 16450, average_reward -0.076170
Episode 16450: Total Loss of tensor([[11.5864]], grad_fn=<SubBackward0>)
[2022-11-05 17:51:44.909520] Process 4. Episode 17150, average_reward -0.072945
Episode 17150: Total Loss of tensor([[3.3452]], grad_fn=<SubBackward0>)
[2022-11-05 17:52:12.900342] Process 3. Episode 16050, average_reward -0.073271
Episode 16050: Total Loss of tensor([[8.0055]], grad_fn=<SubBackward0>)
[2022-11-05 17:52:49.760769] Process 1. Episode 15900, average_reward -0.067987
Episode 15900: Total Loss of tensor([[11.1313]], grad_fn=<SubBackward0>)
[2022-11-05 17:53:15.552022] Process 0. Episode 16200, average_reward -0.071296
Episode 16200: Total Loss of tensor([[17.8773]], grad_fn=<SubBackward0>)
[2022-11-05 17:53:59.068600] Process 2. Episode 16500, average_reward -0.076182
Episode 16500: Total Loss of tensor([[-6.3311]], grad_fn=<SubBackward0>)
[2022-11-05 17:53:59.589640] Process 5. Episode 16350, average_reward -0.069969
Episode 16350: Total Loss of tensor([[10.6797]], grad_fn=<SubBackward0>)
[2022-11-05 17:54:04.671378] Process 4. Episode 17200, average_reward -0.072849
Episode 17200: Total Loss of tensor([[8.4030]], grad_fn=<SubBackward0>)
[2022-11-05 17:54:51.487514] Process 3. Episode 16100, average_reward -0.073416
Episode 16100: Total Loss of tensor([[13.5117]], grad_fn=<SubBackward0>)
[2022-11-05 17:55:24.478282] Process 1. Episode 15950, average_reward -0.068025
Episode 15950: Total Loss of tensor([[32.8458]], grad_fn=<SubBackward0>)
[2022-11-05 17:55:52.189433] Process 0. Episode 16250, average_reward -0.071323
Episode 16250: Total Loss of tensor([[15.1579]], grad_fn=<SubBackward0>)
[2022-11-05 17:56:18.470600] Process 5. Episode 16400, average_reward -0.069817
Episode 16400: Total Loss of tensor([[32.6637]], grad_fn=<SubBackward0>)
[2022-11-05 17:56:22.887258] Process 2. Episode 16550, average_reward -0.076314
Episode 16550: Total Loss of tensor([[2.4010]], grad_fn=<SubBackward0>)
[2022-11-05 17:56:23.177501] Process 4. Episode 17250, average_reward -0.072870
Episode 17250: Total Loss of tensor([[-97.8213]], grad_fn=<SubBackward0>)
[2022-11-05 17:57:30.307536] Process 3. Episode 16150, average_reward -0.073560
Episode 16150: Total Loss of tensor([[-1.9018]], grad_fn=<SubBackward0>)
[2022-11-05 17:58:02.728069] Process 1. Episode 16000, average_reward -0.068125
Episode 16000: Total Loss of tensor([[-114.6853]], grad_fn=<SubBackward0>)
[2022-11-05 17:58:20.647296] Process 0. Episode 16300, average_reward -0.071166
Episode 16300: Total Loss of tensor([[-0.9145]], grad_fn=<SubBackward0>)
[2022-11-05 17:58:41.862425] Process 5. Episode 16450, average_reward -0.069666
Episode 16450: Total Loss of tensor([[-1.9973]], grad_fn=<SubBackward0>)
[2022-11-05 17:58:43.231137] Process 2. Episode 16600, average_reward -0.076084
Episode 16600: Total Loss of tensor([[5.3618]], grad_fn=<SubBackward0>)
[2022-11-05 17:58:45.420401] Process 4. Episode 17300, average_reward -0.072775
Episode 17300: Total Loss of tensor([[5.4628]], grad_fn=<SubBackward0>)
[2022-11-05 18:00:03.728915] Process 3. Episode 16200, average_reward -0.073580
Episode 16200: Total Loss of tensor([[-18.0433]], grad_fn=<SubBackward0>)
[2022-11-05 18:00:36.069104] Process 1. Episode 16050, average_reward -0.068224
Episode 16050: Total Loss of tensor([[-21.1064]], grad_fn=<SubBackward0>)
[2022-11-05 18:00:50.264433] Process 0. Episode 16350, average_reward -0.071193
Episode 16350: Total Loss of tensor([[4.5003]], grad_fn=<SubBackward0>)
[2022-11-05 18:01:03.742365] Process 2. Episode 16650, average_reward -0.075916
Episode 16650: Total Loss of tensor([[12.6074]], grad_fn=<SubBackward0>)
[2022-11-05 18:01:07.886016] Process 5. Episode 16500, average_reward -0.069758
Episode 16500: Total Loss of tensor([[7.7851]], grad_fn=<SubBackward0>)
[2022-11-05 18:01:10.882457] Process 4. Episode 17350, average_reward -0.072853
Episode 17350: Total Loss of tensor([[4.1528]], grad_fn=<SubBackward0>)
[2022-11-05 18:02:37.327332] Process 3. Episode 16250, average_reward -0.073600
Episode 16250: Total Loss of tensor([[10.5735]], grad_fn=<SubBackward0>)
[2022-11-05 18:03:18.766355] Process 1. Episode 16100, average_reward -0.068137
Episode 16100: Total Loss of tensor([[6.0226]], grad_fn=<SubBackward0>)
[2022-11-05 18:03:20.212452] Process 0. Episode 16400, average_reward -0.071280
Episode 16400: Total Loss of tensor([[0.1453]], grad_fn=<SubBackward0>)
[2022-11-05 18:03:21.626404] Process 2. Episode 16700, average_reward -0.075928
Episode 16700: Total Loss of tensor([[10.4046]], grad_fn=<SubBackward0>)
[2022-11-05 18:03:28.658448] Process 5. Episode 16550, average_reward -0.069789
Episode 16550: Total Loss of tensor([[-3.7397]], grad_fn=<SubBackward0>)
[2022-11-05 18:03:31.431803] Process 4. Episode 17400, average_reward -0.072759
Episode 17400: Total Loss of tensor([[5.3944]], grad_fn=<SubBackward0>)
[2022-11-05 18:05:11.017821] Process 3. Episode 16300, average_reward -0.073620
Episode 16300: Total Loss of tensor([[3.5687]], grad_fn=<SubBackward0>)
[2022-11-05 18:05:42.162163] Process 2. Episode 16750, average_reward -0.075761
Episode 16750: Total Loss of tensor([[-0.4150]], grad_fn=<SubBackward0>)
[2022-11-05 18:05:51.707047] Process 5. Episode 16600, average_reward -0.069819
Episode 16600: Total Loss of tensor([[-3.7555]], grad_fn=<SubBackward0>)
[2022-11-05 18:05:52.153949] Process 0. Episode 16450, average_reward -0.071125
Episode 16450: Total Loss of tensor([[5.5828]], grad_fn=<SubBackward0>)
[2022-11-05 18:05:53.483634] Process 4. Episode 17450, average_reward -0.072607
Episode 17450: Total Loss of tensor([[2.7652]], grad_fn=<SubBackward0>)
[2022-11-05 18:05:59.968401] Process 1. Episode 16150, average_reward -0.068111
Episode 16150: Total Loss of tensor([[-112.9199]], grad_fn=<SubBackward0>)
[2022-11-05 18:07:39.679440] Process 3. Episode 16350, average_reward -0.073700
Episode 16350: Total Loss of tensor([[-28.6658]], grad_fn=<SubBackward0>)
[2022-11-05 18:08:11.293483] Process 2. Episode 16800, average_reward -0.075714
Episode 16800: Total Loss of tensor([[8.1840]], grad_fn=<SubBackward0>)
[2022-11-05 18:08:15.418443] Process 4. Episode 17500, average_reward -0.072629
Episode 17500: Total Loss of tensor([[-1.2980]], grad_fn=<SubBackward0>)
[2022-11-05 18:08:17.397220] Process 5. Episode 16650, average_reward -0.069850
Episode 16650: Total Loss of tensor([[0.6050]], grad_fn=<SubBackward0>)
[2022-11-05 18:08:19.106512] Process 0. Episode 16500, average_reward -0.070970
Episode 16500: Total Loss of tensor([[9.5055]], grad_fn=<SubBackward0>)
[2022-11-05 18:08:36.887430] Process 1. Episode 16200, average_reward -0.067963
Episode 16200: Total Loss of tensor([[5.0701]], grad_fn=<SubBackward0>)
[2022-11-05 18:10:08.908026] Process 3. Episode 16400, average_reward -0.073598
Episode 16400: Total Loss of tensor([[5.8067]], grad_fn=<SubBackward0>)
[2022-11-05 18:10:37.207949] Process 2. Episode 16850, average_reward -0.075668
Episode 16850: Total Loss of tensor([[17.9513]], grad_fn=<SubBackward0>)
[2022-11-05 18:10:39.180551] Process 4. Episode 17550, average_reward -0.072593
Episode 17550: Total Loss of tensor([[-2.7623]], grad_fn=<SubBackward0>)
[2022-11-05 18:10:44.033935] Process 5. Episode 16700, average_reward -0.070000
Episode 16700: Total Loss of tensor([[6.5201]], grad_fn=<SubBackward0>)
[2022-11-05 18:10:47.856716] Process 0. Episode 16550, average_reward -0.070876
Episode 16550: Total Loss of tensor([[-3.9445]], grad_fn=<SubBackward0>)
[2022-11-05 18:11:13.372199] Process 1. Episode 16250, average_reward -0.067938
Episode 16250: Total Loss of tensor([[-2.2971]], grad_fn=<SubBackward0>)
[2022-11-05 18:12:42.072828] Process 3. Episode 16450, average_reward -0.073495
Episode 16450: Total Loss of tensor([[1.0385]], grad_fn=<SubBackward0>)
[2022-11-05 18:12:58.657478] Process 2. Episode 16900, average_reward -0.075799
Episode 16900: Total Loss of tensor([[14.4432]], grad_fn=<SubBackward0>)
[2022-11-05 18:13:03.035447] Process 4. Episode 17600, average_reward -0.072500
Episode 17600: Total Loss of tensor([[7.2705]], grad_fn=<SubBackward0>)
[2022-11-05 18:13:07.452883] Process 5. Episode 16750, average_reward -0.070090
Episode 16750: Total Loss of tensor([[-32.9747]], grad_fn=<SubBackward0>)
[2022-11-05 18:13:20.209875] Process 0. Episode 16600, average_reward -0.070964
Episode 16600: Total Loss of tensor([[12.9058]], grad_fn=<SubBackward0>)
[2022-11-05 18:13:51.260239] Process 1. Episode 16300, average_reward -0.067914
Episode 16300: Total Loss of tensor([[3.0816]], grad_fn=<SubBackward0>)
[2022-11-05 18:15:10.261091] Process 3. Episode 16500, average_reward -0.073455
Episode 16500: Total Loss of tensor([[5.3493]], grad_fn=<SubBackward0>)
[2022-11-05 18:15:21.159460] Process 2. Episode 16950, average_reward -0.075870
Episode 16950: Total Loss of tensor([[3.9419]], grad_fn=<SubBackward0>)
[2022-11-05 18:15:27.992364] Process 5. Episode 16800, average_reward -0.070238
Episode 16800: Total Loss of tensor([[3.8200]], grad_fn=<SubBackward0>)
[2022-11-05 18:15:30.103689] Process 4. Episode 17650, average_reward -0.072408
Episode 17650: Total Loss of tensor([[2.2489]], grad_fn=<SubBackward0>)
[2022-11-05 18:15:49.117375] Process 0. Episode 16650, average_reward -0.070751
Episode 16650: Total Loss of tensor([[0.4881]], grad_fn=<SubBackward0>)
[2022-11-05 18:16:36.249029] Process 1. Episode 16350, average_reward -0.067829
Episode 16350: Total Loss of tensor([[2.6458]], grad_fn=<SubBackward0>)
[2022-11-05 18:17:46.174270] Process 2. Episode 17000, average_reward -0.075765
Episode 17000: Total Loss of tensor([[-40.5062]], grad_fn=<SubBackward0>)
[2022-11-05 18:17:46.316469] Process 3. Episode 16550, average_reward -0.073414
Episode 16550: Total Loss of tensor([[2.9802]], grad_fn=<SubBackward0>)
[2022-11-05 18:17:52.537668] Process 5. Episode 16850, average_reward -0.070148
Episode 16850: Total Loss of tensor([[14.7761]], grad_fn=<SubBackward0>)
[2022-11-05 18:17:55.283858] Process 4. Episode 17700, average_reward -0.072316
Episode 17700: Total Loss of tensor([[-3.1318]], grad_fn=<SubBackward0>)
[2022-11-05 18:18:16.349107] Process 0. Episode 16700, average_reward -0.070659
Episode 16700: Total Loss of tensor([[3.0091]], grad_fn=<SubBackward0>)
[2022-11-05 18:19:17.172603] Process 1. Episode 16400, average_reward -0.067927
Episode 16400: Total Loss of tensor([[0.2079]], grad_fn=<SubBackward0>)
[2022-11-05 18:20:13.309781] Process 4. Episode 17750, average_reward -0.072169
Episode 17750: Total Loss of tensor([[17.2722]], grad_fn=<SubBackward0>)
[2022-11-05 18:20:18.121660] Process 5. Episode 16900, average_reward -0.070059
Episode 16900: Total Loss of tensor([[-6.5135]], grad_fn=<SubBackward0>)
[2022-11-05 18:20:19.090655] Process 2. Episode 17050, average_reward -0.075777
Episode 17050: Total Loss of tensor([[11.2735]], grad_fn=<SubBackward0>)
[2022-11-05 18:20:27.555716] Process 3. Episode 16600, average_reward -0.073434
Episode 16600: Total Loss of tensor([[10.8870]], grad_fn=<SubBackward0>)
[2022-11-05 18:20:39.195366] Process 0. Episode 16750, average_reward -0.070746
Episode 16750: Total Loss of tensor([[12.2756]], grad_fn=<SubBackward0>)
[2022-11-05 18:21:56.791146] Process 1. Episode 16450, average_reward -0.067903
Episode 16450: Total Loss of tensor([[19.3996]], grad_fn=<SubBackward0>)
[2022-11-05 18:22:32.167663] Process 4. Episode 17800, average_reward -0.072191
Episode 17800: Total Loss of tensor([[15.6013]], grad_fn=<SubBackward0>)
[2022-11-05 18:22:46.004199] Process 5. Episode 16950, average_reward -0.070147
Episode 16950: Total Loss of tensor([[16.5476]], grad_fn=<SubBackward0>)
[2022-11-05 18:22:48.247084] Process 2. Episode 17100, average_reward -0.075789
Episode 17100: Total Loss of tensor([[-14.9489]], grad_fn=<SubBackward0>)
[2022-11-05 18:23:00.433771] Process 0. Episode 16800, average_reward -0.070595
Episode 16800: Total Loss of tensor([[4.5184]], grad_fn=<SubBackward0>)
[2022-11-05 18:23:08.790861] Process 3. Episode 16650, average_reward -0.073574
Episode 16650: Total Loss of tensor([[7.5195]], grad_fn=<SubBackward0>)
[2022-11-05 18:24:32.259453] Process 1. Episode 16500, average_reward -0.068000
Episode 16500: Total Loss of tensor([[-22.4128]], grad_fn=<SubBackward0>)
[2022-11-05 18:24:50.539478] Process 4. Episode 17850, average_reward -0.072045
Episode 17850: Total Loss of tensor([[10.1245]], grad_fn=<SubBackward0>)
[2022-11-05 18:25:12.924420] Process 2. Episode 17150, average_reward -0.075685
Episode 17150: Total Loss of tensor([[-125.1816]], grad_fn=<SubBackward0>)
[2022-11-05 18:25:16.130299] Process 5. Episode 17000, average_reward -0.070000
Episode 17000: Total Loss of tensor([[19.8601]], grad_fn=<SubBackward0>)
[2022-11-05 18:25:36.914110] Process 0. Episode 16850, average_reward -0.070564
Episode 16850: Total Loss of tensor([[9.7222]], grad_fn=<SubBackward0>)
[2022-11-05 18:25:40.975721] Process 3. Episode 16700, average_reward -0.073593
Episode 16700: Total Loss of tensor([[10.0837]], grad_fn=<SubBackward0>)
[2022-11-05 18:27:06.048254] Process 1. Episode 16550, average_reward -0.067976
Episode 16550: Total Loss of tensor([[-41.9839]], grad_fn=<SubBackward0>)
[2022-11-05 18:27:09.953782] Process 4. Episode 17900, average_reward -0.072123
Episode 17900: Total Loss of tensor([[-96.0673]], grad_fn=<SubBackward0>)
[2022-11-05 18:27:32.961284] Process 2. Episode 17200, average_reward -0.075814
Episode 17200: Total Loss of tensor([[-2.4137]], grad_fn=<SubBackward0>)
[2022-11-05 18:27:44.556980] Process 5. Episode 17050, average_reward -0.070029
Episode 17050: Total Loss of tensor([[10.9589]], grad_fn=<SubBackward0>)
[2022-11-05 18:28:14.555999] Process 0. Episode 16900, average_reward -0.070473
Episode 16900: Total Loss of tensor([[2.0765]], grad_fn=<SubBackward0>)
[2022-11-05 18:28:25.734143] Process 3. Episode 16750, average_reward -0.073373
Episode 16750: Total Loss of tensor([[11.7839]], grad_fn=<SubBackward0>)
[2022-11-05 18:29:29.672420] Process 4. Episode 17950, average_reward -0.072089
Episode 17950: Total Loss of tensor([[5.9597]], grad_fn=<SubBackward0>)
[2022-11-05 18:29:42.154725] Process 1. Episode 16600, average_reward -0.067892
Episode 16600: Total Loss of tensor([[6.9189]], grad_fn=<SubBackward0>)
[2022-11-05 18:29:53.695631] Process 2. Episode 17250, average_reward -0.075884
Episode 17250: Total Loss of tensor([[9.6327]], grad_fn=<SubBackward0>)
[2022-11-05 18:30:12.146944] Process 5. Episode 17100, average_reward -0.070117
Episode 17100: Total Loss of tensor([[-9.2081]], grad_fn=<SubBackward0>)
[2022-11-05 18:30:58.590536] Process 0. Episode 16950, average_reward -0.070560
Episode 16950: Total Loss of tensor([[13.2385]], grad_fn=<SubBackward0>)
[2022-11-05 18:31:04.097020] Process 3. Episode 16800, average_reward -0.073452
Episode 16800: Total Loss of tensor([[6.7633]], grad_fn=<SubBackward0>)
[2022-11-05 18:31:48.973183] Process 4. Episode 18000, average_reward -0.072056
Episode 18000: Total Loss of tensor([[15.4426]], grad_fn=<SubBackward0>)
[2022-11-05 18:32:10.848098] Process 1. Episode 16650, average_reward -0.067988
Episode 16650: Total Loss of tensor([[8.8483]], grad_fn=<SubBackward0>)
[2022-11-05 18:32:18.350143] Process 2. Episode 17300, average_reward -0.075896
Episode 17300: Total Loss of tensor([[-11.5691]], grad_fn=<SubBackward0>)
[2022-11-05 18:32:34.834628] Process 5. Episode 17150, average_reward -0.070146
Episode 17150: Total Loss of tensor([[-20.6720]], grad_fn=<SubBackward0>)
[2022-11-05 18:33:28.411025] Process 3. Episode 16850, average_reward -0.073472
Episode 16850: Total Loss of tensor([[-5.3123]], grad_fn=<SubBackward0>)
[2022-11-05 18:33:36.305412] Process 0. Episode 17000, average_reward -0.070412
Episode 17000: Total Loss of tensor([[13.4670]], grad_fn=<SubBackward0>)
[2022-11-05 18:34:11.992170] Process 4. Episode 18050, average_reward -0.072188
Episode 18050: Total Loss of tensor([[13.4249]], grad_fn=<SubBackward0>)
[2022-11-05 18:34:42.052683] Process 2. Episode 17350, average_reward -0.075908
Episode 17350: Total Loss of tensor([[-3.3336]], grad_fn=<SubBackward0>)
[2022-11-05 18:34:57.031685] Process 1. Episode 16700, average_reward -0.067964
Episode 16700: Total Loss of tensor([[-1.9612]], grad_fn=<SubBackward0>)
[2022-11-05 18:35:00.343571] Process 5. Episode 17200, average_reward -0.070116
Episode 17200: Total Loss of tensor([[-119.9544]], grad_fn=<SubBackward0>)
[2022-11-05 18:35:51.178423] Process 3. Episode 16900, average_reward -0.073491
Episode 16900: Total Loss of tensor([[14.2406]], grad_fn=<SubBackward0>)
[2022-11-05 18:36:20.693544] Process 0. Episode 17050, average_reward -0.070381
Episode 17050: Total Loss of tensor([[-2.2786]], grad_fn=<SubBackward0>)
[2022-11-05 18:36:32.277667] Process 4. Episode 18100, average_reward -0.072044
Episode 18100: Total Loss of tensor([[2.0218]], grad_fn=<SubBackward0>)
[2022-11-05 18:37:05.379590] Process 2. Episode 17400, average_reward -0.075977
Episode 17400: Total Loss of tensor([[12.0239]], grad_fn=<SubBackward0>)
[2022-11-05 18:37:28.763731] Process 5. Episode 17250, average_reward -0.070087
Episode 17250: Total Loss of tensor([[10.8599]], grad_fn=<SubBackward0>)
[2022-11-05 18:37:37.152108] Process 1. Episode 16750, average_reward -0.067940
Episode 16750: Total Loss of tensor([[-103.1790]], grad_fn=<SubBackward0>)
[2022-11-05 18:38:24.658294] Process 3. Episode 16950, average_reward -0.073569
Episode 16950: Total Loss of tensor([[1.9610]], grad_fn=<SubBackward0>)
[2022-11-05 18:38:49.668922] Process 4. Episode 18150, average_reward -0.072121
Episode 18150: Total Loss of tensor([[11.7551]], grad_fn=<SubBackward0>)
[2022-11-05 18:38:51.925354] Process 0. Episode 17100, average_reward -0.070292
Episode 17100: Total Loss of tensor([[-21.8249]], grad_fn=<SubBackward0>)
[2022-11-05 18:39:31.998797] Process 2. Episode 17450, average_reward -0.075989
Episode 17450: Total Loss of tensor([[6.1673]], grad_fn=<SubBackward0>)
[2022-11-05 18:39:57.635301] Process 5. Episode 17300, average_reward -0.070058
Episode 17300: Total Loss of tensor([[12.6714]], grad_fn=<SubBackward0>)
[2022-11-05 18:40:12.081749] Process 1. Episode 16800, average_reward -0.067857
Episode 16800: Total Loss of tensor([[2.2033]], grad_fn=<SubBackward0>)
[2022-11-05 18:40:50.626078] Process 3. Episode 17000, average_reward -0.073529
Episode 17000: Total Loss of tensor([[6.6185]], grad_fn=<SubBackward0>)
[2022-11-05 18:41:11.251295] Process 4. Episode 18200, average_reward -0.072033
Episode 18200: Total Loss of tensor([[7.8390]], grad_fn=<SubBackward0>)
[2022-11-05 18:41:33.897245] Process 0. Episode 17150, average_reward -0.070379
Episode 17150: Total Loss of tensor([[8.6911]], grad_fn=<SubBackward0>)
[2022-11-05 18:42:09.868305] Process 2. Episode 17500, average_reward -0.076000
Episode 17500: Total Loss of tensor([[6.9946]], grad_fn=<SubBackward0>)
[2022-11-05 18:42:18.960937] Process 5. Episode 17350, average_reward -0.069914
Episode 17350: Total Loss of tensor([[-4.2959]], grad_fn=<SubBackward0>)
[2022-11-05 18:42:50.384163] Process 1. Episode 16850, average_reward -0.067774
Episode 16850: Total Loss of tensor([[5.0768]], grad_fn=<SubBackward0>)
[2022-11-05 18:43:26.462038] Process 3. Episode 17050, average_reward -0.073372
Episode 17050: Total Loss of tensor([[3.0590]], grad_fn=<SubBackward0>)
[2022-11-05 18:43:38.241726] Process 4. Episode 18250, average_reward -0.072110
Episode 18250: Total Loss of tensor([[8.2509]], grad_fn=<SubBackward0>)
[2022-11-05 18:44:02.407554] Process 0. Episode 17200, average_reward -0.070349
Episode 17200: Total Loss of tensor([[-93.4032]], grad_fn=<SubBackward0>)
[2022-11-05 18:44:34.992359] Process 2. Episode 17550, average_reward -0.075954
Episode 17550: Total Loss of tensor([[3.2992]], grad_fn=<SubBackward0>)
[2022-11-05 18:44:41.448144] Process 5. Episode 17400, average_reward -0.069885
Episode 17400: Total Loss of tensor([[9.2593]], grad_fn=<SubBackward0>)
[2022-11-05 18:45:30.283747] Process 1. Episode 16900, average_reward -0.067870
Episode 16900: Total Loss of tensor([[-0.3862]], grad_fn=<SubBackward0>)
[2022-11-05 18:45:57.891519] Process 4. Episode 18300, average_reward -0.072131
Episode 18300: Total Loss of tensor([[-0.2364]], grad_fn=<SubBackward0>)
[2022-11-05 18:46:05.710677] Process 3. Episode 17100, average_reward -0.073333
Episode 17100: Total Loss of tensor([[12.7979]], grad_fn=<SubBackward0>)
[2022-11-05 18:46:25.969245] Process 0. Episode 17250, average_reward -0.070145
Episode 17250: Total Loss of tensor([[-0.3243]], grad_fn=<SubBackward0>)
[2022-11-05 18:46:58.090338] Process 2. Episode 17600, average_reward -0.075739
Episode 17600: Total Loss of tensor([[14.6757]], grad_fn=<SubBackward0>)
[2022-11-05 18:47:11.687316] Process 5. Episode 17450, average_reward -0.069799
Episode 17450: Total Loss of tensor([[2.7566]], grad_fn=<SubBackward0>)
[2022-11-05 18:48:08.438880] Process 1. Episode 16950, average_reward -0.067847
Episode 16950: Total Loss of tensor([[6.4769]], grad_fn=<SubBackward0>)
[2022-11-05 18:48:18.479475] Process 4. Episode 18350, average_reward -0.072153
Episode 18350: Total Loss of tensor([[3.0414]], grad_fn=<SubBackward0>)
[2022-11-05 18:48:33.453659] Process 3. Episode 17150, average_reward -0.073411
Episode 17150: Total Loss of tensor([[-33.7658]], grad_fn=<SubBackward0>)
[2022-11-05 18:48:55.104042] Process 0. Episode 17300, average_reward -0.070231
Episode 17300: Total Loss of tensor([[2.2415]], grad_fn=<SubBackward0>)
[2022-11-05 18:49:20.206178] Process 2. Episode 17650, average_reward -0.075694
Episode 17650: Total Loss of tensor([[-105.1505]], grad_fn=<SubBackward0>)
[2022-11-05 18:49:37.251412] Process 5. Episode 17500, average_reward -0.069714
Episode 17500: Total Loss of tensor([[13.1224]], grad_fn=<SubBackward0>)
[2022-11-05 18:50:38.109108] Process 4. Episode 18400, average_reward -0.072120
Episode 18400: Total Loss of tensor([[11.6167]], grad_fn=<SubBackward0>)
[2022-11-05 18:50:40.262856] Process 1. Episode 17000, average_reward -0.067824
Episode 17000: Total Loss of tensor([[-15.8313]], grad_fn=<SubBackward0>)
[2022-11-05 18:51:13.159241] Process 3. Episode 17200, average_reward -0.073372
Episode 17200: Total Loss of tensor([[12.6638]], grad_fn=<SubBackward0>)
[2022-11-05 18:51:27.045809] Process 0. Episode 17350, average_reward -0.070202
Episode 17350: Total Loss of tensor([[1.7073]], grad_fn=<SubBackward0>)
[2022-11-05 18:51:43.442558] Process 2. Episode 17700, average_reward -0.075819
Episode 17700: Total Loss of tensor([[-0.5492]], grad_fn=<SubBackward0>)
[2022-11-05 18:52:03.736830] Process 5. Episode 17550, average_reward -0.069687
Episode 17550: Total Loss of tensor([[4.8216]], grad_fn=<SubBackward0>)
[2022-11-05 18:53:02.952012] Process 4. Episode 18450, average_reward -0.072141
Episode 18450: Total Loss of tensor([[17.3517]], grad_fn=<SubBackward0>)
[2022-11-05 18:53:15.290041] Process 1. Episode 17050, average_reward -0.068035
Episode 17050: Total Loss of tensor([[-10.6154]], grad_fn=<SubBackward0>)
[2022-11-05 18:53:48.259333] Process 3. Episode 17250, average_reward -0.073391
Episode 17250: Total Loss of tensor([[5.6008]], grad_fn=<SubBackward0>)
[2022-11-05 18:53:49.909270] Process 0. Episode 17400, average_reward -0.070287
Episode 17400: Total Loss of tensor([[6.7374]], grad_fn=<SubBackward0>)
[2022-11-05 18:54:04.783230] Process 2. Episode 17750, average_reward -0.075831
Episode 17750: Total Loss of tensor([[-4.0918]], grad_fn=<SubBackward0>)
[2022-11-05 18:54:28.387054] Process 5. Episode 17600, average_reward -0.069659
Episode 17600: Total Loss of tensor([[-1.0720]], grad_fn=<SubBackward0>)
[2022-11-05 18:55:30.245815] Process 4. Episode 18500, average_reward -0.072162
Episode 18500: Total Loss of tensor([[6.5131]], grad_fn=<SubBackward0>)
[2022-11-05 18:55:47.617643] Process 1. Episode 17100, average_reward -0.068129
Episode 17100: Total Loss of tensor([[-9.0891]], grad_fn=<SubBackward0>)
[2022-11-05 18:56:16.911999] Process 0. Episode 17450, average_reward -0.070086
Episode 17450: Total Loss of tensor([[-2.3136]], grad_fn=<SubBackward0>)
[2022-11-05 18:56:17.504981] Process 3. Episode 17300, average_reward -0.073353
Episode 17300: Total Loss of tensor([[10.5512]], grad_fn=<SubBackward0>)
[2022-11-05 18:56:29.488929] Process 2. Episode 17800, average_reward -0.075787
Episode 17800: Total Loss of tensor([[13.9440]], grad_fn=<SubBackward0>)
[2022-11-05 18:56:56.509121] Process 5. Episode 17650, average_reward -0.069688
Episode 17650: Total Loss of tensor([[3.5700]], grad_fn=<SubBackward0>)
[2022-11-05 18:57:52.644148] Process 4. Episode 18550, average_reward -0.072183
Episode 18550: Total Loss of tensor([[-30.2628]], grad_fn=<SubBackward0>)
[2022-11-05 18:58:12.254755] Process 1. Episode 17150, average_reward -0.068047
Episode 17150: Total Loss of tensor([[0.2210]], grad_fn=<SubBackward0>)
[2022-11-05 18:58:49.533903] Process 0. Episode 17500, average_reward -0.070000
Episode 17500: Total Loss of tensor([[13.4271]], grad_fn=<SubBackward0>)
[2022-11-05 18:58:53.137754] Process 2. Episode 17850, average_reward -0.075686
Episode 17850: Total Loss of tensor([[-36.2253]], grad_fn=<SubBackward0>)
[2022-11-05 18:58:56.293020] Process 3. Episode 17350, average_reward -0.073314
Episode 17350: Total Loss of tensor([[3.5908]], grad_fn=<SubBackward0>)
[2022-11-05 18:59:29.242088] Process 5. Episode 17700, average_reward -0.069718
Episode 17700: Total Loss of tensor([[-24.5568]], grad_fn=<SubBackward0>)
[2022-11-05 19:00:13.125731] Process 4. Episode 18600, average_reward -0.072151
Episode 18600: Total Loss of tensor([[10.6625]], grad_fn=<SubBackward0>)
[2022-11-05 19:00:43.038039] Process 1. Episode 17200, average_reward -0.068140
Episode 17200: Total Loss of tensor([[12.2187]], grad_fn=<SubBackward0>)
[2022-11-05 19:01:11.943649] Process 0. Episode 17550, average_reward -0.069972
Episode 17550: Total Loss of tensor([[1.8285]], grad_fn=<SubBackward0>)
[2022-11-05 19:01:21.979971] Process 2. Episode 17900, average_reward -0.075642
Episode 17900: Total Loss of tensor([[11.5076]], grad_fn=<SubBackward0>)
[2022-11-05 19:01:23.561740] Process 3. Episode 17400, average_reward -0.073218
Episode 17400: Total Loss of tensor([[11.6071]], grad_fn=<SubBackward0>)
[2022-11-05 19:01:53.587605] Process 5. Episode 17750, average_reward -0.069746
Episode 17750: Total Loss of tensor([[0.9448]], grad_fn=<SubBackward0>)
[2022-11-05 19:02:36.668780] Process 4. Episode 18650, average_reward -0.072064
Episode 18650: Total Loss of tensor([[14.1141]], grad_fn=<SubBackward0>)
[2022-11-05 19:03:19.333814] Process 1. Episode 17250, average_reward -0.068174
Episode 17250: Total Loss of tensor([[4.5898]], grad_fn=<SubBackward0>)
[2022-11-05 19:03:41.688649] Process 2. Episode 17950, average_reward -0.075543
Episode 17950: Total Loss of tensor([[-47.7149]], grad_fn=<SubBackward0>)
[2022-11-05 19:03:51.079433] Process 0. Episode 17600, average_reward -0.069943
Episode 17600: Total Loss of tensor([[-17.0178]], grad_fn=<SubBackward0>)
[2022-11-05 19:03:55.387306] Process 3. Episode 17450, average_reward -0.073238
Episode 17450: Total Loss of tensor([[-117.4636]], grad_fn=<SubBackward0>)
[2022-11-05 19:04:19.478949] Process 5. Episode 17800, average_reward -0.069607
Episode 17800: Total Loss of tensor([[12.8517]], grad_fn=<SubBackward0>)
[2022-11-05 19:04:56.923653] Process 4. Episode 18700, average_reward -0.072086
Episode 18700: Total Loss of tensor([[12.2835]], grad_fn=<SubBackward0>)
[2022-11-05 19:06:02.570279] Process 2. Episode 18000, average_reward -0.075667
Episode 18000: Total Loss of tensor([[7.0017]], grad_fn=<SubBackward0>)
[2022-11-05 19:06:06.604222] Process 1. Episode 17300, average_reward -0.068035
Episode 17300: Total Loss of tensor([[4.5418]], grad_fn=<SubBackward0>)
[2022-11-05 19:06:16.965329] Process 3. Episode 17500, average_reward -0.073429
Episode 17500: Total Loss of tensor([[-121.2171]], grad_fn=<SubBackward0>)
[2022-11-05 19:06:24.524617] Process 0. Episode 17650, average_reward -0.069858
Episode 17650: Total Loss of tensor([[14.6542]], grad_fn=<SubBackward0>)
[2022-11-05 19:06:45.524271] Process 5. Episode 17850, average_reward -0.069692
Episode 17850: Total Loss of tensor([[15.8189]], grad_fn=<SubBackward0>)
[2022-11-05 19:07:20.968225] Process 4. Episode 18750, average_reward -0.072053
Episode 18750: Total Loss of tensor([[-100.1255]], grad_fn=<SubBackward0>)
[2022-11-05 19:08:29.494866] Process 2. Episode 18050, average_reward -0.075679
Episode 18050: Total Loss of tensor([[21.3562]], grad_fn=<SubBackward0>)
[2022-11-05 19:08:38.589989] Process 1. Episode 17350, average_reward -0.068184
Episode 17350: Total Loss of tensor([[-1.4914]], grad_fn=<SubBackward0>)
[2022-11-05 19:08:49.755666] Process 3. Episode 17550, average_reward -0.073504
Episode 17550: Total Loss of tensor([[15.5440]], grad_fn=<SubBackward0>)
[2022-11-05 19:08:56.109754] Process 0. Episode 17700, average_reward -0.069774
Episode 17700: Total Loss of tensor([[0.7271]], grad_fn=<SubBackward0>)
[2022-11-05 19:09:11.201899] Process 5. Episode 17900, average_reward -0.069888
Episode 17900: Total Loss of tensor([[1.6259]], grad_fn=<SubBackward0>)
[2022-11-05 19:09:47.255848] Process 4. Episode 18800, average_reward -0.072074
Episode 18800: Total Loss of tensor([[-116.2812]], grad_fn=<SubBackward0>)
[2022-11-05 19:10:57.131426] Process 2. Episode 18100, average_reward -0.075470
Episode 18100: Total Loss of tensor([[-5.7754]], grad_fn=<SubBackward0>)
[2022-11-05 19:11:06.692512] Process 1. Episode 17400, average_reward -0.068218
Episode 17400: Total Loss of tensor([[3.9412]], grad_fn=<SubBackward0>)
[2022-11-05 19:11:15.146347] Process 3. Episode 17600, average_reward -0.073636
Episode 17600: Total Loss of tensor([[-4.3737]], grad_fn=<SubBackward0>)
[2022-11-05 19:11:31.936628] Process 0. Episode 17750, average_reward -0.069803
Episode 17750: Total Loss of tensor([[12.0816]], grad_fn=<SubBackward0>)
[2022-11-05 19:11:41.818750] Process 5. Episode 17950, average_reward -0.069694
Episode 17950: Total Loss of tensor([[10.6797]], grad_fn=<SubBackward0>)
[2022-11-05 19:12:12.395980] Process 4. Episode 18850, average_reward -0.072095
Episode 18850: Total Loss of tensor([[13.1458]], grad_fn=<SubBackward0>)
[2022-11-05 19:13:24.888840] Process 2. Episode 18150, average_reward -0.075537
Episode 18150: Total Loss of tensor([[-3.5760]], grad_fn=<SubBackward0>)
[2022-11-05 19:13:42.674287] Process 1. Episode 17450, average_reward -0.068252
Episode 17450: Total Loss of tensor([[-2.3239]], grad_fn=<SubBackward0>)
[2022-11-05 19:13:42.864489] Process 3. Episode 17650, average_reward -0.073598
Episode 17650: Total Loss of tensor([[4.6425]], grad_fn=<SubBackward0>)
[2022-11-05 19:13:56.293494] Process 0. Episode 17800, average_reward -0.069831
Episode 17800: Total Loss of tensor([[-2.5197]], grad_fn=<SubBackward0>)
[2022-11-05 19:14:03.307841] Process 5. Episode 18000, average_reward -0.069778
Episode 18000: Total Loss of tensor([[12.9501]], grad_fn=<SubBackward0>)
[2022-11-05 19:14:36.518219] Process 4. Episode 18900, average_reward -0.072011
Episode 18900: Total Loss of tensor([[12.1331]], grad_fn=<SubBackward0>)
[2022-11-05 19:16:05.260128] Process 2. Episode 18200, average_reward -0.075495
Episode 18200: Total Loss of tensor([[2.8186]], grad_fn=<SubBackward0>)
[2022-11-05 19:16:08.894017] Process 3. Episode 17700, average_reward -0.073559
Episode 17700: Total Loss of tensor([[6.8368]], grad_fn=<SubBackward0>)
[2022-11-05 19:16:16.420828] Process 1. Episode 17500, average_reward -0.068286
Episode 17500: Total Loss of tensor([[6.5018]], grad_fn=<SubBackward0>)
[2022-11-05 19:16:24.106888] Process 0. Episode 17850, average_reward -0.069860
Episode 17850: Total Loss of tensor([[10.6796]], grad_fn=<SubBackward0>)
[2022-11-05 19:16:33.628880] Process 5. Episode 18050, average_reward -0.069751
Episode 18050: Total Loss of tensor([[8.4531]], grad_fn=<SubBackward0>)
[2022-11-05 19:16:54.409172] Process 4. Episode 18950, average_reward -0.072032
Episode 18950: Total Loss of tensor([[3.5503]], grad_fn=<SubBackward0>)
[2022-11-05 19:18:33.992054] Process 2. Episode 18250, average_reward -0.075562
Episode 18250: Total Loss of tensor([[4.1276]], grad_fn=<SubBackward0>)
[2022-11-05 19:18:37.362254] Process 3. Episode 17750, average_reward -0.073577
Episode 17750: Total Loss of tensor([[8.7827]], grad_fn=<SubBackward0>)
[2022-11-05 19:18:54.630724] Process 1. Episode 17550, average_reward -0.068490
Episode 17550: Total Loss of tensor([[0.8504]], grad_fn=<SubBackward0>)
[2022-11-05 19:18:57.964223] Process 5. Episode 18100, average_reward -0.069724
Episode 18100: Total Loss of tensor([[-71.2226]], grad_fn=<SubBackward0>)
[2022-11-05 19:19:01.726613] Process 0. Episode 17900, average_reward -0.069888
Episode 17900: Total Loss of tensor([[9.7335]], grad_fn=<SubBackward0>)
[2022-11-05 19:19:13.241138] Process 4. Episode 19000, average_reward -0.072053
Episode 19000: Total Loss of tensor([[13.3372]], grad_fn=<SubBackward0>)
[2022-11-05 19:21:00.511900] Process 2. Episode 18300, average_reward -0.075683
Episode 18300: Total Loss of tensor([[3.5780]], grad_fn=<SubBackward0>)
[2022-11-05 19:21:09.488967] Process 3. Episode 17800, average_reward -0.073764
Episode 17800: Total Loss of tensor([[9.4056]], grad_fn=<SubBackward0>)
[2022-11-05 19:21:24.191790] Process 1. Episode 17600, average_reward -0.068580
Episode 17600: Total Loss of tensor([[22.9235]], grad_fn=<SubBackward0>)
[2022-11-05 19:21:28.180755] Process 0. Episode 17950, average_reward -0.070028
Episode 17950: Total Loss of tensor([[-58.5316]], grad_fn=<SubBackward0>)
[2022-11-05 19:21:33.441346] Process 5. Episode 18150, average_reward -0.069697
Episode 18150: Total Loss of tensor([[25.2634]], grad_fn=<SubBackward0>)
[2022-11-05 19:21:35.883182] Process 4. Episode 19050, average_reward -0.072021
Episode 19050: Total Loss of tensor([[9.8127]], grad_fn=<SubBackward0>)
[2022-11-05 19:23:26.767429] Process 2. Episode 18350, average_reward -0.075640
Episode 18350: Total Loss of tensor([[-22.3570]], grad_fn=<SubBackward0>)
[2022-11-05 19:23:36.929409] Process 3. Episode 17850, average_reward -0.073782
Episode 17850: Total Loss of tensor([[-0.1909]], grad_fn=<SubBackward0>)
[2022-11-05 19:23:54.461586] Process 4. Episode 19100, average_reward -0.072094
Episode 19100: Total Loss of tensor([[15.5574]], grad_fn=<SubBackward0>)
[2022-11-05 19:23:55.818220] Process 0. Episode 18000, average_reward -0.069944
Episode 18000: Total Loss of tensor([[11.9014]], grad_fn=<SubBackward0>)
[2022-11-05 19:24:00.200081] Process 1. Episode 17650, average_reward -0.068555
Episode 17650: Total Loss of tensor([[-2.0799]], grad_fn=<SubBackward0>)
[2022-11-05 19:24:10.567938] Process 5. Episode 18200, average_reward -0.069560
Episode 18200: Total Loss of tensor([[-12.9272]], grad_fn=<SubBackward0>)
[2022-11-05 19:25:56.078674] Process 2. Episode 18400, average_reward -0.075652
Episode 18400: Total Loss of tensor([[1.9231]], grad_fn=<SubBackward0>)
[2022-11-05 19:26:08.220447] Process 3. Episode 17900, average_reward -0.073799
Episode 17900: Total Loss of tensor([[8.0287]], grad_fn=<SubBackward0>)
[2022-11-05 19:26:14.813480] Process 4. Episode 19150, average_reward -0.072115
Episode 19150: Total Loss of tensor([[-110.0828]], grad_fn=<SubBackward0>)
[2022-11-05 19:26:33.744046] Process 0. Episode 18050, average_reward -0.070028
Episode 18050: Total Loss of tensor([[18.6848]], grad_fn=<SubBackward0>)
[2022-11-05 19:26:39.065748] Process 1. Episode 17700, average_reward -0.068531
Episode 17700: Total Loss of tensor([[-0.7637]], grad_fn=<SubBackward0>)
[2022-11-05 19:26:39.865528] Process 5. Episode 18250, average_reward -0.069479
Episode 18250: Total Loss of tensor([[0.4220]], grad_fn=<SubBackward0>)
[2022-11-05 19:28:27.185422] Process 2. Episode 18450, average_reward -0.075827
Episode 18450: Total Loss of tensor([[-23.4333]], grad_fn=<SubBackward0>)
[2022-11-05 19:28:33.386949] Process 4. Episode 19200, average_reward -0.072292
Episode 19200: Total Loss of tensor([[14.8025]], grad_fn=<SubBackward0>)
[2022-11-05 19:28:43.786827] Process 3. Episode 17950, average_reward -0.073816
Episode 17950: Total Loss of tensor([[19.9578]], grad_fn=<SubBackward0>)
[2022-11-05 19:29:04.872719] Process 0. Episode 18100, average_reward -0.069890
Episode 18100: Total Loss of tensor([[11.4823]], grad_fn=<SubBackward0>)
[2022-11-05 19:29:08.985069] Process 5. Episode 18300, average_reward -0.069563
Episode 18300: Total Loss of tensor([[21.9028]], grad_fn=<SubBackward0>)
[2022-11-05 19:29:17.244649] Process 1. Episode 17750, average_reward -0.068563
Episode 17750: Total Loss of tensor([[11.9802]], grad_fn=<SubBackward0>)
[2022-11-05 19:30:54.919475] Process 2. Episode 18500, average_reward -0.075784
Episode 18500: Total Loss of tensor([[4.7188]], grad_fn=<SubBackward0>)
[2022-11-05 19:30:55.488232] Process 4. Episode 19250, average_reward -0.072260
Episode 19250: Total Loss of tensor([[6.8319]], grad_fn=<SubBackward0>)
[2022-11-05 19:31:05.681450] Process 3. Episode 18000, average_reward -0.073833
Episode 18000: Total Loss of tensor([[10.6552]], grad_fn=<SubBackward0>)
[2022-11-05 19:31:31.736097] Process 5. Episode 18350, average_reward -0.069537
Episode 18350: Total Loss of tensor([[-38.8172]], grad_fn=<SubBackward0>)
[2022-11-05 19:31:35.987035] Process 0. Episode 18150, average_reward -0.069807
Episode 18150: Total Loss of tensor([[3.2417]], grad_fn=<SubBackward0>)
[2022-11-05 19:32:05.027454] Process 1. Episode 17800, average_reward -0.068539
Episode 17800: Total Loss of tensor([[6.1084]], grad_fn=<SubBackward0>)
[2022-11-05 19:33:17.101219] Process 2. Episode 18550, average_reward -0.075687
Episode 18550: Total Loss of tensor([[20.1411]], grad_fn=<SubBackward0>)
[2022-11-05 19:33:26.744846] Process 4. Episode 19300, average_reward -0.072228
Episode 19300: Total Loss of tensor([[23.4770]], grad_fn=<SubBackward0>)
[2022-11-05 19:33:27.489913] Process 3. Episode 18050, average_reward -0.073795
Episode 18050: Total Loss of tensor([[7.8921]], grad_fn=<SubBackward0>)
[2022-11-05 19:33:56.972109] Process 5. Episode 18400, average_reward -0.069511
Episode 18400: Total Loss of tensor([[16.3932]], grad_fn=<SubBackward0>)
[2022-11-05 19:34:02.306382] Process 0. Episode 18200, average_reward -0.069725
Episode 18200: Total Loss of tensor([[17.6757]], grad_fn=<SubBackward0>)
[2022-11-05 19:34:44.190697] Process 1. Episode 17850, average_reward -0.068796
Episode 17850: Total Loss of tensor([[5.8297]], grad_fn=<SubBackward0>)
[2022-11-05 19:35:37.663699] Process 2. Episode 18600, average_reward -0.075699
Episode 18600: Total Loss of tensor([[8.7202]], grad_fn=<SubBackward0>)
[2022-11-05 19:35:47.857079] Process 4. Episode 19350, average_reward -0.072300
Episode 19350: Total Loss of tensor([[3.8642]], grad_fn=<SubBackward0>)
[2022-11-05 19:35:58.819194] Process 3. Episode 18100, average_reward -0.073757
Episode 18100: Total Loss of tensor([[-2.8378]], grad_fn=<SubBackward0>)
[2022-11-05 19:36:25.695540] Process 5. Episode 18450, average_reward -0.069539
Episode 18450: Total Loss of tensor([[23.1793]], grad_fn=<SubBackward0>)
[2022-11-05 19:36:33.013032] Process 0. Episode 18250, average_reward -0.069534
Episode 18250: Total Loss of tensor([[10.6125]], grad_fn=<SubBackward0>)
[2022-11-05 19:37:15.052198] Process 1. Episode 17900, average_reward -0.068715
Episode 17900: Total Loss of tensor([[8.2440]], grad_fn=<SubBackward0>)
[2022-11-05 19:38:02.695267] Process 2. Episode 18650, average_reward -0.075871
Episode 18650: Total Loss of tensor([[-19.7020]], grad_fn=<SubBackward0>)
[2022-11-05 19:38:09.576028] Process 4. Episode 19400, average_reward -0.072268
Episode 19400: Total Loss of tensor([[22.9658]], grad_fn=<SubBackward0>)
[2022-11-05 19:38:25.269245] Process 3. Episode 18150, average_reward -0.073884
Episode 18150: Total Loss of tensor([[-1.3085]], grad_fn=<SubBackward0>)
[2022-11-05 19:38:50.547615] Process 5. Episode 18500, average_reward -0.069405
Episode 18500: Total Loss of tensor([[5.6350]], grad_fn=<SubBackward0>)
[2022-11-05 19:39:12.401779] Process 0. Episode 18300, average_reward -0.069399
Episode 18300: Total Loss of tensor([[1.4337]], grad_fn=<SubBackward0>)
[2022-11-05 19:39:53.937092] Process 1. Episode 17950, average_reward -0.068802
Episode 17950: Total Loss of tensor([[0.0172]], grad_fn=<SubBackward0>)
[2022-11-05 19:40:30.801301] Process 4. Episode 19450, average_reward -0.072288
Episode 19450: Total Loss of tensor([[4.5767]], grad_fn=<SubBackward0>)
[2022-11-05 19:40:34.366889] Process 2. Episode 18700, average_reward -0.075936
Episode 18700: Total Loss of tensor([[-14.8814]], grad_fn=<SubBackward0>)
[2022-11-05 19:40:49.654126] Process 3. Episode 18200, average_reward -0.073846
Episode 18200: Total Loss of tensor([[2.0245]], grad_fn=<SubBackward0>)
[2022-11-05 19:41:14.007724] Process 5. Episode 18550, average_reward -0.069380
Episode 18550: Total Loss of tensor([[12.4158]], grad_fn=<SubBackward0>)
[2022-11-05 19:41:47.983861] Process 0. Episode 18350, average_reward -0.069319
Episode 18350: Total Loss of tensor([[10.6599]], grad_fn=<SubBackward0>)
[2022-11-05 19:42:31.624486] Process 1. Episode 18000, average_reward -0.068778
Episode 18000: Total Loss of tensor([[2.2912]], grad_fn=<SubBackward0>)
[2022-11-05 19:42:50.244917] Process 4. Episode 19500, average_reward -0.072308
Episode 19500: Total Loss of tensor([[-112.2085]], grad_fn=<SubBackward0>)
[2022-11-05 19:42:59.854993] Process 2. Episode 18750, average_reward -0.075787
Episode 18750: Total Loss of tensor([[1.5696]], grad_fn=<SubBackward0>)
[2022-11-05 19:43:21.478617] Process 3. Episode 18250, average_reward -0.073753
Episode 18250: Total Loss of tensor([[-7.8666]], grad_fn=<SubBackward0>)
[2022-11-05 19:43:33.306068] Process 5. Episode 18600, average_reward -0.069301
Episode 18600: Total Loss of tensor([[-69.8338]], grad_fn=<SubBackward0>)
[2022-11-05 19:44:28.732147] Process 0. Episode 18400, average_reward -0.069185
Episode 18400: Total Loss of tensor([[9.1448]], grad_fn=<SubBackward0>)
[2022-11-05 19:45:02.950899] Process 1. Episode 18050, average_reward -0.068698
Episode 18050: Total Loss of tensor([[6.8454]], grad_fn=<SubBackward0>)
[2022-11-05 19:45:15.609642] Process 4. Episode 19550, average_reward -0.072174
Episode 19550: Total Loss of tensor([[14.3053]], grad_fn=<SubBackward0>)
[2022-11-05 19:45:25.068469] Process 2. Episode 18800, average_reward -0.075851
Episode 18800: Total Loss of tensor([[4.9474]], grad_fn=<SubBackward0>)
[2022-11-05 19:45:45.564116] Process 3. Episode 18300, average_reward -0.073607
Episode 18300: Total Loss of tensor([[-8.3879]], grad_fn=<SubBackward0>)
[2022-11-05 19:45:57.502585] Process 5. Episode 18650, average_reward -0.069330
Episode 18650: Total Loss of tensor([[6.2959]], grad_fn=<SubBackward0>)
[2022-11-05 19:47:08.214283] Process 0. Episode 18450, average_reward -0.069268
Episode 18450: Total Loss of tensor([[1.5545]], grad_fn=<SubBackward0>)
[2022-11-05 19:47:35.077119] Process 4. Episode 19600, average_reward -0.072143
Episode 19600: Total Loss of tensor([[2.9781]], grad_fn=<SubBackward0>)
[2022-11-05 19:47:36.560354] Process 1. Episode 18100, average_reward -0.068674
Episode 18100: Total Loss of tensor([[7.4158]], grad_fn=<SubBackward0>)
[2022-11-05 19:47:50.324629] Process 2. Episode 18850, average_reward -0.076021
Episode 18850: Total Loss of tensor([[12.0436]], grad_fn=<SubBackward0>)
[2022-11-05 19:48:13.059283] Process 3. Episode 18350, average_reward -0.073460
Episode 18350: Total Loss of tensor([[-3.7106]], grad_fn=<SubBackward0>)
[2022-11-05 19:48:26.052255] Process 5. Episode 18700, average_reward -0.069251
Episode 18700: Total Loss of tensor([[-5.4670]], grad_fn=<SubBackward0>)
[2022-11-05 19:49:52.136281] Process 0. Episode 18500, average_reward -0.069189
Episode 18500: Total Loss of tensor([[-123.7077]], grad_fn=<SubBackward0>)
[2022-11-05 19:49:56.284792] Process 4. Episode 19650, average_reward -0.072163
Episode 19650: Total Loss of tensor([[6.6207]], grad_fn=<SubBackward0>)
[2022-11-05 19:50:00.501163] Process 1. Episode 18150, average_reward -0.068595
Episode 18150: Total Loss of tensor([[-41.8037]], grad_fn=<SubBackward0>)
[2022-11-05 19:50:25.001536] Process 2. Episode 18900, average_reward -0.076032
Episode 18900: Total Loss of tensor([[6.7343]], grad_fn=<SubBackward0>)
[2022-11-05 19:50:39.267297] Process 3. Episode 18400, average_reward -0.073478
Episode 18400: Total Loss of tensor([[13.6639]], grad_fn=<SubBackward0>)
[2022-11-05 19:50:48.942912] Process 5. Episode 18750, average_reward -0.069333
Episode 18750: Total Loss of tensor([[12.7857]], grad_fn=<SubBackward0>)
[2022-11-05 19:52:13.723895] Process 4. Episode 19700, average_reward -0.072132
Episode 19700: Total Loss of tensor([[8.9237]], grad_fn=<SubBackward0>)
[2022-11-05 19:52:31.290371] Process 0. Episode 18550, average_reward -0.069111
Episode 18550: Total Loss of tensor([[16.9218]], grad_fn=<SubBackward0>)
[2022-11-05 19:52:40.124492] Process 1. Episode 18200, average_reward -0.068516
Episode 18200: Total Loss of tensor([[-7.2856]], grad_fn=<SubBackward0>)
[2022-11-05 19:53:03.701283] Process 2. Episode 18950, average_reward -0.075937
Episode 18950: Total Loss of tensor([[14.2235]], grad_fn=<SubBackward0>)
[2022-11-05 19:53:08.860341] Process 3. Episode 18450, average_reward -0.073333
Episode 18450: Total Loss of tensor([[0.7229]], grad_fn=<SubBackward0>)
[2022-11-05 19:53:11.777894] Process 5. Episode 18800, average_reward -0.069309
Episode 18800: Total Loss of tensor([[-3.0440]], grad_fn=<SubBackward0>)
[2022-11-05 19:54:36.442801] Process 4. Episode 19750, average_reward -0.072101
Episode 19750: Total Loss of tensor([[10.3527]], grad_fn=<SubBackward0>)
[2022-11-05 19:54:53.223712] Process 0. Episode 18600, average_reward -0.069194
Episode 18600: Total Loss of tensor([[11.2857]], grad_fn=<SubBackward0>)
[2022-11-05 19:55:11.208940] Process 1. Episode 18250, average_reward -0.068384
Episode 18250: Total Loss of tensor([[-24.0620]], grad_fn=<SubBackward0>)
[2022-11-05 19:55:35.531007] Process 2. Episode 19000, average_reward -0.075789
Episode 19000: Total Loss of tensor([[-8.3821]], grad_fn=<SubBackward0>)
[2022-11-05 19:55:40.079281] Process 5. Episode 18850, average_reward -0.069337
Episode 18850: Total Loss of tensor([[12.2933]], grad_fn=<SubBackward0>)
[2022-11-05 19:55:44.631028] Process 3. Episode 18500, average_reward -0.073351
Episode 18500: Total Loss of tensor([[13.1434]], grad_fn=<SubBackward0>)
[2022-11-05 19:56:54.402882] Process 4. Episode 19800, average_reward -0.071970
Episode 19800: Total Loss of tensor([[9.8043]], grad_fn=<SubBackward0>)
[2022-11-05 19:57:15.427383] Process 0. Episode 18650, average_reward -0.069062
Episode 18650: Total Loss of tensor([[6.7525]], grad_fn=<SubBackward0>)
[2022-11-05 19:57:47.248244] Process 1. Episode 18300, average_reward -0.068470
Episode 18300: Total Loss of tensor([[4.6839]], grad_fn=<SubBackward0>)
[2022-11-05 19:58:10.590176] Process 5. Episode 18900, average_reward -0.069312
Episode 18900: Total Loss of tensor([[13.5851]], grad_fn=<SubBackward0>)
[2022-11-05 19:58:15.285135] Process 2. Episode 19050, average_reward -0.075696
Episode 19050: Total Loss of tensor([[-125.3338]], grad_fn=<SubBackward0>)
[2022-11-05 19:58:17.544055] Process 3. Episode 18550, average_reward -0.073261
Episode 18550: Total Loss of tensor([[4.9617]], grad_fn=<SubBackward0>)
[2022-11-05 19:59:12.366232] Process 4. Episode 19850, average_reward -0.071889
Episode 19850: Total Loss of tensor([[-6.0653]], grad_fn=<SubBackward0>)
[2022-11-05 19:59:39.314651] Process 0. Episode 18700, average_reward -0.069198
Episode 18700: Total Loss of tensor([[3.0166]], grad_fn=<SubBackward0>)
[2022-11-05 20:00:18.908943] Process 1. Episode 18350, average_reward -0.068447
Episode 18350: Total Loss of tensor([[9.3689]], grad_fn=<SubBackward0>)
[2022-11-05 20:00:35.289826] Process 5. Episode 18950, average_reward -0.069499
Episode 18950: Total Loss of tensor([[2.2551]], grad_fn=<SubBackward0>)
[2022-11-05 20:00:49.555102] Process 2. Episode 19100, average_reward -0.075497
Episode 19100: Total Loss of tensor([[12.9625]], grad_fn=<SubBackward0>)
[2022-11-05 20:00:55.876098] Process 3. Episode 18600, average_reward -0.073387
Episode 18600: Total Loss of tensor([[-29.9597]], grad_fn=<SubBackward0>)
[2022-11-05 20:01:33.377898] Process 4. Episode 19900, average_reward -0.071910
Episode 19900: Total Loss of tensor([[15.9031]], grad_fn=<SubBackward0>)
[2022-11-05 20:02:10.734489] Process 0. Episode 18750, average_reward -0.069173
Episode 18750: Total Loss of tensor([[16.9281]], grad_fn=<SubBackward0>)
[2022-11-05 20:02:55.782699] Process 5. Episode 19000, average_reward -0.069632
Episode 19000: Total Loss of tensor([[11.5321]], grad_fn=<SubBackward0>)
[2022-11-05 20:02:57.293916] Process 1. Episode 18400, average_reward -0.068587
Episode 18400: Total Loss of tensor([[6.3581]], grad_fn=<SubBackward0>)
[2022-11-05 20:03:15.622230] Process 2. Episode 19150, average_reward -0.075666
Episode 19150: Total Loss of tensor([[7.9599]], grad_fn=<SubBackward0>)
[2022-11-05 20:03:29.176677] Process 3. Episode 18650, average_reward -0.073405
Episode 18650: Total Loss of tensor([[7.7145]], grad_fn=<SubBackward0>)
[2022-11-05 20:03:53.290077] Process 4. Episode 19950, average_reward -0.071930
Episode 19950: Total Loss of tensor([[9.5439]], grad_fn=<SubBackward0>)
[2022-11-05 20:04:41.511134] Process 0. Episode 18800, average_reward -0.069043
Episode 18800: Total Loss of tensor([[9.3209]], grad_fn=<SubBackward0>)
[2022-11-05 20:05:32.024222] Process 1. Episode 18450, average_reward -0.068455
Episode 18450: Total Loss of tensor([[-5.5371]], grad_fn=<SubBackward0>)
[2022-11-05 20:05:35.625583] Process 5. Episode 19050, average_reward -0.069501
Episode 19050: Total Loss of tensor([[2.3885]], grad_fn=<SubBackward0>)
[2022-11-05 20:05:46.941412] Process 2. Episode 19200, average_reward -0.075521
Episode 19200: Total Loss of tensor([[5.9930]], grad_fn=<SubBackward0>)
[2022-11-05 20:05:58.688547] Process 3. Episode 18700, average_reward -0.073476
Episode 18700: Total Loss of tensor([[-2.1512]], grad_fn=<SubBackward0>)
[2022-11-05 20:06:16.817884] Process 4. Episode 20000, average_reward -0.071900
Episode 20000: Total Loss of tensor([[16.3858]], grad_fn=<SubBackward0>)
[2022-11-05 20:07:06.542621] Process 0. Episode 18850, average_reward -0.069019
Episode 18850: Total Loss of tensor([[0.0237]], grad_fn=<SubBackward0>)
[2022-11-05 20:07:56.104515] Process 1. Episode 18500, average_reward -0.068486
Episode 18500: Total Loss of tensor([[-5.7504]], grad_fn=<SubBackward0>)
[2022-11-05 20:08:06.177405] Process 5. Episode 19100, average_reward -0.069424
Episode 19100: Total Loss of tensor([[0.4815]], grad_fn=<SubBackward0>)
[2022-11-05 20:08:23.111608] Process 3. Episode 18750, average_reward -0.073333
Episode 18750: Total Loss of tensor([[-3.4924]], grad_fn=<SubBackward0>)
[2022-11-05 20:08:34.540686] Process 2. Episode 19250, average_reward -0.075532
Episode 19250: Total Loss of tensor([[-113.9752]], grad_fn=<SubBackward0>)
[2022-11-05 20:08:38.079482] Process 4. Episode 20050, average_reward -0.072020
Episode 20050: Total Loss of tensor([[4.3025]], grad_fn=<SubBackward0>)
[2022-11-05 20:09:31.318383] Process 0. Episode 18900, average_reward -0.068995
Episode 18900: Total Loss of tensor([[-0.9224]], grad_fn=<SubBackward0>)
[2022-11-05 20:10:30.814893] Process 1. Episode 18550, average_reward -0.068464
Episode 18550: Total Loss of tensor([[9.6097]], grad_fn=<SubBackward0>)
[2022-11-05 20:10:40.673937] Process 5. Episode 19150, average_reward -0.069399
Episode 19150: Total Loss of tensor([[12.9646]], grad_fn=<SubBackward0>)
[2022-11-05 20:10:45.033076] Process 3. Episode 18800, average_reward -0.073404
Episode 18800: Total Loss of tensor([[5.7167]], grad_fn=<SubBackward0>)
[2022-11-05 20:10:59.949200] Process 4. Episode 20100, average_reward -0.072040
Episode 20100: Total Loss of tensor([[10.6300]], grad_fn=<SubBackward0>)
[2022-11-05 20:11:15.811597] Process 2. Episode 19300, average_reward -0.075596
Episode 19300: Total Loss of tensor([[10.8387]], grad_fn=<SubBackward0>)
[2022-11-05 20:11:58.768460] Process 0. Episode 18950, average_reward -0.069024
Episode 18950: Total Loss of tensor([[14.3809]], grad_fn=<SubBackward0>)
[2022-11-05 20:12:58.490728] Process 1. Episode 18600, average_reward -0.068495
Episode 18600: Total Loss of tensor([[5.2011]], grad_fn=<SubBackward0>)
[2022-11-05 20:13:01.411372] Process 5. Episode 19200, average_reward -0.069427
Episode 19200: Total Loss of tensor([[4.2697]], grad_fn=<SubBackward0>)
[2022-11-05 20:13:09.598231] Process 3. Episode 18850, average_reward -0.073475
Episode 18850: Total Loss of tensor([[-0.9037]], grad_fn=<SubBackward0>)
[2022-11-05 20:13:19.668525] Process 4. Episode 20150, average_reward -0.071911
Episode 20150: Total Loss of tensor([[2.8105]], grad_fn=<SubBackward0>)
[2022-11-05 20:13:43.727591] Process 2. Episode 19350, average_reward -0.075452
Episode 19350: Total Loss of tensor([[3.1341]], grad_fn=<SubBackward0>)
[2022-11-05 20:14:39.756863] Process 0. Episode 19000, average_reward -0.069105
Episode 19000: Total Loss of tensor([[2.4856]], grad_fn=<SubBackward0>)
[2022-11-05 20:15:33.805466] Process 5. Episode 19250, average_reward -0.069403
Episode 19250: Total Loss of tensor([[9.2656]], grad_fn=<SubBackward0>)
[2022-11-05 20:15:37.260651] Process 4. Episode 20200, average_reward -0.071832
Episode 20200: Total Loss of tensor([[2.3832]], grad_fn=<SubBackward0>)
[2022-11-05 20:15:39.072652] Process 3. Episode 18900, average_reward -0.073492
Episode 18900: Total Loss of tensor([[3.3974]], grad_fn=<SubBackward0>)
[2022-11-05 20:15:39.209643] Process 1. Episode 18650, average_reward -0.068525
Episode 18650: Total Loss of tensor([[5.8861]], grad_fn=<SubBackward0>)
[2022-11-05 20:16:12.794415] Process 2. Episode 19400, average_reward -0.075567
Episode 19400: Total Loss of tensor([[14.1937]], grad_fn=<SubBackward0>)
[2022-11-05 20:17:15.580471] Process 0. Episode 19050, average_reward -0.069291
Episode 19050: Total Loss of tensor([[10.3447]], grad_fn=<SubBackward0>)
[2022-11-05 20:17:58.181517] Process 5. Episode 19300, average_reward -0.069430
Episode 19300: Total Loss of tensor([[-2.9203]], grad_fn=<SubBackward0>)
[2022-11-05 20:17:58.387575] Process 4. Episode 20250, average_reward -0.071901
Episode 20250: Total Loss of tensor([[4.3263]], grad_fn=<SubBackward0>)
[2022-11-05 20:18:07.460178] Process 3. Episode 18950, average_reward -0.073456
Episode 18950: Total Loss of tensor([[-4.3858]], grad_fn=<SubBackward0>)
[2022-11-05 20:18:12.515221] Process 1. Episode 18700, average_reward -0.068503
Episode 18700: Total Loss of tensor([[8.8774]], grad_fn=<SubBackward0>)
[2022-11-05 20:18:45.090760] Process 2. Episode 19450, average_reward -0.075476
Episode 19450: Total Loss of tensor([[-0.0785]], grad_fn=<SubBackward0>)
[2022-11-05 20:19:45.502259] Process 0. Episode 19100, average_reward -0.069110
Episode 19100: Total Loss of tensor([[1.7411]], grad_fn=<SubBackward0>)
[2022-11-05 20:20:16.115779] Process 4. Episode 20300, average_reward -0.071970
Episode 20300: Total Loss of tensor([[0.6061]], grad_fn=<SubBackward0>)
[2022-11-05 20:20:20.986028] Process 5. Episode 19350, average_reward -0.069302
Episode 19350: Total Loss of tensor([[7.0731]], grad_fn=<SubBackward0>)
[2022-11-05 20:20:32.760527] Process 3. Episode 19000, average_reward -0.073526
Episode 19000: Total Loss of tensor([[-15.8937]], grad_fn=<SubBackward0>)
[2022-11-05 20:20:55.306686] Process 1. Episode 18750, average_reward -0.068373
Episode 18750: Total Loss of tensor([[-5.9885]], grad_fn=<SubBackward0>)
[2022-11-05 20:21:21.370648] Process 2. Episode 19500, average_reward -0.075538
Episode 19500: Total Loss of tensor([[1.8185]], grad_fn=<SubBackward0>)
[2022-11-05 20:22:16.635926] Process 0. Episode 19150, average_reward -0.069138
Episode 19150: Total Loss of tensor([[6.8307]], grad_fn=<SubBackward0>)
[2022-11-05 20:22:34.914099] Process 4. Episode 20350, average_reward -0.071843
Episode 20350: Total Loss of tensor([[-5.4926]], grad_fn=<SubBackward0>)
[2022-11-05 20:22:49.524433] Process 5. Episode 19400, average_reward -0.069381
Episode 19400: Total Loss of tensor([[2.2734]], grad_fn=<SubBackward0>)
[2022-11-05 20:22:55.223146] Process 3. Episode 19050, average_reward -0.073438
Episode 19050: Total Loss of tensor([[-63.5268]], grad_fn=<SubBackward0>)
[2022-11-05 20:23:29.508616] Process 1. Episode 18800, average_reward -0.068298
Episode 18800: Total Loss of tensor([[-36.5658]], grad_fn=<SubBackward0>)
[2022-11-05 20:23:49.079087] Process 2. Episode 19550, average_reward -0.075499
Episode 19550: Total Loss of tensor([[0.1674]], grad_fn=<SubBackward0>)
[2022-11-05 20:24:46.804475] Process 0. Episode 19200, average_reward -0.069167
Episode 19200: Total Loss of tensor([[13.0286]], grad_fn=<SubBackward0>)
[2022-11-05 20:24:55.386115] Process 4. Episode 20400, average_reward -0.071863
Episode 20400: Total Loss of tensor([[10.5550]], grad_fn=<SubBackward0>)
[2022-11-05 20:25:17.492206] Process 3. Episode 19100, average_reward -0.073351
Episode 19100: Total Loss of tensor([[-12.3347]], grad_fn=<SubBackward0>)
[2022-11-05 20:25:21.076387] Process 5. Episode 19450, average_reward -0.069460
Episode 19450: Total Loss of tensor([[-80.1651]], grad_fn=<SubBackward0>)
[2022-11-05 20:26:06.640299] Process 1. Episode 18850, average_reward -0.068435
Episode 18850: Total Loss of tensor([[-19.4475]], grad_fn=<SubBackward0>)
[2022-11-05 20:26:22.799084] Process 2. Episode 19600, average_reward -0.075357
Episode 19600: Total Loss of tensor([[11.0155]], grad_fn=<SubBackward0>)
[2022-11-05 20:27:12.676710] Process 0. Episode 19250, average_reward -0.069247
Episode 19250: Total Loss of tensor([[7.6315]], grad_fn=<SubBackward0>)
[2022-11-05 20:27:13.496930] Process 4. Episode 20450, average_reward -0.071834
Episode 20450: Total Loss of tensor([[9.2419]], grad_fn=<SubBackward0>)
[2022-11-05 20:27:48.551742] Process 5. Episode 19500, average_reward -0.069436
Episode 19500: Total Loss of tensor([[15.4889]], grad_fn=<SubBackward0>)
[2022-11-05 20:27:48.958609] Process 3. Episode 19150, average_reward -0.073264
Episode 19150: Total Loss of tensor([[1.7054]], grad_fn=<SubBackward0>)
[2022-11-05 20:28:45.975497] Process 1. Episode 18900, average_reward -0.068307
Episode 18900: Total Loss of tensor([[7.6776]], grad_fn=<SubBackward0>)
[2022-11-05 20:28:53.982034] Process 2. Episode 19650, average_reward -0.075165
Episode 19650: Total Loss of tensor([[3.7539]], grad_fn=<SubBackward0>)
[2022-11-05 20:29:35.409184] Process 4. Episode 20500, average_reward -0.071756
Episode 20500: Total Loss of tensor([[6.0078]], grad_fn=<SubBackward0>)
[2022-11-05 20:29:38.787921] Process 0. Episode 19300, average_reward -0.069223
Episode 19300: Total Loss of tensor([[10.7219]], grad_fn=<SubBackward0>)
[2022-11-05 20:30:13.421066] Process 5. Episode 19550, average_reward -0.069463
Episode 19550: Total Loss of tensor([[4.6474]], grad_fn=<SubBackward0>)
[2022-11-05 20:30:14.168062] Process 3. Episode 19200, average_reward -0.073229
Episode 19200: Total Loss of tensor([[6.8616]], grad_fn=<SubBackward0>)
[2022-11-05 20:31:16.011890] Process 2. Episode 19700, average_reward -0.075076
Episode 19700: Total Loss of tensor([[0.2721]], grad_fn=<SubBackward0>)
[2022-11-05 20:31:18.932741] Process 1. Episode 18950, average_reward -0.068179
Episode 18950: Total Loss of tensor([[13.3231]], grad_fn=<SubBackward0>)
[2022-11-05 20:32:00.585745] Process 4. Episode 20550, average_reward -0.071679
Episode 20550: Total Loss of tensor([[11.8486]], grad_fn=<SubBackward0>)
[2022-11-05 20:32:15.848967] Process 0. Episode 19350, average_reward -0.069199
Episode 19350: Total Loss of tensor([[8.3178]], grad_fn=<SubBackward0>)
[2022-11-05 20:32:39.928454] Process 3. Episode 19250, average_reward -0.073195
Episode 19250: Total Loss of tensor([[6.6650]], grad_fn=<SubBackward0>)
[2022-11-05 20:32:50.128216] Process 5. Episode 19600, average_reward -0.069490
Episode 19600: Total Loss of tensor([[-19.4611]], grad_fn=<SubBackward0>)
[2022-11-05 20:33:37.958754] Process 2. Episode 19750, average_reward -0.075038
Episode 19750: Total Loss of tensor([[3.7415]], grad_fn=<SubBackward0>)
[2022-11-05 20:33:51.782583] Process 1. Episode 19000, average_reward -0.068368
Episode 19000: Total Loss of tensor([[-0.2273]], grad_fn=<SubBackward0>)
[2022-11-05 20:34:22.813447] Process 4. Episode 20600, average_reward -0.071699
Episode 20600: Total Loss of tensor([[1.5716]], grad_fn=<SubBackward0>)
[2022-11-05 20:34:51.178163] Process 0. Episode 19400, average_reward -0.069124
Episode 19400: Total Loss of tensor([[4.4806]], grad_fn=<SubBackward0>)
[2022-11-05 20:35:10.886078] Process 3. Episode 19300, average_reward -0.073212
Episode 19300: Total Loss of tensor([[0.3284]], grad_fn=<SubBackward0>)
[2022-11-05 20:35:34.319760] Process 5. Episode 19650, average_reward -0.069466
Episode 19650: Total Loss of tensor([[4.7455]], grad_fn=<SubBackward0>)
[2022-11-05 20:36:02.558048] Process 2. Episode 19800, average_reward -0.075051
Episode 19800: Total Loss of tensor([[6.9043]], grad_fn=<SubBackward0>)
[2022-11-05 20:36:25.499919] Process 1. Episode 19050, average_reward -0.068399
Episode 19050: Total Loss of tensor([[1.9221]], grad_fn=<SubBackward0>)
[2022-11-05 20:36:47.005145] Process 4. Episode 20650, average_reward -0.071622
Episode 20650: Total Loss of tensor([[2.5909]], grad_fn=<SubBackward0>)
[2022-11-05 20:37:22.396627] Process 0. Episode 19450, average_reward -0.069100
Episode 19450: Total Loss of tensor([[9.4579]], grad_fn=<SubBackward0>)
[2022-11-05 20:37:41.435737] Process 3. Episode 19350, average_reward -0.073127
Episode 19350: Total Loss of tensor([[-0.8362]], grad_fn=<SubBackward0>)
[2022-11-05 20:38:03.111229] Process 5. Episode 19700, average_reward -0.069442
Episode 19700: Total Loss of tensor([[8.5932]], grad_fn=<SubBackward0>)
[2022-11-05 20:38:25.456817] Process 2. Episode 19850, average_reward -0.075164
Episode 19850: Total Loss of tensor([[15.6836]], grad_fn=<SubBackward0>)
[2022-11-05 20:39:00.296031] Process 1. Episode 19100, average_reward -0.068534
Episode 19100: Total Loss of tensor([[-74.4904]], grad_fn=<SubBackward0>)
[2022-11-05 20:39:09.252555] Process 4. Episode 20700, average_reward -0.071739
Episode 20700: Total Loss of tensor([[27.3162]], grad_fn=<SubBackward0>)
[2022-11-05 20:39:52.157816] Process 0. Episode 19500, average_reward -0.069128
Episode 19500: Total Loss of tensor([[7.1807]], grad_fn=<SubBackward0>)
[2022-11-05 20:40:05.607694] Process 3. Episode 19400, average_reward -0.072938
Episode 19400: Total Loss of tensor([[0.1387]], grad_fn=<SubBackward0>)
[2022-11-05 20:40:37.100024] Process 5. Episode 19750, average_reward -0.069316
Episode 19750: Total Loss of tensor([[7.0674]], grad_fn=<SubBackward0>)
[2022-11-05 20:40:52.786012] Process 2. Episode 19900, average_reward -0.075075
Episode 19900: Total Loss of tensor([[5.0251]], grad_fn=<SubBackward0>)
[2022-11-05 20:41:28.435923] Process 4. Episode 20750, average_reward -0.071614
Episode 20750: Total Loss of tensor([[5.6964]], grad_fn=<SubBackward0>)
[2022-11-05 20:41:39.706854] Process 1. Episode 19150, average_reward -0.068407
Episode 19150: Total Loss of tensor([[9.3852]], grad_fn=<SubBackward0>)
[2022-11-05 20:42:20.768222] Process 0. Episode 19550, average_reward -0.069156
Episode 19550: Total Loss of tensor([[-16.1829]], grad_fn=<SubBackward0>)
[2022-11-05 20:42:29.140469] Process 3. Episode 19450, average_reward -0.072853
Episode 19450: Total Loss of tensor([[12.9116]], grad_fn=<SubBackward0>)
[2022-11-05 20:43:09.375064] Process 5. Episode 19800, average_reward -0.069444
Episode 19800: Total Loss of tensor([[10.4174]], grad_fn=<SubBackward0>)
[2022-11-05 20:43:16.372820] Process 2. Episode 19950, average_reward -0.075138
Episode 19950: Total Loss of tensor([[8.8349]], grad_fn=<SubBackward0>)
[2022-11-05 20:43:49.618732] Process 4. Episode 20800, average_reward -0.071538
Episode 20800: Total Loss of tensor([[25.3902]], grad_fn=<SubBackward0>)
[2022-11-05 20:44:16.750288] Process 1. Episode 19200, average_reward -0.068437
Episode 19200: Total Loss of tensor([[14.3394]], grad_fn=<SubBackward0>)
[2022-11-05 20:44:48.874296] Process 0. Episode 19600, average_reward -0.069184
Episode 19600: Total Loss of tensor([[-0.0192]], grad_fn=<SubBackward0>)
[2022-11-05 20:45:02.667819] Process 3. Episode 19500, average_reward -0.072974
Episode 19500: Total Loss of tensor([[21.5188]], grad_fn=<SubBackward0>)
[2022-11-05 20:45:44.052524] Process 2. Episode 20000, average_reward -0.075050
Episode 20000: Total Loss of tensor([[-5.4516]], grad_fn=<SubBackward0>)
[2022-11-05 20:45:45.644855] Process 5. Episode 19850, average_reward -0.069572
Episode 19850: Total Loss of tensor([[7.8524]], grad_fn=<SubBackward0>)
[2022-11-05 20:46:09.190828] Process 4. Episode 20850, average_reward -0.071607
Episode 20850: Total Loss of tensor([[2.4525]], grad_fn=<SubBackward0>)
[2022-11-05 20:47:01.980196] Process 1. Episode 19250, average_reward -0.068364
Episode 19250: Total Loss of tensor([[12.4426]], grad_fn=<SubBackward0>)
[2022-11-05 20:47:12.764850] Process 0. Episode 19650, average_reward -0.069262
Episode 19650: Total Loss of tensor([[9.3381]], grad_fn=<SubBackward0>)
[2022-11-05 20:47:41.169873] Process 3. Episode 19550, average_reward -0.073043
Episode 19550: Total Loss of tensor([[-0.1283]], grad_fn=<SubBackward0>)
[2022-11-05 20:48:08.089811] Process 5. Episode 19900, average_reward -0.069598
Episode 19900: Total Loss of tensor([[9.9553]], grad_fn=<SubBackward0>)
[2022-11-05 20:48:17.693870] Process 2. Episode 20050, average_reward -0.075012
Episode 20050: Total Loss of tensor([[13.3097]], grad_fn=<SubBackward0>)
[2022-11-05 20:48:30.309287] Process 4. Episode 20900, average_reward -0.071627
Episode 20900: Total Loss of tensor([[8.4482]], grad_fn=<SubBackward0>)
[2022-11-05 20:49:36.019642] Process 0. Episode 19700, average_reward -0.069391
Episode 19700: Total Loss of tensor([[14.0326]], grad_fn=<SubBackward0>)
[2022-11-05 20:49:39.565494] Process 1. Episode 19300, average_reward -0.068290
Episode 19300: Total Loss of tensor([[8.3296]], grad_fn=<SubBackward0>)
[2022-11-05 20:50:16.214169] Process 3. Episode 19600, average_reward -0.072959
Episode 19600: Total Loss of tensor([[20.5345]], grad_fn=<SubBackward0>)
[2022-11-05 20:50:41.926095] Process 5. Episode 19950, average_reward -0.069574
Episode 19950: Total Loss of tensor([[5.6567]], grad_fn=<SubBackward0>)
[2022-11-05 20:50:54.326912] Process 4. Episode 20950, average_reward -0.071647
Episode 20950: Total Loss of tensor([[13.0660]], grad_fn=<SubBackward0>)
[2022-11-05 20:50:55.997422] Process 2. Episode 20100, average_reward -0.074975
Episode 20100: Total Loss of tensor([[-75.3303]], grad_fn=<SubBackward0>)
[2022-11-05 20:52:07.800241] Process 0. Episode 19750, average_reward -0.069367
Episode 19750: Total Loss of tensor([[-5.8758]], grad_fn=<SubBackward0>)
[2022-11-05 20:52:15.899411] Process 1. Episode 19350, average_reward -0.068165
Episode 19350: Total Loss of tensor([[13.7695]], grad_fn=<SubBackward0>)
[2022-11-05 20:52:48.683705] Process 3. Episode 19650, average_reward -0.072875
Episode 19650: Total Loss of tensor([[6.8258]], grad_fn=<SubBackward0>)
[2022-11-05 20:53:12.261049] Process 5. Episode 20000, average_reward -0.069600
Episode 20000: Total Loss of tensor([[21.7030]], grad_fn=<SubBackward0>)
[2022-11-05 20:53:16.059842] Process 4. Episode 21000, average_reward -0.071667
Episode 21000: Total Loss of tensor([[17.2732]], grad_fn=<SubBackward0>)
[2022-11-05 20:53:24.723586] Process 2. Episode 20150, average_reward -0.074938
Episode 20150: Total Loss of tensor([[22.1188]], grad_fn=<SubBackward0>)
[2022-11-05 20:54:36.001637] Process 0. Episode 19800, average_reward -0.069394
Episode 19800: Total Loss of tensor([[5.1446]], grad_fn=<SubBackward0>)
[2022-11-05 20:54:56.459622] Process 1. Episode 19400, average_reward -0.068093
Episode 19400: Total Loss of tensor([[15.8607]], grad_fn=<SubBackward0>)
[2022-11-05 20:55:19.160583] Process 3. Episode 19700, average_reward -0.073046
Episode 19700: Total Loss of tensor([[5.7230]], grad_fn=<SubBackward0>)
[2022-11-05 20:55:42.565073] Process 4. Episode 21050, average_reward -0.071591
Episode 21050: Total Loss of tensor([[14.9952]], grad_fn=<SubBackward0>)
[2022-11-05 20:55:44.095770] Process 5. Episode 20050, average_reward -0.069526
Episode 20050: Total Loss of tensor([[13.2131]], grad_fn=<SubBackward0>)
[2022-11-05 20:56:07.959108] Process 2. Episode 20200, average_reward -0.074950
Episode 20200: Total Loss of tensor([[15.4545]], grad_fn=<SubBackward0>)
[2022-11-05 20:57:03.790378] Process 0. Episode 19850, average_reward -0.069219
Episode 19850: Total Loss of tensor([[6.2845]], grad_fn=<SubBackward0>)
[2022-11-05 20:57:36.666760] Process 1. Episode 19450, average_reward -0.068278
Episode 19450: Total Loss of tensor([[12.4466]], grad_fn=<SubBackward0>)
[2022-11-05 20:57:41.731803] Process 3. Episode 19750, average_reward -0.072861
Episode 19750: Total Loss of tensor([[7.2505]], grad_fn=<SubBackward0>)
[2022-11-05 20:58:04.226483] Process 4. Episode 21100, average_reward -0.071564
Episode 21100: Total Loss of tensor([[12.8790]], grad_fn=<SubBackward0>)
[2022-11-05 20:58:07.209938] Process 5. Episode 20100, average_reward -0.069652
Episode 20100: Total Loss of tensor([[-19.5346]], grad_fn=<SubBackward0>)
[2022-11-05 20:58:40.423767] Process 2. Episode 20250, average_reward -0.074914
Episode 20250: Total Loss of tensor([[-28.3746]], grad_fn=<SubBackward0>)
[2022-11-05 20:59:31.836218] Process 0. Episode 19900, average_reward -0.069347
Episode 19900: Total Loss of tensor([[9.9772]], grad_fn=<SubBackward0>)
[2022-11-05 21:00:14.535617] Process 3. Episode 19800, average_reward -0.072879
Episode 19800: Total Loss of tensor([[6.9360]], grad_fn=<SubBackward0>)
[2022-11-05 21:00:19.903394] Process 1. Episode 19500, average_reward -0.068359
Episode 19500: Total Loss of tensor([[-10.6521]], grad_fn=<SubBackward0>)
[2022-11-05 21:00:27.216240] Process 4. Episode 21150, average_reward -0.071537
Episode 21150: Total Loss of tensor([[28.7298]], grad_fn=<SubBackward0>)
[2022-11-05 21:00:35.554498] Process 5. Episode 20150, average_reward -0.069628
Episode 20150: Total Loss of tensor([[9.6932]], grad_fn=<SubBackward0>)
[2022-11-05 21:01:06.852386] Process 2. Episode 20300, average_reward -0.074828
Episode 20300: Total Loss of tensor([[5.7842]], grad_fn=<SubBackward0>)
[2022-11-05 21:02:05.238061] Process 0. Episode 19950, average_reward -0.069524
Episode 19950: Total Loss of tensor([[-4.3900]], grad_fn=<SubBackward0>)
[2022-11-05 21:02:44.391430] Process 3. Episode 19850, average_reward -0.072796
Episode 19850: Total Loss of tensor([[-48.7501]], grad_fn=<SubBackward0>)
[2022-11-05 21:02:44.666956] Process 4. Episode 21200, average_reward -0.071415
Episode 21200: Total Loss of tensor([[0.0574]], grad_fn=<SubBackward0>)
[2022-11-05 21:02:58.862025] Process 1. Episode 19550, average_reward -0.068338
Episode 19550: Total Loss of tensor([[6.7199]], grad_fn=<SubBackward0>)
[2022-11-05 21:03:09.403581] Process 5. Episode 20200, average_reward -0.069505
Episode 20200: Total Loss of tensor([[3.4772]], grad_fn=<SubBackward0>)
[2022-11-05 21:03:36.323464] Process 2. Episode 20350, average_reward -0.074988
Episode 20350: Total Loss of tensor([[10.6863]], grad_fn=<SubBackward0>)
[2022-11-05 21:04:32.290654] Process 0. Episode 20000, average_reward -0.069550
Episode 20000: Total Loss of tensor([[8.9133]], grad_fn=<SubBackward0>)
[2022-11-05 21:05:05.093547] Process 4. Episode 21250, average_reward -0.071435
Episode 21250: Total Loss of tensor([[-21.0506]], grad_fn=<SubBackward0>)
[2022-11-05 21:05:06.714018] Process 3. Episode 19900, average_reward -0.072915
Episode 19900: Total Loss of tensor([[-129.8276]], grad_fn=<SubBackward0>)
[2022-11-05 21:05:34.561383] Process 1. Episode 19600, average_reward -0.068316
Episode 19600: Total Loss of tensor([[20.7184]], grad_fn=<SubBackward0>)
[2022-11-05 21:05:41.964377] Process 5. Episode 20250, average_reward -0.069383
Episode 20250: Total Loss of tensor([[3.3045]], grad_fn=<SubBackward0>)
[2022-11-05 21:06:00.336669] Process 2. Episode 20400, average_reward -0.074902
Episode 20400: Total Loss of tensor([[5.8521]], grad_fn=<SubBackward0>)
[2022-11-05 21:07:12.086022] Process 0. Episode 20050, average_reward -0.069576
Episode 20050: Total Loss of tensor([[22.1398]], grad_fn=<SubBackward0>)
[2022-11-05 21:07:24.223325] Process 4. Episode 21300, average_reward -0.071455
Episode 21300: Total Loss of tensor([[-109.7911]], grad_fn=<SubBackward0>)
[2022-11-05 21:07:30.814895] Process 3. Episode 19950, average_reward -0.072832
Episode 19950: Total Loss of tensor([[1.2464]], grad_fn=<SubBackward0>)
[2022-11-05 21:08:03.752819] Process 1. Episode 19650, average_reward -0.068346
Episode 19650: Total Loss of tensor([[13.2185]], grad_fn=<SubBackward0>)
[2022-11-05 21:08:14.459330] Process 5. Episode 20300, average_reward -0.069507
Episode 20300: Total Loss of tensor([[9.9840]], grad_fn=<SubBackward0>)
[2022-11-05 21:08:23.156166] Process 2. Episode 20450, average_reward -0.074963
Episode 20450: Total Loss of tensor([[-1.1849]], grad_fn=<SubBackward0>)
[2022-11-05 21:09:42.899892] Process 4. Episode 21350, average_reward -0.071522
Episode 21350: Total Loss of tensor([[2.5146]], grad_fn=<SubBackward0>)
[2022-11-05 21:09:54.983745] Process 0. Episode 20100, average_reward -0.069602
Episode 20100: Total Loss of tensor([[-59.2923]], grad_fn=<SubBackward0>)
[2022-11-05 21:09:55.131072] Process 3. Episode 20000, average_reward -0.072750
Episode 20000: Total Loss of tensor([[4.6988]], grad_fn=<SubBackward0>)
[2022-11-05 21:10:37.591440] Process 1. Episode 19700, average_reward -0.068376
Episode 19700: Total Loss of tensor([[-15.6558]], grad_fn=<SubBackward0>)
[2022-11-05 21:10:46.332724] Process 5. Episode 20350, average_reward -0.069337
Episode 20350: Total Loss of tensor([[-0.6256]], grad_fn=<SubBackward0>)
[2022-11-05 21:10:46.843142] Process 2. Episode 20500, average_reward -0.075024
Episode 20500: Total Loss of tensor([[11.9653]], grad_fn=<SubBackward0>)
[2022-11-05 21:12:05.426589] Process 4. Episode 21400, average_reward -0.071449
Episode 21400: Total Loss of tensor([[11.2986]], grad_fn=<SubBackward0>)
[2022-11-05 21:12:25.599524] Process 3. Episode 20050, average_reward -0.072718
Episode 20050: Total Loss of tensor([[10.0214]], grad_fn=<SubBackward0>)
[2022-11-05 21:12:43.008855] Process 0. Episode 20150, average_reward -0.069529
Episode 20150: Total Loss of tensor([[-3.1318]], grad_fn=<SubBackward0>)
[2022-11-05 21:13:06.632960] Process 1. Episode 19750, average_reward -0.068456
Episode 19750: Total Loss of tensor([[-3.2156]], grad_fn=<SubBackward0>)
[2022-11-05 21:13:11.890299] Process 2. Episode 20550, average_reward -0.074939
Episode 20550: Total Loss of tensor([[1.9701]], grad_fn=<SubBackward0>)
[2022-11-05 21:13:18.163625] Process 5. Episode 20400, average_reward -0.069216
Episode 20400: Total Loss of tensor([[10.9369]], grad_fn=<SubBackward0>)
[2022-11-05 21:14:27.013475] Process 4. Episode 21450, average_reward -0.071562
Episode 21450: Total Loss of tensor([[6.9715]], grad_fn=<SubBackward0>)
[2022-11-05 21:15:00.049448] Process 3. Episode 20100, average_reward -0.072687
Episode 20100: Total Loss of tensor([[-68.5551]], grad_fn=<SubBackward0>)
[2022-11-05 21:15:22.771755] Process 0. Episode 20200, average_reward -0.069604
Episode 20200: Total Loss of tensor([[7.1485]], grad_fn=<SubBackward0>)
[2022-11-05 21:15:33.240268] Process 1. Episode 19800, average_reward -0.068535
Episode 19800: Total Loss of tensor([[8.8313]], grad_fn=<SubBackward0>)
[2022-11-05 21:15:37.399658] Process 2. Episode 20600, average_reward -0.075097
Episode 20600: Total Loss of tensor([[-78.5456]], grad_fn=<SubBackward0>)
[2022-11-05 21:15:51.036319] Process 5. Episode 20450, average_reward -0.069291
Episode 20450: Total Loss of tensor([[5.1260]], grad_fn=<SubBackward0>)
[2022-11-05 21:16:46.988802] Process 4. Episode 21500, average_reward -0.071581
Episode 21500: Total Loss of tensor([[8.6501]], grad_fn=<SubBackward0>)
[2022-11-05 21:17:42.581229] Process 3. Episode 20150, average_reward -0.072705
Episode 20150: Total Loss of tensor([[-93.6804]], grad_fn=<SubBackward0>)
[2022-11-05 21:17:50.114090] Process 0. Episode 20250, average_reward -0.069630
Episode 20250: Total Loss of tensor([[21.4273]], grad_fn=<SubBackward0>)
[2022-11-05 21:18:04.508437] Process 1. Episode 19850, average_reward -0.068615
Episode 19850: Total Loss of tensor([[-117.3294]], grad_fn=<SubBackward0>)
[2022-11-05 21:18:11.728362] Process 2. Episode 20650, average_reward -0.075012
Episode 20650: Total Loss of tensor([[15.7343]], grad_fn=<SubBackward0>)
[2022-11-05 21:18:20.377801] Process 5. Episode 20500, average_reward -0.069268
Episode 20500: Total Loss of tensor([[5.1593]], grad_fn=<SubBackward0>)
[2022-11-05 21:19:07.830591] Process 4. Episode 21550, average_reward -0.071694
Episode 21550: Total Loss of tensor([[-39.3416]], grad_fn=<SubBackward0>)
[2022-11-05 21:20:14.519685] Process 0. Episode 20300, average_reward -0.069606
Episode 20300: Total Loss of tensor([[15.0484]], grad_fn=<SubBackward0>)
[2022-11-05 21:20:14.690172] Process 3. Episode 20200, average_reward -0.072624
Episode 20200: Total Loss of tensor([[-7.8218]], grad_fn=<SubBackward0>)
[2022-11-05 21:20:39.955671] Process 1. Episode 19900, average_reward -0.068593
Episode 19900: Total Loss of tensor([[11.3147]], grad_fn=<SubBackward0>)
[2022-11-05 21:20:44.652856] Process 5. Episode 20550, average_reward -0.069440
Episode 20550: Total Loss of tensor([[7.2490]], grad_fn=<SubBackward0>)
[2022-11-05 21:20:49.372455] Process 2. Episode 20700, average_reward -0.074976
Episode 20700: Total Loss of tensor([[11.3523]], grad_fn=<SubBackward0>)
[2022-11-05 21:21:24.517154] Process 4. Episode 21600, average_reward -0.071620
Episode 21600: Total Loss of tensor([[1.4692]], grad_fn=<SubBackward0>)
[2022-11-05 21:22:39.062630] Process 0. Episode 20350, average_reward -0.069631
Episode 20350: Total Loss of tensor([[8.6308]], grad_fn=<SubBackward0>)
[2022-11-05 21:22:47.413811] Process 3. Episode 20250, average_reward -0.072494
Episode 20250: Total Loss of tensor([[4.2524]], grad_fn=<SubBackward0>)
[2022-11-05 21:23:14.814457] Process 5. Episode 20600, average_reward -0.069515
Episode 20600: Total Loss of tensor([[2.5754]], grad_fn=<SubBackward0>)
[2022-11-05 21:23:16.400363] Process 2. Episode 20750, average_reward -0.074940
Episode 20750: Total Loss of tensor([[13.3014]], grad_fn=<SubBackward0>)
[2022-11-05 21:23:17.044244] Process 1. Episode 19950, average_reward -0.068622
Episode 19950: Total Loss of tensor([[15.0468]], grad_fn=<SubBackward0>)
[2022-11-05 21:23:47.038368] Process 4. Episode 21650, average_reward -0.071501
Episode 21650: Total Loss of tensor([[-2.4505]], grad_fn=<SubBackward0>)
[2022-11-05 21:25:02.865026] Process 0. Episode 20400, average_reward -0.069706
Episode 20400: Total Loss of tensor([[9.2468]], grad_fn=<SubBackward0>)
[2022-11-05 21:25:13.971088] Process 3. Episode 20300, average_reward -0.072611
Episode 20300: Total Loss of tensor([[1.0044]], grad_fn=<SubBackward0>)
[2022-11-05 21:25:39.244668] Process 2. Episode 20800, average_reward -0.074952
Episode 20800: Total Loss of tensor([[-1.2827]], grad_fn=<SubBackward0>)
[2022-11-05 21:25:51.614190] Process 1. Episode 20000, average_reward -0.068600
Episode 20000: Total Loss of tensor([[11.8925]], grad_fn=<SubBackward0>)
[2022-11-05 21:25:55.213960] Process 5. Episode 20650, average_reward -0.069588
Episode 20650: Total Loss of tensor([[-97.6106]], grad_fn=<SubBackward0>)
[2022-11-05 21:26:07.726961] Process 4. Episode 21700, average_reward -0.071613
Episode 21700: Total Loss of tensor([[-1.5248]], grad_fn=<SubBackward0>)
[2022-11-05 21:27:29.910456] Process 0. Episode 20450, average_reward -0.069682
Episode 20450: Total Loss of tensor([[18.9790]], grad_fn=<SubBackward0>)
[2022-11-05 21:27:51.292182] Process 3. Episode 20350, average_reward -0.072580
Episode 20350: Total Loss of tensor([[-5.2953]], grad_fn=<SubBackward0>)
[2022-11-05 21:28:02.867265] Process 2. Episode 20850, average_reward -0.074868
Episode 20850: Total Loss of tensor([[9.4522]], grad_fn=<SubBackward0>)
[2022-11-05 21:28:20.894076] Process 1. Episode 20050, average_reward -0.068579
Episode 20050: Total Loss of tensor([[10.1308]], grad_fn=<SubBackward0>)
[2022-11-05 21:28:25.022659] Process 5. Episode 20700, average_reward -0.069614
Episode 20700: Total Loss of tensor([[6.8020]], grad_fn=<SubBackward0>)
[2022-11-05 21:28:30.681321] Process 4. Episode 21750, average_reward -0.071724
Episode 21750: Total Loss of tensor([[-5.6309]], grad_fn=<SubBackward0>)
[2022-11-05 21:30:00.789661] Process 0. Episode 20500, average_reward -0.069756
Episode 20500: Total Loss of tensor([[-6.3103]], grad_fn=<SubBackward0>)
[2022-11-05 21:30:19.921737] Process 3. Episode 20400, average_reward -0.072500
Episode 20400: Total Loss of tensor([[18.0807]], grad_fn=<SubBackward0>)
[2022-11-05 21:30:31.322865] Process 2. Episode 20900, average_reward -0.074833
Episode 20900: Total Loss of tensor([[0.7289]], grad_fn=<SubBackward0>)
[2022-11-05 21:30:50.359967] Process 1. Episode 20100, average_reward -0.068507
Episode 20100: Total Loss of tensor([[2.9981]], grad_fn=<SubBackward0>)
[2022-11-05 21:30:51.536738] Process 5. Episode 20750, average_reward -0.069639
Episode 20750: Total Loss of tensor([[-9.1149]], grad_fn=<SubBackward0>)
[2022-11-05 21:30:53.687864] Process 4. Episode 21800, average_reward -0.071743
Episode 21800: Total Loss of tensor([[3.9506]], grad_fn=<SubBackward0>)
[2022-11-05 21:32:29.997727] Process 0. Episode 20550, average_reward -0.069781
Episode 20550: Total Loss of tensor([[-5.1079]], grad_fn=<SubBackward0>)
[2022-11-05 21:32:46.886888] Process 3. Episode 20450, average_reward -0.072469
Episode 20450: Total Loss of tensor([[10.6211]], grad_fn=<SubBackward0>)
[2022-11-05 21:32:53.214881] Process 2. Episode 20950, average_reward -0.074893
Episode 20950: Total Loss of tensor([[19.5413]], grad_fn=<SubBackward0>)
[2022-11-05 21:33:15.695070] Process 4. Episode 21850, average_reward -0.071762
Episode 21850: Total Loss of tensor([[14.5581]], grad_fn=<SubBackward0>)
[2022-11-05 21:33:19.454717] Process 5. Episode 20800, average_reward -0.069712
Episode 20800: Total Loss of tensor([[5.8195]], grad_fn=<SubBackward0>)
[2022-11-05 21:33:21.952013] Process 1. Episode 20150, average_reward -0.068387
Episode 20150: Total Loss of tensor([[17.0723]], grad_fn=<SubBackward0>)
[2022-11-05 21:35:05.549135] Process 0. Episode 20600, average_reward -0.069806
Episode 20600: Total Loss of tensor([[3.7345]], grad_fn=<SubBackward0>)
[2022-11-05 21:35:13.160416] Process 3. Episode 20500, average_reward -0.072634
Episode 20500: Total Loss of tensor([[18.8659]], grad_fn=<SubBackward0>)
[2022-11-05 21:35:17.427449] Process 2. Episode 21000, average_reward -0.074952
Episode 21000: Total Loss of tensor([[8.8466]], grad_fn=<SubBackward0>)
[2022-11-05 21:35:44.058196] Process 4. Episode 21900, average_reward -0.071644
Episode 21900: Total Loss of tensor([[1.3607]], grad_fn=<SubBackward0>)
[2022-11-05 21:35:46.482756] Process 5. Episode 20850, average_reward -0.069784
Episode 20850: Total Loss of tensor([[4.8679]], grad_fn=<SubBackward0>)
[2022-11-05 21:35:57.964853] Process 1. Episode 20200, average_reward -0.068465
Episode 20200: Total Loss of tensor([[6.3148]], grad_fn=<SubBackward0>)
[2022-11-05 21:37:34.534930] Process 3. Episode 20550, average_reward -0.072555
Episode 20550: Total Loss of tensor([[-2.0450]], grad_fn=<SubBackward0>)
[2022-11-05 21:37:37.764935] Process 0. Episode 20650, average_reward -0.069782
Episode 20650: Total Loss of tensor([[18.3848]], grad_fn=<SubBackward0>)
[2022-11-05 21:37:46.432035] Process 2. Episode 21050, average_reward -0.075012
Episode 21050: Total Loss of tensor([[12.1778]], grad_fn=<SubBackward0>)
[2022-11-05 21:38:06.106277] Process 4. Episode 21950, average_reward -0.071663
Episode 21950: Total Loss of tensor([[15.8618]], grad_fn=<SubBackward0>)
[2022-11-05 21:38:15.656963] Process 5. Episode 20900, average_reward -0.069856
Episode 20900: Total Loss of tensor([[-3.7538]], grad_fn=<SubBackward0>)
[2022-11-05 21:38:39.662957] Process 1. Episode 20250, average_reward -0.068346
Episode 20250: Total Loss of tensor([[2.0584]], grad_fn=<SubBackward0>)
[2022-11-05 21:39:58.164187] Process 3. Episode 20600, average_reward -0.072524
Episode 20600: Total Loss of tensor([[13.4812]], grad_fn=<SubBackward0>)
[2022-11-05 21:40:10.518617] Process 2. Episode 21100, average_reward -0.075071
Episode 21100: Total Loss of tensor([[-106.1744]], grad_fn=<SubBackward0>)
[2022-11-05 21:40:11.400687] Process 0. Episode 20700, average_reward -0.069710
Episode 20700: Total Loss of tensor([[0.7394]], grad_fn=<SubBackward0>)
[2022-11-05 21:40:29.427163] Process 4. Episode 22000, average_reward -0.071636
Episode 22000: Total Loss of tensor([[9.6825]], grad_fn=<SubBackward0>)
[2022-11-05 21:40:54.512472] Process 5. Episode 20950, average_reward -0.069928
Episode 20950: Total Loss of tensor([[-49.2704]], grad_fn=<SubBackward0>)
[2022-11-05 21:41:05.746005] Process 1. Episode 20300, average_reward -0.068325
Episode 20300: Total Loss of tensor([[9.4758]], grad_fn=<SubBackward0>)
[2022-11-05 21:42:27.752609] Process 3. Episode 20650, average_reward -0.072591
Episode 20650: Total Loss of tensor([[23.3611]], grad_fn=<SubBackward0>)
[2022-11-05 21:42:38.258695] Process 2. Episode 21150, average_reward -0.074988
Episode 21150: Total Loss of tensor([[1.6527]], grad_fn=<SubBackward0>)
[2022-11-05 21:42:41.600427] Process 0. Episode 20750, average_reward -0.069639
Episode 20750: Total Loss of tensor([[4.1176]], grad_fn=<SubBackward0>)
[2022-11-05 21:42:52.158872] Process 4. Episode 22050, average_reward -0.071655
Episode 22050: Total Loss of tensor([[5.7299]], grad_fn=<SubBackward0>)
[2022-11-05 21:43:28.167050] Process 1. Episode 20350, average_reward -0.068403
Episode 20350: Total Loss of tensor([[14.2823]], grad_fn=<SubBackward0>)
[2022-11-05 21:43:33.679743] Process 5. Episode 21000, average_reward -0.069952
Episode 21000: Total Loss of tensor([[7.3493]], grad_fn=<SubBackward0>)
[2022-11-05 21:44:53.851317] Process 3. Episode 20700, average_reward -0.072560
Episode 20700: Total Loss of tensor([[10.1631]], grad_fn=<SubBackward0>)
[2022-11-05 21:44:59.929983] Process 2. Episode 21200, average_reward -0.075047
Episode 21200: Total Loss of tensor([[4.7906]], grad_fn=<SubBackward0>)
[2022-11-05 21:45:09.567556] Process 0. Episode 20800, average_reward -0.069760
Episode 20800: Total Loss of tensor([[20.3765]], grad_fn=<SubBackward0>)
[2022-11-05 21:45:16.087491] Process 4. Episode 22100, average_reward -0.071674
Episode 22100: Total Loss of tensor([[5.4844]], grad_fn=<SubBackward0>)
[2022-11-05 21:46:01.434229] Process 1. Episode 20400, average_reward -0.068529
Episode 20400: Total Loss of tensor([[-1.1660]], grad_fn=<SubBackward0>)
[2022-11-05 21:46:13.339957] Process 5. Episode 21050, average_reward -0.069881
Episode 21050: Total Loss of tensor([[7.3522]], grad_fn=<SubBackward0>)
[2022-11-05 21:47:26.545288] Process 2. Episode 21250, average_reward -0.074965
Episode 21250: Total Loss of tensor([[-27.9333]], grad_fn=<SubBackward0>)
[2022-11-05 21:47:28.784934] Process 3. Episode 20750, average_reward -0.072627
Episode 20750: Total Loss of tensor([[0.9847]], grad_fn=<SubBackward0>)
[2022-11-05 21:47:36.845933] Process 4. Episode 22150, average_reward -0.071693
Episode 22150: Total Loss of tensor([[4.1023]], grad_fn=<SubBackward0>)
[2022-11-05 21:47:41.745417] Process 0. Episode 20850, average_reward -0.069688
Episode 20850: Total Loss of tensor([[-0.1865]], grad_fn=<SubBackward0>)
[2022-11-05 21:48:31.687822] Process 1. Episode 20450, average_reward -0.068606
Episode 20450: Total Loss of tensor([[10.6859]], grad_fn=<SubBackward0>)
[2022-11-05 21:48:54.157089] Process 5. Episode 21100, average_reward -0.069810
Episode 21100: Total Loss of tensor([[-20.6807]], grad_fn=<SubBackward0>)
[2022-11-05 21:49:50.849115] Process 2. Episode 21300, average_reward -0.074977
Episode 21300: Total Loss of tensor([[2.3988]], grad_fn=<SubBackward0>)
[2022-11-05 21:49:56.524837] Process 3. Episode 20800, average_reward -0.072692
Episode 20800: Total Loss of tensor([[0.4658]], grad_fn=<SubBackward0>)
[2022-11-05 21:49:58.530844] Process 4. Episode 22200, average_reward -0.071802
Episode 22200: Total Loss of tensor([[9.8760]], grad_fn=<SubBackward0>)
[2022-11-05 21:50:13.603279] Process 0. Episode 20900, average_reward -0.069761
Episode 20900: Total Loss of tensor([[3.8488]], grad_fn=<SubBackward0>)
[2022-11-05 21:50:57.338095] Process 1. Episode 20500, average_reward -0.068537
Episode 20500: Total Loss of tensor([[17.5370]], grad_fn=<SubBackward0>)
[2022-11-05 21:51:29.579260] Process 5. Episode 21150, average_reward -0.069787
Episode 21150: Total Loss of tensor([[-6.2771]], grad_fn=<SubBackward0>)
[2022-11-05 21:52:18.080371] Process 2. Episode 21350, average_reward -0.075082
Episode 21350: Total Loss of tensor([[21.9997]], grad_fn=<SubBackward0>)
[2022-11-05 21:52:21.698493] Process 4. Episode 22250, average_reward -0.071775
Episode 22250: Total Loss of tensor([[20.4693]], grad_fn=<SubBackward0>)
[2022-11-05 21:52:33.779720] Process 3. Episode 20850, average_reward -0.072662
Episode 20850: Total Loss of tensor([[0.9347]], grad_fn=<SubBackward0>)
[2022-11-05 21:52:38.074785] Process 0. Episode 20950, average_reward -0.069737
Episode 20950: Total Loss of tensor([[6.7456]], grad_fn=<SubBackward0>)
[2022-11-05 21:53:25.539096] Process 1. Episode 20550, average_reward -0.068564
Episode 20550: Total Loss of tensor([[5.3958]], grad_fn=<SubBackward0>)
[2022-11-05 21:54:01.234771] Process 5. Episode 21200, average_reward -0.069858
Episode 21200: Total Loss of tensor([[-40.4613]], grad_fn=<SubBackward0>)
[2022-11-05 21:54:40.940754] Process 4. Episode 22300, average_reward -0.071794
Episode 22300: Total Loss of tensor([[-22.8678]], grad_fn=<SubBackward0>)
[2022-11-05 21:54:43.196062] Process 2. Episode 21400, average_reward -0.075000
Episode 21400: Total Loss of tensor([[10.3762]], grad_fn=<SubBackward0>)
[2022-11-05 21:54:57.338196] Process 0. Episode 21000, average_reward -0.069762
Episode 21000: Total Loss of tensor([[19.4607]], grad_fn=<SubBackward0>)
[2022-11-05 21:55:11.267837] Process 3. Episode 20900, average_reward -0.072727
Episode 20900: Total Loss of tensor([[3.2373]], grad_fn=<SubBackward0>)
[2022-11-05 21:56:00.049011] Process 1. Episode 20600, average_reward -0.068592
Episode 20600: Total Loss of tensor([[4.4924]], grad_fn=<SubBackward0>)
[2022-11-05 21:56:28.405782] Process 5. Episode 21250, average_reward -0.069929
Episode 21250: Total Loss of tensor([[1.3869]], grad_fn=<SubBackward0>)
[2022-11-05 21:57:03.894739] Process 4. Episode 22350, average_reward -0.071767
Episode 22350: Total Loss of tensor([[8.7912]], grad_fn=<SubBackward0>)
[2022-11-05 21:57:09.153774] Process 2. Episode 21450, average_reward -0.074965
Episode 21450: Total Loss of tensor([[-11.7342]], grad_fn=<SubBackward0>)
[2022-11-05 21:57:19.838978] Process 0. Episode 21050, average_reward -0.069691
Episode 21050: Total Loss of tensor([[6.9788]], grad_fn=<SubBackward0>)
[2022-11-05 21:57:38.989563] Process 3. Episode 20950, average_reward -0.072601
Episode 20950: Total Loss of tensor([[-36.2015]], grad_fn=<SubBackward0>)
[2022-11-05 21:58:38.975615] Process 1. Episode 20650, average_reward -0.068523
Episode 20650: Total Loss of tensor([[8.8824]], grad_fn=<SubBackward0>)
[2022-11-05 21:59:02.323881] Process 5. Episode 21300, average_reward -0.069906
Episode 21300: Total Loss of tensor([[-60.3658]], grad_fn=<SubBackward0>)
[2022-11-05 21:59:27.094299] Process 4. Episode 22400, average_reward -0.071875
Episode 22400: Total Loss of tensor([[23.5146]], grad_fn=<SubBackward0>)
[2022-11-05 21:59:36.415175] Process 2. Episode 21500, average_reward -0.074930
Episode 21500: Total Loss of tensor([[7.7489]], grad_fn=<SubBackward0>)
[2022-11-05 21:59:50.076745] Process 0. Episode 21100, average_reward -0.069810
Episode 21100: Total Loss of tensor([[-128.4588]], grad_fn=<SubBackward0>)
[2022-11-05 22:00:10.739363] Process 3. Episode 21000, average_reward -0.072619
Episode 21000: Total Loss of tensor([[7.5171]], grad_fn=<SubBackward0>)
[2022-11-05 22:01:11.137458] Process 1. Episode 20700, average_reward -0.068647
Episode 20700: Total Loss of tensor([[-114.5953]], grad_fn=<SubBackward0>)
[2022-11-05 22:01:28.197235] Process 5. Episode 21350, average_reward -0.069930
Episode 21350: Total Loss of tensor([[23.6559]], grad_fn=<SubBackward0>)
[2022-11-05 22:01:48.996943] Process 4. Episode 22450, average_reward -0.071893
Episode 22450: Total Loss of tensor([[3.1718]], grad_fn=<SubBackward0>)
[2022-11-05 22:02:10.196395] Process 0. Episode 21150, average_reward -0.069882
Episode 21150: Total Loss of tensor([[25.5296]], grad_fn=<SubBackward0>)
[2022-11-05 22:02:15.460374] Process 2. Episode 21550, average_reward -0.074942
Episode 21550: Total Loss of tensor([[14.2866]], grad_fn=<SubBackward0>)
[2022-11-05 22:02:51.240733] Process 3. Episode 21050, average_reward -0.072732
Episode 21050: Total Loss of tensor([[14.3013]], grad_fn=<SubBackward0>)
[2022-11-05 22:03:44.431745] Process 1. Episode 20750, average_reward -0.068675
Episode 20750: Total Loss of tensor([[2.8973]], grad_fn=<SubBackward0>)
[2022-11-05 22:03:58.806742] Process 5. Episode 21400, average_reward -0.070000
Episode 21400: Total Loss of tensor([[8.4679]], grad_fn=<SubBackward0>)
[2022-11-05 22:04:10.133732] Process 4. Episode 22500, average_reward -0.071956
Episode 22500: Total Loss of tensor([[12.0055]], grad_fn=<SubBackward0>)
[2022-11-05 22:04:37.663848] Process 2. Episode 21600, average_reward -0.074954
Episode 21600: Total Loss of tensor([[4.6060]], grad_fn=<SubBackward0>)
[2022-11-05 22:04:39.680276] Process 0. Episode 21200, average_reward -0.070047
Episode 21200: Total Loss of tensor([[9.7257]], grad_fn=<SubBackward0>)
[2022-11-05 22:05:19.115421] Process 3. Episode 21100, average_reward -0.072654
Episode 21100: Total Loss of tensor([[4.4641]], grad_fn=<SubBackward0>)
[2022-11-05 22:06:22.851053] Process 1. Episode 20800, average_reward -0.068654
Episode 20800: Total Loss of tensor([[20.9664]], grad_fn=<SubBackward0>)
[2022-11-05 22:06:25.160735] Process 5. Episode 21450, average_reward -0.070023
Episode 21450: Total Loss of tensor([[8.5826]], grad_fn=<SubBackward0>)
[2022-11-05 22:06:32.981491] Process 4. Episode 22550, average_reward -0.071885
Episode 22550: Total Loss of tensor([[-2.9715]], grad_fn=<SubBackward0>)
[2022-11-05 22:07:05.706674] Process 2. Episode 21650, average_reward -0.074873
Episode 21650: Total Loss of tensor([[1.5156]], grad_fn=<SubBackward0>)
[2022-11-05 22:07:10.411705] Process 0. Episode 21250, average_reward -0.069929
Episode 21250: Total Loss of tensor([[-0.5219]], grad_fn=<SubBackward0>)
[2022-11-05 22:07:45.146563] Process 3. Episode 21150, average_reward -0.072624
Episode 21150: Total Loss of tensor([[-3.6543]], grad_fn=<SubBackward0>)
[2022-11-05 22:08:55.400682] Process 4. Episode 22600, average_reward -0.071991
Episode 22600: Total Loss of tensor([[-7.6964]], grad_fn=<SubBackward0>)
[2022-11-05 22:08:57.755805] Process 5. Episode 21500, average_reward -0.069907
Episode 21500: Total Loss of tensor([[9.3506]], grad_fn=<SubBackward0>)
[2022-11-05 22:09:07.453302] Process 1. Episode 20850, average_reward -0.068729
Episode 20850: Total Loss of tensor([[26.3458]], grad_fn=<SubBackward0>)
[2022-11-05 22:09:28.478125] Process 2. Episode 21700, average_reward -0.074885
Episode 21700: Total Loss of tensor([[-3.5262]], grad_fn=<SubBackward0>)
[2022-11-05 22:09:33.181474] Process 0. Episode 21300, average_reward -0.069859
Episode 21300: Total Loss of tensor([[9.7907]], grad_fn=<SubBackward0>)
[2022-11-05 22:10:18.215188] Process 3. Episode 21200, average_reward -0.072594
Episode 21200: Total Loss of tensor([[11.6966]], grad_fn=<SubBackward0>)
[2022-11-05 22:11:18.395209] Process 4. Episode 22650, average_reward -0.072009
Episode 22650: Total Loss of tensor([[3.3067]], grad_fn=<SubBackward0>)
[2022-11-05 22:11:30.545554] Process 5. Episode 21550, average_reward -0.069838
Episode 21550: Total Loss of tensor([[25.1665]], grad_fn=<SubBackward0>)
[2022-11-05 22:11:41.927765] Process 1. Episode 20900, average_reward -0.068804
Episode 20900: Total Loss of tensor([[0.1155]], grad_fn=<SubBackward0>)
[2022-11-05 22:11:56.813863] Process 2. Episode 21750, average_reward -0.074989
Episode 21750: Total Loss of tensor([[7.9727]], grad_fn=<SubBackward0>)
[2022-11-05 22:12:02.906780] Process 0. Episode 21350, average_reward -0.070023
Episode 21350: Total Loss of tensor([[24.4057]], grad_fn=<SubBackward0>)
[2022-11-05 22:12:45.010327] Process 3. Episode 21250, average_reward -0.072565
Episode 21250: Total Loss of tensor([[22.4173]], grad_fn=<SubBackward0>)
[2022-11-05 22:13:43.527340] Process 4. Episode 22700, average_reward -0.072159
Episode 22700: Total Loss of tensor([[-0.4498]], grad_fn=<SubBackward0>)
[2022-11-05 22:13:56.073179] Process 5. Episode 21600, average_reward -0.069769
Episode 21600: Total Loss of tensor([[12.5903]], grad_fn=<SubBackward0>)
[2022-11-05 22:14:25.982409] Process 2. Episode 21800, average_reward -0.074954
Episode 21800: Total Loss of tensor([[-5.7540]], grad_fn=<SubBackward0>)
[2022-11-05 22:14:26.396801] Process 1. Episode 20950, average_reward -0.068831
Episode 20950: Total Loss of tensor([[7.1120]], grad_fn=<SubBackward0>)
[2022-11-05 22:14:38.214310] Process 0. Episode 21400, average_reward -0.069953
Episode 21400: Total Loss of tensor([[17.5551]], grad_fn=<SubBackward0>)
[2022-11-05 22:15:23.786407] Process 3. Episode 21300, average_reward -0.072629
Episode 21300: Total Loss of tensor([[15.9184]], grad_fn=<SubBackward0>)
[2022-11-05 22:16:03.613036] Process 4. Episode 22750, average_reward -0.072264
Episode 22750: Total Loss of tensor([[-105.3723]], grad_fn=<SubBackward0>)
[2022-11-05 22:16:22.966266] Process 5. Episode 21650, average_reward -0.069700
Episode 21650: Total Loss of tensor([[-0.7071]], grad_fn=<SubBackward0>)
[2022-11-05 22:16:51.724537] Process 2. Episode 21850, average_reward -0.074874
Episode 21850: Total Loss of tensor([[-1.7837]], grad_fn=<SubBackward0>)
[2022-11-05 22:16:57.173134] Process 1. Episode 21000, average_reward -0.068714
Episode 21000: Total Loss of tensor([[5.7464]], grad_fn=<SubBackward0>)
[2022-11-05 22:17:13.753302] Process 0. Episode 21450, average_reward -0.069883
Episode 21450: Total Loss of tensor([[7.9512]], grad_fn=<SubBackward0>)
[2022-11-05 22:17:59.319001] Process 3. Episode 21350, average_reward -0.072506
Episode 21350: Total Loss of tensor([[-3.6483]], grad_fn=<SubBackward0>)
[2022-11-05 22:18:22.985621] Process 4. Episode 22800, average_reward -0.072237
Episode 22800: Total Loss of tensor([[5.7401]], grad_fn=<SubBackward0>)
[2022-11-05 22:18:56.552176] Process 5. Episode 21700, average_reward -0.069724
Episode 21700: Total Loss of tensor([[0.1072]], grad_fn=<SubBackward0>)
[2022-11-05 22:19:25.612642] Process 2. Episode 21900, average_reward -0.074840
Episode 21900: Total Loss of tensor([[12.1702]], grad_fn=<SubBackward0>)
[2022-11-05 22:19:34.891143] Process 1. Episode 21050, average_reward -0.068789
Episode 21050: Total Loss of tensor([[13.2273]], grad_fn=<SubBackward0>)
[2022-11-05 22:19:38.312847] Process 0. Episode 21500, average_reward -0.070000
Episode 21500: Total Loss of tensor([[13.7605]], grad_fn=<SubBackward0>)
[2022-11-05 22:20:22.412091] Process 3. Episode 21400, average_reward -0.072477
Episode 21400: Total Loss of tensor([[12.5235]], grad_fn=<SubBackward0>)
[2022-11-05 22:20:44.374187] Process 4. Episode 22850, average_reward -0.072254
Episode 22850: Total Loss of tensor([[1.3673]], grad_fn=<SubBackward0>)
[2022-11-05 22:21:24.080235] Process 5. Episode 21750, average_reward -0.069701
Episode 21750: Total Loss of tensor([[6.8268]], grad_fn=<SubBackward0>)
[2022-11-05 22:22:05.211309] Process 2. Episode 21950, average_reward -0.074852
Episode 21950: Total Loss of tensor([[0.9680]], grad_fn=<SubBackward0>)
[2022-11-05 22:22:07.446503] Process 0. Episode 21550, average_reward -0.069977
Episode 21550: Total Loss of tensor([[6.7912]], grad_fn=<SubBackward0>)
[2022-11-05 22:22:15.275244] Process 1. Episode 21100, average_reward -0.068863
Episode 21100: Total Loss of tensor([[-105.4435]], grad_fn=<SubBackward0>)
[2022-11-05 22:22:45.954527] Process 3. Episode 21450, average_reward -0.072494
Episode 21450: Total Loss of tensor([[1.8232]], grad_fn=<SubBackward0>)
[2022-11-05 22:23:03.502797] Process 4. Episode 22900, average_reward -0.072271
Episode 22900: Total Loss of tensor([[1.7379]], grad_fn=<SubBackward0>)
[2022-11-05 22:24:02.654431] Process 5. Episode 21800, average_reward -0.069679
Episode 21800: Total Loss of tensor([[13.4225]], grad_fn=<SubBackward0>)
[2022-11-05 22:24:34.909584] Process 0. Episode 21600, average_reward -0.069861
Episode 21600: Total Loss of tensor([[1.3083]], grad_fn=<SubBackward0>)
[2022-11-05 22:24:37.238708] Process 2. Episode 22000, average_reward -0.074773
Episode 22000: Total Loss of tensor([[-3.2442]], grad_fn=<SubBackward0>)
[2022-11-05 22:24:50.250229] Process 1. Episode 21150, average_reward -0.068936
Episode 21150: Total Loss of tensor([[3.2237]], grad_fn=<SubBackward0>)
[2022-11-05 22:25:10.882483] Process 3. Episode 21500, average_reward -0.072512
Episode 21500: Total Loss of tensor([[5.7297]], grad_fn=<SubBackward0>)
[2022-11-05 22:25:23.918048] Process 4. Episode 22950, average_reward -0.072200
Episode 22950: Total Loss of tensor([[-104.8603]], grad_fn=<SubBackward0>)
[2022-11-05 22:26:37.690964] Process 5. Episode 21850, average_reward -0.069748
Episode 21850: Total Loss of tensor([[15.0846]], grad_fn=<SubBackward0>)
[2022-11-05 22:26:59.099317] Process 2. Episode 22050, average_reward -0.074830
Episode 22050: Total Loss of tensor([[-106.9343]], grad_fn=<SubBackward0>)
[2022-11-05 22:27:02.847929] Process 0. Episode 21650, average_reward -0.069931
Episode 21650: Total Loss of tensor([[4.2031]], grad_fn=<SubBackward0>)
[2022-11-05 22:27:35.515408] Process 3. Episode 21550, average_reward -0.072622
Episode 21550: Total Loss of tensor([[2.3907]], grad_fn=<SubBackward0>)
[2022-11-05 22:27:38.456338] Process 1. Episode 21200, average_reward -0.068821
Episode 21200: Total Loss of tensor([[31.5318]], grad_fn=<SubBackward0>)
[2022-11-05 22:27:44.474099] Process 4. Episode 23000, average_reward -0.072391
Episode 23000: Total Loss of tensor([[14.7607]], grad_fn=<SubBackward0>)
[2022-11-05 22:29:18.554806] Process 5. Episode 21900, average_reward -0.069680
Episode 21900: Total Loss of tensor([[10.2569]], grad_fn=<SubBackward0>)
[2022-11-05 22:29:20.733427] Process 2. Episode 22100, average_reward -0.074887
Episode 22100: Total Loss of tensor([[13.5491]], grad_fn=<SubBackward0>)
[2022-11-05 22:29:27.472020] Process 0. Episode 21700, average_reward -0.069816
Episode 21700: Total Loss of tensor([[6.7270]], grad_fn=<SubBackward0>)
[2022-11-05 22:30:02.302739] Process 4. Episode 23050, average_reward -0.072364
Episode 23050: Total Loss of tensor([[1.5324]], grad_fn=<SubBackward0>)
[2022-11-05 22:30:02.752358] Process 3. Episode 21600, average_reward -0.072546
Episode 21600: Total Loss of tensor([[9.3545]], grad_fn=<SubBackward0>)
[2022-11-05 22:30:21.625931] Process 1. Episode 21250, average_reward -0.068800
Episode 21250: Total Loss of tensor([[5.9655]], grad_fn=<SubBackward0>)
[2022-11-05 22:31:44.342644] Process 2. Episode 22150, average_reward -0.075034
Episode 22150: Total Loss of tensor([[7.0767]], grad_fn=<SubBackward0>)
[2022-11-05 22:31:57.918544] Process 0. Episode 21750, average_reward -0.069839
Episode 21750: Total Loss of tensor([[17.0177]], grad_fn=<SubBackward0>)
[2022-11-05 22:31:58.907255] Process 5. Episode 21950, average_reward -0.069704
Episode 21950: Total Loss of tensor([[-3.8304]], grad_fn=<SubBackward0>)
[2022-11-05 22:32:22.674127] Process 4. Episode 23100, average_reward -0.072294
Episode 23100: Total Loss of tensor([[5.9287]], grad_fn=<SubBackward0>)
[2022-11-05 22:32:35.722974] Process 3. Episode 21650, average_reward -0.072656
Episode 21650: Total Loss of tensor([[13.2027]], grad_fn=<SubBackward0>)
[2022-11-05 22:32:59.843306] Process 1. Episode 21300, average_reward -0.068826
Episode 21300: Total Loss of tensor([[15.9909]], grad_fn=<SubBackward0>)
[2022-11-05 22:34:06.233397] Process 2. Episode 22200, average_reward -0.075090
Episode 22200: Total Loss of tensor([[-5.3831]], grad_fn=<SubBackward0>)
[2022-11-05 22:34:22.686144] Process 0. Episode 21800, average_reward -0.069817
Episode 21800: Total Loss of tensor([[2.3559]], grad_fn=<SubBackward0>)
[2022-11-05 22:34:41.189733] Process 5. Episode 22000, average_reward -0.069773
Episode 22000: Total Loss of tensor([[7.0793]], grad_fn=<SubBackward0>)
[2022-11-05 22:34:43.434266] Process 4. Episode 23150, average_reward -0.072225
Episode 23150: Total Loss of tensor([[2.7043]], grad_fn=<SubBackward0>)
[2022-11-05 22:34:56.418640] Process 3. Episode 21700, average_reward -0.072581
Episode 21700: Total Loss of tensor([[1.8543]], grad_fn=<SubBackward0>)
[2022-11-05 22:35:44.245624] Process 1. Episode 21350, average_reward -0.068852
Episode 21350: Total Loss of tensor([[9.2134]], grad_fn=<SubBackward0>)
[2022-11-05 22:36:45.531102] Process 2. Episode 22250, average_reward -0.075056
Episode 22250: Total Loss of tensor([[4.3288]], grad_fn=<SubBackward0>)
[2022-11-05 22:36:45.598837] Process 0. Episode 21850, average_reward -0.069886
Episode 21850: Total Loss of tensor([[-8.7455]], grad_fn=<SubBackward0>)
[2022-11-05 22:37:02.925845] Process 4. Episode 23200, average_reward -0.072284
Episode 23200: Total Loss of tensor([[19.9925]], grad_fn=<SubBackward0>)
[2022-11-05 22:37:18.393176] Process 3. Episode 21750, average_reward -0.072690
Episode 21750: Total Loss of tensor([[13.9010]], grad_fn=<SubBackward0>)
[2022-11-05 22:37:21.435325] Process 5. Episode 22050, average_reward -0.069887
Episode 22050: Total Loss of tensor([[17.2123]], grad_fn=<SubBackward0>)
[2022-11-05 22:38:19.912952] Process 1. Episode 21400, average_reward -0.068785
Episode 21400: Total Loss of tensor([[10.6679]], grad_fn=<SubBackward0>)
[2022-11-05 22:39:15.989667] Process 0. Episode 21900, average_reward -0.069772
Episode 21900: Total Loss of tensor([[12.5668]], grad_fn=<SubBackward0>)
[2022-11-05 22:39:23.485380] Process 4. Episode 23250, average_reward -0.072301
Episode 23250: Total Loss of tensor([[9.7504]], grad_fn=<SubBackward0>)
[2022-11-05 22:39:28.118100] Process 2. Episode 22300, average_reward -0.075067
Episode 22300: Total Loss of tensor([[6.1222]], grad_fn=<SubBackward0>)
[2022-11-05 22:39:50.409981] Process 3. Episode 21800, average_reward -0.072798
Episode 21800: Total Loss of tensor([[-2.4484]], grad_fn=<SubBackward0>)
[2022-11-05 22:39:55.574150] Process 5. Episode 22100, average_reward -0.069955
Episode 22100: Total Loss of tensor([[-16.2395]], grad_fn=<SubBackward0>)
[2022-11-05 22:40:50.343058] Process 1. Episode 21450, average_reward -0.068811
Episode 21450: Total Loss of tensor([[1.2667]], grad_fn=<SubBackward0>)
[2022-11-05 22:41:43.099691] Process 0. Episode 21950, average_reward -0.069795
Episode 21950: Total Loss of tensor([[-60.8717]], grad_fn=<SubBackward0>)
[2022-11-05 22:41:45.974077] Process 4. Episode 23300, average_reward -0.072361
Episode 23300: Total Loss of tensor([[-1.3479]], grad_fn=<SubBackward0>)
[2022-11-05 22:41:52.057084] Process 2. Episode 22350, average_reward -0.075078
Episode 22350: Total Loss of tensor([[-3.3538]], grad_fn=<SubBackward0>)
[2022-11-05 22:42:27.159341] Process 5. Episode 22150, average_reward -0.069842
Episode 22150: Total Loss of tensor([[18.1517]], grad_fn=<SubBackward0>)
[2022-11-05 22:42:28.187416] Process 3. Episode 21850, average_reward -0.072723
Episode 21850: Total Loss of tensor([[8.2319]], grad_fn=<SubBackward0>)
[2022-11-05 22:43:21.900394] Process 1. Episode 21500, average_reward -0.068744
Episode 21500: Total Loss of tensor([[4.3719]], grad_fn=<SubBackward0>)
[2022-11-05 22:44:09.310571] Process 4. Episode 23350, average_reward -0.072377
Episode 23350: Total Loss of tensor([[4.3228]], grad_fn=<SubBackward0>)
[2022-11-05 22:44:10.897193] Process 0. Episode 22000, average_reward -0.069909
Episode 22000: Total Loss of tensor([[23.0341]], grad_fn=<SubBackward0>)
[2022-11-05 22:44:21.176300] Process 2. Episode 22400, average_reward -0.075089
Episode 22400: Total Loss of tensor([[0.0663]], grad_fn=<SubBackward0>)
[2022-11-05 22:44:57.585933] Process 5. Episode 22200, average_reward -0.069955
Episode 22200: Total Loss of tensor([[-0.8879]], grad_fn=<SubBackward0>)
[2022-11-05 22:45:04.363366] Process 3. Episode 21900, average_reward -0.072785
Episode 21900: Total Loss of tensor([[-37.0435]], grad_fn=<SubBackward0>)
[2022-11-05 22:45:45.603422] Process 1. Episode 21550, average_reward -0.068631
Episode 21550: Total Loss of tensor([[10.4550]], grad_fn=<SubBackward0>)
[2022-11-05 22:46:25.633499] Process 4. Episode 23400, average_reward -0.072350
Episode 23400: Total Loss of tensor([[7.7073]], grad_fn=<SubBackward0>)
[2022-11-05 22:46:45.056849] Process 0. Episode 22050, average_reward -0.069841
Episode 22050: Total Loss of tensor([[19.8896]], grad_fn=<SubBackward0>)
[2022-11-05 22:47:03.620521] Process 2. Episode 22450, average_reward -0.075056
Episode 22450: Total Loss of tensor([[-1.5917]], grad_fn=<SubBackward0>)
[2022-11-05 22:47:38.912365] Process 5. Episode 22250, average_reward -0.069978
Episode 22250: Total Loss of tensor([[-60.0489]], grad_fn=<SubBackward0>)
[2022-11-05 22:47:43.353026] Process 3. Episode 21950, average_reward -0.072802
Episode 21950: Total Loss of tensor([[11.2506]], grad_fn=<SubBackward0>)
[2022-11-05 22:48:05.667343] Process 1. Episode 21600, average_reward -0.068704
Episode 21600: Total Loss of tensor([[5.2459]], grad_fn=<SubBackward0>)
[2022-11-05 22:48:43.273412] Process 4. Episode 23450, average_reward -0.072452
Episode 23450: Total Loss of tensor([[-52.2160]], grad_fn=<SubBackward0>)
[2022-11-05 22:49:23.935012] Process 0. Episode 22100, average_reward -0.069819
Episode 22100: Total Loss of tensor([[9.9999]], grad_fn=<SubBackward0>)
[2022-11-05 22:49:41.102537] Process 2. Episode 22500, average_reward -0.075111
Episode 22500: Total Loss of tensor([[13.2245]], grad_fn=<SubBackward0>)
[2022-11-05 22:50:09.230902] Process 5. Episode 22300, average_reward -0.070090
Episode 22300: Total Loss of tensor([[-14.4698]], grad_fn=<SubBackward0>)
[2022-11-05 22:50:22.429107] Process 3. Episode 22000, average_reward -0.072818
Episode 22000: Total Loss of tensor([[0.4329]], grad_fn=<SubBackward0>)
[2022-11-05 22:50:29.532300] Process 1. Episode 21650, average_reward -0.068591
Episode 21650: Total Loss of tensor([[1.0784]], grad_fn=<SubBackward0>)
[2022-11-05 22:51:06.741905] Process 4. Episode 23500, average_reward -0.072468
Episode 23500: Total Loss of tensor([[1.3381]], grad_fn=<SubBackward0>)
[2022-11-05 22:51:52.540991] Process 0. Episode 22150, average_reward -0.069797
Episode 22150: Total Loss of tensor([[-51.5803]], grad_fn=<SubBackward0>)
[2022-11-05 22:52:06.169769] Process 2. Episode 22550, average_reward -0.075078
Episode 22550: Total Loss of tensor([[11.1677]], grad_fn=<SubBackward0>)
[2022-11-05 22:52:44.882190] Process 5. Episode 22350, average_reward -0.070067
Episode 22350: Total Loss of tensor([[16.7616]], grad_fn=<SubBackward0>)
[2022-11-05 22:52:55.057385] Process 1. Episode 21700, average_reward -0.068571
Episode 21700: Total Loss of tensor([[-129.1064]], grad_fn=<SubBackward0>)
[2022-11-05 22:53:08.808776] Process 3. Episode 22050, average_reward -0.072789
Episode 22050: Total Loss of tensor([[6.5785]], grad_fn=<SubBackward0>)
[2022-11-05 22:53:27.384866] Process 4. Episode 23550, average_reward -0.072399
Episode 23550: Total Loss of tensor([[3.0090]], grad_fn=<SubBackward0>)
[2022-11-05 22:54:15.548386] Process 0. Episode 22200, average_reward -0.069820
Episode 22200: Total Loss of tensor([[10.4127]], grad_fn=<SubBackward0>)
[2022-11-05 22:54:30.337018] Process 2. Episode 22600, average_reward -0.075044
Episode 22600: Total Loss of tensor([[-73.6037]], grad_fn=<SubBackward0>)
[2022-11-05 22:55:18.769983] Process 5. Episode 22400, average_reward -0.070089
Episode 22400: Total Loss of tensor([[9.2158]], grad_fn=<SubBackward0>)
[2022-11-05 22:55:28.365991] Process 1. Episode 21750, average_reward -0.068644
Episode 21750: Total Loss of tensor([[30.4699]], grad_fn=<SubBackward0>)
[2022-11-05 22:55:46.985837] Process 3. Episode 22100, average_reward -0.072805
Episode 22100: Total Loss of tensor([[8.8033]], grad_fn=<SubBackward0>)
[2022-11-05 22:55:47.195910] Process 4. Episode 23600, average_reward -0.072415
Episode 23600: Total Loss of tensor([[3.4131]], grad_fn=<SubBackward0>)
[2022-11-05 22:56:43.331160] Process 0. Episode 22250, average_reward -0.069933
Episode 22250: Total Loss of tensor([[-86.2297]], grad_fn=<SubBackward0>)
[2022-11-05 22:56:54.337793] Process 2. Episode 22650, average_reward -0.075011
Episode 22650: Total Loss of tensor([[-20.0727]], grad_fn=<SubBackward0>)
[2022-11-05 22:57:58.600794] Process 5. Episode 22450, average_reward -0.069933
Episode 22450: Total Loss of tensor([[10.0162]], grad_fn=<SubBackward0>)
[2022-11-05 22:58:06.710758] Process 4. Episode 23650, average_reward -0.072304
Episode 23650: Total Loss of tensor([[2.1472]], grad_fn=<SubBackward0>)
[2022-11-05 22:58:09.028634] Process 1. Episode 21800, average_reward -0.068624
Episode 21800: Total Loss of tensor([[13.8789]], grad_fn=<SubBackward0>)
[2022-11-05 22:58:17.125130] Process 3. Episode 22150, average_reward -0.072777
Episode 22150: Total Loss of tensor([[-46.6187]], grad_fn=<SubBackward0>)
[2022-11-05 22:59:12.034355] Process 0. Episode 22300, average_reward -0.070000
Episode 22300: Total Loss of tensor([[0.6971]], grad_fn=<SubBackward0>)
[2022-11-05 22:59:20.563887] Process 2. Episode 22700, average_reward -0.074890
Episode 22700: Total Loss of tensor([[17.4057]], grad_fn=<SubBackward0>)
[2022-11-05 23:00:25.430495] Process 4. Episode 23700, average_reward -0.072278
Episode 23700: Total Loss of tensor([[-37.9393]], grad_fn=<SubBackward0>)
[2022-11-05 23:00:27.438166] Process 5. Episode 22500, average_reward -0.070000
Episode 22500: Total Loss of tensor([[3.8699]], grad_fn=<SubBackward0>)
[2022-11-05 23:00:48.232634] Process 1. Episode 21850, average_reward -0.068558
Episode 21850: Total Loss of tensor([[-0.2172]], grad_fn=<SubBackward0>)
[2022-11-05 23:00:49.032737] Process 3. Episode 22200, average_reward -0.072883
Episode 22200: Total Loss of tensor([[4.0400]], grad_fn=<SubBackward0>)
[2022-11-05 23:01:40.831526] Process 0. Episode 22350, average_reward -0.069933
Episode 22350: Total Loss of tensor([[9.4449]], grad_fn=<SubBackward0>)
[2022-11-05 23:01:45.452494] Process 2. Episode 22750, average_reward -0.074857
Episode 22750: Total Loss of tensor([[2.4600]], grad_fn=<SubBackward0>)
[2022-11-05 23:02:47.137219] Process 4. Episode 23750, average_reward -0.072421
Episode 23750: Total Loss of tensor([[6.9972]], grad_fn=<SubBackward0>)
[2022-11-05 23:03:08.343489] Process 5. Episode 22550, average_reward -0.070067
Episode 22550: Total Loss of tensor([[14.3548]], grad_fn=<SubBackward0>)
[2022-11-05 23:03:14.867535] Process 1. Episode 21900, average_reward -0.068584
Episode 21900: Total Loss of tensor([[11.6080]], grad_fn=<SubBackward0>)
[2022-11-05 23:03:14.944268] Process 3. Episode 22250, average_reward -0.072854
Episode 22250: Total Loss of tensor([[4.6714]], grad_fn=<SubBackward0>)
[2022-11-05 23:04:04.889849] Process 0. Episode 22400, average_reward -0.069866
Episode 22400: Total Loss of tensor([[4.1733]], grad_fn=<SubBackward0>)
[2022-11-05 23:04:14.315288] Process 2. Episode 22800, average_reward -0.074737
Episode 22800: Total Loss of tensor([[1.3109]], grad_fn=<SubBackward0>)
[2022-11-05 23:05:08.997643] Process 4. Episode 23800, average_reward -0.072395
Episode 23800: Total Loss of tensor([[0.6485]], grad_fn=<SubBackward0>)
[2022-11-05 23:05:40.981255] Process 3. Episode 22300, average_reward -0.072915
Episode 22300: Total Loss of tensor([[4.9203]], grad_fn=<SubBackward0>)
[2022-11-05 23:05:46.861936] Process 1. Episode 21950, average_reward -0.068702
Episode 21950: Total Loss of tensor([[7.6334]], grad_fn=<SubBackward0>)
[2022-11-05 23:05:51.766614] Process 5. Episode 22600, average_reward -0.070221
Episode 22600: Total Loss of tensor([[17.0832]], grad_fn=<SubBackward0>)
[2022-11-05 23:06:32.408206] Process 0. Episode 22450, average_reward -0.069800
Episode 22450: Total Loss of tensor([[16.2727]], grad_fn=<SubBackward0>)
[2022-11-05 23:06:40.113061] Process 2. Episode 22850, average_reward -0.074661
Episode 22850: Total Loss of tensor([[8.1747]], grad_fn=<SubBackward0>)
[2022-11-05 23:07:30.412153] Process 4. Episode 23850, average_reward -0.072369
Episode 23850: Total Loss of tensor([[17.6839]], grad_fn=<SubBackward0>)
[2022-11-05 23:08:13.369733] Process 3. Episode 22350, average_reward -0.072886
Episode 22350: Total Loss of tensor([[15.0874]], grad_fn=<SubBackward0>)
[2022-11-05 23:08:23.001606] Process 1. Episode 22000, average_reward -0.068682
Episode 22000: Total Loss of tensor([[-3.3573]], grad_fn=<SubBackward0>)
[2022-11-05 23:08:26.782916] Process 5. Episode 22650, average_reward -0.070331
Episode 22650: Total Loss of tensor([[6.3995]], grad_fn=<SubBackward0>)
[2022-11-05 23:09:06.802327] Process 2. Episode 22900, average_reward -0.074585
Episode 22900: Total Loss of tensor([[12.5068]], grad_fn=<SubBackward0>)
[2022-11-05 23:09:11.305462] Process 0. Episode 22500, average_reward -0.069733
Episode 22500: Total Loss of tensor([[2.1935]], grad_fn=<SubBackward0>)
[2022-11-05 23:09:51.635840] Process 4. Episode 23900, average_reward -0.072343
Episode 23900: Total Loss of tensor([[-111.9007]], grad_fn=<SubBackward0>)
[2022-11-05 23:10:41.082759] Process 3. Episode 22400, average_reward -0.072946
Episode 22400: Total Loss of tensor([[-81.4735]], grad_fn=<SubBackward0>)
[2022-11-05 23:10:55.421911] Process 1. Episode 22050, average_reward -0.068617
Episode 22050: Total Loss of tensor([[-0.0844]], grad_fn=<SubBackward0>)
[2022-11-05 23:11:07.587212] Process 5. Episode 22700, average_reward -0.070352
Episode 22700: Total Loss of tensor([[-16.2882]], grad_fn=<SubBackward0>)
[2022-11-05 23:11:34.828053] Process 2. Episode 22950, average_reward -0.074815
Episode 22950: Total Loss of tensor([[23.3605]], grad_fn=<SubBackward0>)
[2022-11-05 23:11:53.209292] Process 0. Episode 22550, average_reward -0.069667
Episode 22550: Total Loss of tensor([[-18.7429]], grad_fn=<SubBackward0>)
[2022-11-05 23:12:13.907552] Process 4. Episode 23950, average_reward -0.072234
Episode 23950: Total Loss of tensor([[4.4317]], grad_fn=<SubBackward0>)
[2022-11-05 23:13:26.970328] Process 3. Episode 22450, average_reward -0.072962
Episode 22450: Total Loss of tensor([[9.4651]], grad_fn=<SubBackward0>)
[2022-11-05 23:13:35.168259] Process 1. Episode 22100, average_reward -0.068643
Episode 22100: Total Loss of tensor([[-22.8169]], grad_fn=<SubBackward0>)
[2022-11-05 23:13:45.664761] Process 5. Episode 22750, average_reward -0.070330
Episode 22750: Total Loss of tensor([[3.3229]], grad_fn=<SubBackward0>)
[2022-11-05 23:14:00.569030] Process 2. Episode 23000, average_reward -0.074739
Episode 23000: Total Loss of tensor([[16.3955]], grad_fn=<SubBackward0>)
[2022-11-05 23:14:26.834710] Process 0. Episode 22600, average_reward -0.069602
Episode 22600: Total Loss of tensor([[13.9246]], grad_fn=<SubBackward0>)
[2022-11-05 23:14:30.049039] Process 4. Episode 24000, average_reward -0.072333
Episode 24000: Total Loss of tensor([[10.0218]], grad_fn=<SubBackward0>)
[2022-11-05 23:15:57.816101] Process 3. Episode 22500, average_reward -0.072889
Episode 22500: Total Loss of tensor([[9.8734]], grad_fn=<SubBackward0>)
[2022-11-05 23:16:16.447080] Process 1. Episode 22150, average_reward -0.068804
Episode 22150: Total Loss of tensor([[-26.6093]], grad_fn=<SubBackward0>)
[2022-11-05 23:16:30.841514] Process 5. Episode 22800, average_reward -0.070395
Episode 22800: Total Loss of tensor([[9.6837]], grad_fn=<SubBackward0>)
[2022-11-05 23:16:32.796293] Process 2. Episode 23050, average_reward -0.074620
Episode 23050: Total Loss of tensor([[5.9319]], grad_fn=<SubBackward0>)
[2022-11-05 23:16:59.420084] Process 4. Episode 24050, average_reward -0.072266
Episode 24050: Total Loss of tensor([[-42.6975]], grad_fn=<SubBackward0>)
[2022-11-05 23:17:01.489716] Process 0. Episode 22650, average_reward -0.069581
Episode 22650: Total Loss of tensor([[4.5086]], grad_fn=<SubBackward0>)
[2022-11-05 23:18:44.560957] Process 3. Episode 22550, average_reward -0.072905
Episode 22550: Total Loss of tensor([[5.9604]], grad_fn=<SubBackward0>)
[2022-11-05 23:19:13.948141] Process 2. Episode 23100, average_reward -0.074632
Episode 23100: Total Loss of tensor([[8.6117]], grad_fn=<SubBackward0>)
[2022-11-05 23:19:17.495762] Process 1. Episode 22200, average_reward -0.068784
Episode 22200: Total Loss of tensor([[9.1791]], grad_fn=<SubBackward0>)
[2022-11-05 23:19:31.985369] Process 5. Episode 22850, average_reward -0.070372
Episode 22850: Total Loss of tensor([[-2.8525]], grad_fn=<SubBackward0>)
[2022-11-05 23:19:36.213251] Process 4. Episode 24100, average_reward -0.072241
Episode 24100: Total Loss of tensor([[-105.8531]], grad_fn=<SubBackward0>)
[2022-11-05 23:19:40.087311] Process 0. Episode 22700, average_reward -0.069604
Episode 22700: Total Loss of tensor([[-1.0902]], grad_fn=<SubBackward0>)
[2022-11-05 23:21:21.028272] Process 3. Episode 22600, average_reward -0.072832
Episode 22600: Total Loss of tensor([[8.3281]], grad_fn=<SubBackward0>)
[2022-11-05 23:21:49.088241] Process 2. Episode 23150, average_reward -0.074730
Episode 23150: Total Loss of tensor([[6.8466]], grad_fn=<SubBackward0>)
[2022-11-05 23:21:51.078721] Process 1. Episode 22250, average_reward -0.068899
Episode 22250: Total Loss of tensor([[22.2917]], grad_fn=<SubBackward0>)
[2022-11-05 23:22:02.691241] Process 4. Episode 24150, average_reward -0.072257
Episode 24150: Total Loss of tensor([[12.5380]], grad_fn=<SubBackward0>)
[2022-11-05 23:22:11.402966] Process 0. Episode 22750, average_reward -0.069714
Episode 22750: Total Loss of tensor([[15.0495]], grad_fn=<SubBackward0>)
[2022-11-05 23:22:14.027045] Process 5. Episode 22900, average_reward -0.070306
Episode 22900: Total Loss of tensor([[17.1651]], grad_fn=<SubBackward0>)
[2022-11-05 23:23:48.945304] Process 3. Episode 22650, average_reward -0.072715
Episode 22650: Total Loss of tensor([[10.3868]], grad_fn=<SubBackward0>)
[2022-11-05 23:24:27.811223] Process 2. Episode 23200, average_reward -0.074698
Episode 23200: Total Loss of tensor([[1.3980]], grad_fn=<SubBackward0>)
[2022-11-05 23:24:28.258531] Process 1. Episode 22300, average_reward -0.068834
Episode 22300: Total Loss of tensor([[-34.4061]], grad_fn=<SubBackward0>)
[2022-11-05 23:24:29.295273] Process 4. Episode 24200, average_reward -0.072149
Episode 24200: Total Loss of tensor([[-1.7542]], grad_fn=<SubBackward0>)
[2022-11-05 23:24:43.376151] Process 0. Episode 22800, average_reward -0.069649
Episode 22800: Total Loss of tensor([[10.0958]], grad_fn=<SubBackward0>)
[2022-11-05 23:24:53.845701] Process 5. Episode 22950, average_reward -0.070283
Episode 22950: Total Loss of tensor([[7.7168]], grad_fn=<SubBackward0>)
[2022-11-05 23:26:19.066565] Process 3. Episode 22700, average_reward -0.072555
Episode 22700: Total Loss of tensor([[-3.5785]], grad_fn=<SubBackward0>)
[2022-11-05 23:26:54.780515] Process 4. Episode 24250, average_reward -0.072124
Episode 24250: Total Loss of tensor([[-11.3176]], grad_fn=<SubBackward0>)
[2022-11-05 23:26:59.416290] Process 2. Episode 23250, average_reward -0.074624
Episode 23250: Total Loss of tensor([[-0.7470]], grad_fn=<SubBackward0>)
[2022-11-05 23:27:07.388356] Process 1. Episode 22350, average_reward -0.068814
Episode 22350: Total Loss of tensor([[-2.1702]], grad_fn=<SubBackward0>)
[2022-11-05 23:27:12.946820] Process 0. Episode 22850, average_reward -0.069584
Episode 22850: Total Loss of tensor([[6.4665]], grad_fn=<SubBackward0>)
[2022-11-05 23:27:27.714250] Process 5. Episode 23000, average_reward -0.070261
Episode 23000: Total Loss of tensor([[3.9226]], grad_fn=<SubBackward0>)
[2022-11-05 23:28:47.383426] Process 3. Episode 22750, average_reward -0.072527
Episode 22750: Total Loss of tensor([[2.3188]], grad_fn=<SubBackward0>)
[2022-11-05 23:29:18.545735] Process 4. Episode 24300, average_reward -0.072181
Episode 24300: Total Loss of tensor([[-117.8496]], grad_fn=<SubBackward0>)
[2022-11-05 23:29:25.155871] Process 2. Episode 23300, average_reward -0.074721
Episode 23300: Total Loss of tensor([[-96.6835]], grad_fn=<SubBackward0>)
[2022-11-05 23:29:41.643663] Process 0. Episode 22900, average_reward -0.069520
Episode 22900: Total Loss of tensor([[6.7619]], grad_fn=<SubBackward0>)
[2022-11-05 23:29:46.435441] Process 1. Episode 22400, average_reward -0.068795
Episode 22400: Total Loss of tensor([[-29.1865]], grad_fn=<SubBackward0>)
[2022-11-05 23:30:07.977327] Process 5. Episode 23050, average_reward -0.070195
Episode 23050: Total Loss of tensor([[-22.1988]], grad_fn=<SubBackward0>)
[2022-11-05 23:31:17.823761] Process 3. Episode 22800, average_reward -0.072368
Episode 22800: Total Loss of tensor([[12.4234]], grad_fn=<SubBackward0>)
[2022-11-05 23:31:41.113722] Process 4. Episode 24350, average_reward -0.072156
Episode 24350: Total Loss of tensor([[12.5744]], grad_fn=<SubBackward0>)
[2022-11-05 23:31:47.202911] Process 2. Episode 23350, average_reward -0.074690
Episode 23350: Total Loss of tensor([[13.7854]], grad_fn=<SubBackward0>)
[2022-11-05 23:32:09.071130] Process 0. Episode 22950, average_reward -0.069586
Episode 22950: Total Loss of tensor([[12.0141]], grad_fn=<SubBackward0>)
[2022-11-05 23:32:21.201400] Process 1. Episode 22450, average_reward -0.068909
Episode 22450: Total Loss of tensor([[15.1643]], grad_fn=<SubBackward0>)
[2022-11-05 23:32:52.440272] Process 5. Episode 23100, average_reward -0.070087
Episode 23100: Total Loss of tensor([[8.8997]], grad_fn=<SubBackward0>)
[2022-11-05 23:33:53.325738] Process 3. Episode 22850, average_reward -0.072298
Episode 22850: Total Loss of tensor([[8.0378]], grad_fn=<SubBackward0>)
[2022-11-05 23:34:03.750233] Process 4. Episode 24400, average_reward -0.072008
Episode 24400: Total Loss of tensor([[3.1388]], grad_fn=<SubBackward0>)
[2022-11-05 23:34:11.586534] Process 2. Episode 23400, average_reward -0.074744
Episode 23400: Total Loss of tensor([[-8.4809]], grad_fn=<SubBackward0>)
[2022-11-05 23:34:31.623402] Process 0. Episode 23000, average_reward -0.069565
Episode 23000: Total Loss of tensor([[9.8674]], grad_fn=<SubBackward0>)
[2022-11-05 23:34:55.788072] Process 1. Episode 22500, average_reward -0.068800
Episode 22500: Total Loss of tensor([[12.8733]], grad_fn=<SubBackward0>)
[2022-11-05 23:35:26.503235] Process 5. Episode 23150, average_reward -0.070022
Episode 23150: Total Loss of tensor([[-0.0428]], grad_fn=<SubBackward0>)
[2022-11-05 23:36:25.253159] Process 3. Episode 22900, average_reward -0.072314
Episode 22900: Total Loss of tensor([[6.4998]], grad_fn=<SubBackward0>)
[2022-11-05 23:36:25.597021] Process 4. Episode 24450, average_reward -0.072025
Episode 24450: Total Loss of tensor([[0.8655]], grad_fn=<SubBackward0>)
[2022-11-05 23:36:30.683301] Process 2. Episode 23450, average_reward -0.074797
Episode 23450: Total Loss of tensor([[1.7273]], grad_fn=<SubBackward0>)
[2022-11-05 23:36:57.731163] Process 0. Episode 23050, average_reward -0.069588
Episode 23050: Total Loss of tensor([[4.7109]], grad_fn=<SubBackward0>)
[2022-11-05 23:37:29.023794] Process 1. Episode 22550, average_reward -0.068914
Episode 22550: Total Loss of tensor([[-121.0364]], grad_fn=<SubBackward0>)
[2022-11-05 23:38:02.927647] Process 5. Episode 23200, average_reward -0.070086
Episode 23200: Total Loss of tensor([[9.9108]], grad_fn=<SubBackward0>)
[2022-11-05 23:38:46.278113] Process 4. Episode 24500, average_reward -0.072000
Episode 24500: Total Loss of tensor([[3.3901]], grad_fn=<SubBackward0>)
[2022-11-05 23:38:59.386698] Process 2. Episode 23500, average_reward -0.074766
Episode 23500: Total Loss of tensor([[6.9724]], grad_fn=<SubBackward0>)
[2022-11-05 23:39:03.207658] Process 3. Episode 22950, average_reward -0.072157
Episode 22950: Total Loss of tensor([[21.7932]], grad_fn=<SubBackward0>)
[2022-11-05 23:39:23.278459] Process 0. Episode 23100, average_reward -0.069481
Episode 23100: Total Loss of tensor([[8.6736]], grad_fn=<SubBackward0>)
[2022-11-05 23:40:07.211023] Process 1. Episode 22600, average_reward -0.068938
Episode 22600: Total Loss of tensor([[0.8065]], grad_fn=<SubBackward0>)
[2022-11-05 23:40:25.806695] Process 5. Episode 23250, average_reward -0.070151
Episode 23250: Total Loss of tensor([[5.6104]], grad_fn=<SubBackward0>)
[2022-11-05 23:41:02.502999] Process 4. Episode 24550, average_reward -0.071935
Episode 24550: Total Loss of tensor([[15.0204]], grad_fn=<SubBackward0>)
[2022-11-05 23:41:29.355597] Process 2. Episode 23550, average_reward -0.074692
Episode 23550: Total Loss of tensor([[10.9142]], grad_fn=<SubBackward0>)
[2022-11-05 23:41:46.082780] Process 3. Episode 23000, average_reward -0.072130
Episode 23000: Total Loss of tensor([[12.9010]], grad_fn=<SubBackward0>)
[2022-11-05 23:41:47.541909] Process 0. Episode 23150, average_reward -0.069546
Episode 23150: Total Loss of tensor([[2.9418]], grad_fn=<SubBackward0>)
[2022-11-05 23:42:38.403144] Process 1. Episode 22650, average_reward -0.068874
Episode 22650: Total Loss of tensor([[-0.9913]], grad_fn=<SubBackward0>)
[2022-11-05 23:43:00.390498] Process 5. Episode 23300, average_reward -0.070172
Episode 23300: Total Loss of tensor([[1.6678]], grad_fn=<SubBackward0>)
[2022-11-05 23:43:26.009550] Process 4. Episode 24600, average_reward -0.071829
Episode 24600: Total Loss of tensor([[5.7892]], grad_fn=<SubBackward0>)
[2022-11-05 23:43:47.662218] Process 2. Episode 23600, average_reward -0.074576
Episode 23600: Total Loss of tensor([[-2.6258]], grad_fn=<SubBackward0>)
[2022-11-05 23:44:15.264263] Process 0. Episode 23200, average_reward -0.069569
Episode 23200: Total Loss of tensor([[3.6326]], grad_fn=<SubBackward0>)
[2022-11-05 23:44:17.833880] Process 3. Episode 23050, average_reward -0.072148
Episode 23050: Total Loss of tensor([[-134.0792]], grad_fn=<SubBackward0>)
[2022-11-05 23:45:14.091123] Process 1. Episode 22700, average_reward -0.068811
Episode 22700: Total Loss of tensor([[5.2154]], grad_fn=<SubBackward0>)
[2022-11-05 23:45:32.973021] Process 5. Episode 23350, average_reward -0.070193
Episode 23350: Total Loss of tensor([[8.7269]], grad_fn=<SubBackward0>)
[2022-11-05 23:45:51.153138] Process 4. Episode 24650, average_reward -0.071684
Episode 24650: Total Loss of tensor([[13.7818]], grad_fn=<SubBackward0>)
[2022-11-05 23:46:23.204700] Process 2. Episode 23650, average_reward -0.074588
Episode 23650: Total Loss of tensor([[21.8272]], grad_fn=<SubBackward0>)
[2022-11-05 23:46:38.001995] Process 3. Episode 23100, average_reward -0.072208
Episode 23100: Total Loss of tensor([[14.5553]], grad_fn=<SubBackward0>)
[2022-11-05 23:46:42.390822] Process 0. Episode 23250, average_reward -0.069634
Episode 23250: Total Loss of tensor([[10.6905]], grad_fn=<SubBackward0>)
[2022-11-05 23:47:51.795654] Process 1. Episode 22750, average_reward -0.068791
Episode 22750: Total Loss of tensor([[-0.3154]], grad_fn=<SubBackward0>)
[2022-11-05 23:48:01.062800] Process 5. Episode 23400, average_reward -0.070085
Episode 23400: Total Loss of tensor([[12.9516]], grad_fn=<SubBackward0>)
[2022-11-05 23:48:10.995691] Process 4. Episode 24700, average_reward -0.071619
Episode 24700: Total Loss of tensor([[6.9850]], grad_fn=<SubBackward0>)
[2022-11-05 23:48:47.089002] Process 2. Episode 23700, average_reward -0.074726
Episode 23700: Total Loss of tensor([[-19.6644]], grad_fn=<SubBackward0>)
[2022-11-05 23:49:02.422111] Process 3. Episode 23150, average_reward -0.072181
Episode 23150: Total Loss of tensor([[-19.5277]], grad_fn=<SubBackward0>)
[2022-11-05 23:49:17.872299] Process 0. Episode 23300, average_reward -0.069700
Episode 23300: Total Loss of tensor([[5.0668]], grad_fn=<SubBackward0>)
[2022-11-05 23:50:30.373295] Process 5. Episode 23450, average_reward -0.070064
Episode 23450: Total Loss of tensor([[-9.1906]], grad_fn=<SubBackward0>)
[2022-11-05 23:50:33.126329] Process 4. Episode 24750, average_reward -0.071758
Episode 24750: Total Loss of tensor([[13.8959]], grad_fn=<SubBackward0>)
[2022-11-05 23:50:36.538055] Process 1. Episode 22800, average_reward -0.068772
Episode 22800: Total Loss of tensor([[-21.3691]], grad_fn=<SubBackward0>)
[2022-11-05 23:51:15.569671] Process 2. Episode 23750, average_reward -0.074568
Episode 23750: Total Loss of tensor([[6.1792]], grad_fn=<SubBackward0>)
[2022-11-05 23:51:24.305070] Process 3. Episode 23200, average_reward -0.072112
Episode 23200: Total Loss of tensor([[6.2582]], grad_fn=<SubBackward0>)
[2022-11-05 23:51:52.871729] Process 0. Episode 23350, average_reward -0.069679
Episode 23350: Total Loss of tensor([[-0.6224]], grad_fn=<SubBackward0>)
[2022-11-05 23:52:57.622923] Process 4. Episode 24800, average_reward -0.071734
Episode 24800: Total Loss of tensor([[4.8374]], grad_fn=<SubBackward0>)
[2022-11-05 23:53:00.208082] Process 5. Episode 23500, average_reward -0.070000
Episode 23500: Total Loss of tensor([[7.1644]], grad_fn=<SubBackward0>)
[2022-11-05 23:53:07.437873] Process 1. Episode 22850, average_reward -0.068709
Episode 22850: Total Loss of tensor([[10.3700]], grad_fn=<SubBackward0>)
[2022-11-05 23:53:38.348231] Process 2. Episode 23800, average_reward -0.074496
Episode 23800: Total Loss of tensor([[5.6848]], grad_fn=<SubBackward0>)
[2022-11-05 23:53:48.230718] Process 3. Episode 23250, average_reward -0.072215
Episode 23250: Total Loss of tensor([[-14.2451]], grad_fn=<SubBackward0>)
[2022-11-05 23:54:16.535582] Process 0. Episode 23400, average_reward -0.069658
Episode 23400: Total Loss of tensor([[17.7811]], grad_fn=<SubBackward0>)
[2022-11-05 23:55:23.539258] Process 4. Episode 24850, average_reward -0.071670
Episode 24850: Total Loss of tensor([[4.2523]], grad_fn=<SubBackward0>)
[2022-11-05 23:55:35.620327] Process 5. Episode 23550, average_reward -0.070021
Episode 23550: Total Loss of tensor([[17.8073]], grad_fn=<SubBackward0>)
[2022-11-05 23:55:43.417383] Process 1. Episode 22900, average_reward -0.068821
Episode 22900: Total Loss of tensor([[-1.1778]], grad_fn=<SubBackward0>)
[2022-11-05 23:56:02.767513] Process 2. Episode 23850, average_reward -0.074549
Episode 23850: Total Loss of tensor([[17.5513]], grad_fn=<SubBackward0>)
[2022-11-05 23:56:11.369000] Process 3. Episode 23300, average_reward -0.072318
Episode 23300: Total Loss of tensor([[7.7308]], grad_fn=<SubBackward0>)
[2022-11-05 23:56:47.869908] Process 0. Episode 23450, average_reward -0.069723
Episode 23450: Total Loss of tensor([[11.0812]], grad_fn=<SubBackward0>)
[2022-11-05 23:57:47.043811] Process 4. Episode 24900, average_reward -0.071647
Episode 24900: Total Loss of tensor([[22.5797]], grad_fn=<SubBackward0>)
[2022-11-05 23:58:10.617300] Process 1. Episode 22950, average_reward -0.068845
Episode 22950: Total Loss of tensor([[2.9645]], grad_fn=<SubBackward0>)
[2022-11-05 23:58:17.994754] Process 5. Episode 23600, average_reward -0.070042
Episode 23600: Total Loss of tensor([[-29.8319]], grad_fn=<SubBackward0>)
[2022-11-05 23:58:25.107397] Process 2. Episode 23900, average_reward -0.074435
Episode 23900: Total Loss of tensor([[3.3039]], grad_fn=<SubBackward0>)
[2022-11-05 23:58:31.611389] Process 3. Episode 23350, average_reward -0.072420
Episode 23350: Total Loss of tensor([[-7.7936]], grad_fn=<SubBackward0>)
[2022-11-05 23:59:14.637375] Process 0. Episode 23500, average_reward -0.069787
Episode 23500: Total Loss of tensor([[1.0005]], grad_fn=<SubBackward0>)
[2022-11-06 00:00:11.068199] Process 4. Episode 24950, average_reward -0.071623
Episode 24950: Total Loss of tensor([[7.6259]], grad_fn=<SubBackward0>)
[2022-11-06 00:00:38.588457] Process 1. Episode 23000, average_reward -0.068870
Episode 23000: Total Loss of tensor([[16.6761]], grad_fn=<SubBackward0>)
[2022-11-06 00:00:46.826053] Process 2. Episode 23950, average_reward -0.074322
Episode 23950: Total Loss of tensor([[6.4462]], grad_fn=<SubBackward0>)
[2022-11-06 00:00:50.897801] Process 5. Episode 23650, average_reward -0.070106
Episode 23650: Total Loss of tensor([[25.1668]], grad_fn=<SubBackward0>)
[2022-11-06 00:00:53.233547] Process 3. Episode 23400, average_reward -0.072436
Episode 23400: Total Loss of tensor([[7.5927]], grad_fn=<SubBackward0>)
[2022-11-06 00:01:51.020652] Process 0. Episode 23550, average_reward -0.069894
Episode 23550: Total Loss of tensor([[13.6959]], grad_fn=<SubBackward0>)
[2022-11-06 00:02:34.084207] Process 4. Episode 25000, average_reward -0.071680
Episode 25000: Total Loss of tensor([[19.7345]], grad_fn=<SubBackward0>)
[2022-11-06 00:03:02.986754] Process 1. Episode 23050, average_reward -0.068850
Episode 23050: Total Loss of tensor([[8.8370]], grad_fn=<SubBackward0>)
[2022-11-06 00:03:06.290580] Process 2. Episode 24000, average_reward -0.074375
Episode 24000: Total Loss of tensor([[-1.7895]], grad_fn=<SubBackward0>)
[2022-11-06 00:03:24.901990] Process 5. Episode 23700, average_reward -0.070000
Episode 23700: Total Loss of tensor([[15.8660]], grad_fn=<SubBackward0>)
[2022-11-06 00:03:26.592797] Process 3. Episode 23450, average_reward -0.072623
Episode 23450: Total Loss of tensor([[-119.9870]], grad_fn=<SubBackward0>)
[2022-11-06 00:04:23.433667] Process 0. Episode 23600, average_reward -0.069958
Episode 23600: Total Loss of tensor([[18.9336]], grad_fn=<SubBackward0>)
[2022-11-06 00:04:56.783670] Process 4. Episode 25050, average_reward -0.071697
Episode 25050: Total Loss of tensor([[1.3226]], grad_fn=<SubBackward0>)
[2022-11-06 00:05:34.735183] Process 1. Episode 23100, average_reward -0.068874
Episode 23100: Total Loss of tensor([[-2.8247]], grad_fn=<SubBackward0>)
[2022-11-06 00:05:37.466811] Process 2. Episode 24050, average_reward -0.074304
Episode 24050: Total Loss of tensor([[18.6781]], grad_fn=<SubBackward0>)
[2022-11-06 00:05:51.667513] Process 5. Episode 23750, average_reward -0.069937
Episode 23750: Total Loss of tensor([[12.7085]], grad_fn=<SubBackward0>)
[2022-11-06 00:06:00.020490] Process 3. Episode 23500, average_reward -0.072596
Episode 23500: Total Loss of tensor([[25.5625]], grad_fn=<SubBackward0>)
[2022-11-06 00:07:00.768513] Process 0. Episode 23650, average_reward -0.070021
Episode 23650: Total Loss of tensor([[14.1661]], grad_fn=<SubBackward0>)
[2022-11-06 00:07:18.037392] Process 4. Episode 25100, average_reward -0.071793
Episode 25100: Total Loss of tensor([[14.6872]], grad_fn=<SubBackward0>)
[2022-11-06 00:08:04.139442] Process 1. Episode 23150, average_reward -0.068898
Episode 23150: Total Loss of tensor([[6.9343]], grad_fn=<SubBackward0>)
[2022-11-06 00:08:12.582060] Process 2. Episode 24100, average_reward -0.074315
Episode 24100: Total Loss of tensor([[3.4197]], grad_fn=<SubBackward0>)
[2022-11-06 00:08:16.803683] Process 5. Episode 23800, average_reward -0.069916
Episode 23800: Total Loss of tensor([[-10.9047]], grad_fn=<SubBackward0>)
[2022-11-06 00:08:32.741885] Process 3. Episode 23550, average_reward -0.072569
Episode 23550: Total Loss of tensor([[9.1829]], grad_fn=<SubBackward0>)
[2022-11-06 00:09:23.246257] Process 0. Episode 23700, average_reward -0.069958
Episode 23700: Total Loss of tensor([[4.2012]], grad_fn=<SubBackward0>)
[2022-11-06 00:09:39.279397] Process 4. Episode 25150, average_reward -0.071650
Episode 25150: Total Loss of tensor([[9.3258]], grad_fn=<SubBackward0>)
[2022-11-06 00:10:36.607026] Process 1. Episode 23200, average_reward -0.068879
Episode 23200: Total Loss of tensor([[-112.1455]], grad_fn=<SubBackward0>)
[2022-11-06 00:10:40.337155] Process 5. Episode 23850, average_reward -0.069895
Episode 23850: Total Loss of tensor([[-2.4791]], grad_fn=<SubBackward0>)
[2022-11-06 00:10:57.452596] Process 3. Episode 23600, average_reward -0.072627
Episode 23600: Total Loss of tensor([[-7.1972]], grad_fn=<SubBackward0>)
[2022-11-06 00:10:58.466888] Process 2. Episode 24150, average_reward -0.074369
Episode 24150: Total Loss of tensor([[-10.6174]], grad_fn=<SubBackward0>)
[2022-11-06 00:11:45.600670] Process 0. Episode 23750, average_reward -0.069895
Episode 23750: Total Loss of tensor([[12.3866]], grad_fn=<SubBackward0>)
[2022-11-06 00:11:57.480230] Process 4. Episode 25200, average_reward -0.071627
Episode 25200: Total Loss of tensor([[2.2164]], grad_fn=<SubBackward0>)
[2022-11-06 00:13:06.135935] Process 5. Episode 23900, average_reward -0.069874
Episode 23900: Total Loss of tensor([[4.2257]], grad_fn=<SubBackward0>)
[2022-11-06 00:13:07.606876] Process 1. Episode 23250, average_reward -0.068860
Episode 23250: Total Loss of tensor([[-101.0097]], grad_fn=<SubBackward0>)
[2022-11-06 00:13:29.674003] Process 3. Episode 23650, average_reward -0.072558
Episode 23650: Total Loss of tensor([[5.8248]], grad_fn=<SubBackward0>)
[2022-11-06 00:13:35.765548] Process 2. Episode 24200, average_reward -0.074339
Episode 24200: Total Loss of tensor([[-2.2774]], grad_fn=<SubBackward0>)
[2022-11-06 00:14:14.821240] Process 4. Episode 25250, average_reward -0.071525
Episode 25250: Total Loss of tensor([[-44.2040]], grad_fn=<SubBackward0>)
[2022-11-06 00:14:24.913748] Process 0. Episode 23800, average_reward -0.069916
Episode 23800: Total Loss of tensor([[1.4568]], grad_fn=<SubBackward0>)
[2022-11-06 00:15:32.150422] Process 5. Episode 23950, average_reward -0.069896
Episode 23950: Total Loss of tensor([[8.0517]], grad_fn=<SubBackward0>)
[2022-11-06 00:15:39.783630] Process 1. Episode 23300, average_reward -0.069013
Episode 23300: Total Loss of tensor([[-17.8024]], grad_fn=<SubBackward0>)
[2022-11-06 00:16:09.980357] Process 3. Episode 23700, average_reward -0.072658
Episode 23700: Total Loss of tensor([[5.4456]], grad_fn=<SubBackward0>)
[2022-11-06 00:16:20.872712] Process 2. Episode 24250, average_reward -0.074268
Episode 24250: Total Loss of tensor([[-1.2463]], grad_fn=<SubBackward0>)
[2022-11-06 00:16:36.618695] Process 4. Episode 25300, average_reward -0.071502
Episode 25300: Total Loss of tensor([[11.8972]], grad_fn=<SubBackward0>)
[2022-11-06 00:16:50.845215] Process 0. Episode 23850, average_reward -0.070021
Episode 23850: Total Loss of tensor([[-6.7190]], grad_fn=<SubBackward0>)
[2022-11-06 00:18:06.548404] Process 5. Episode 24000, average_reward -0.069792
Episode 24000: Total Loss of tensor([[3.5446]], grad_fn=<SubBackward0>)
[2022-11-06 00:18:14.829862] Process 1. Episode 23350, average_reward -0.068951
Episode 23350: Total Loss of tensor([[5.8683]], grad_fn=<SubBackward0>)
[2022-11-06 00:18:50.964903] Process 2. Episode 24300, average_reward -0.074444
Episode 24300: Total Loss of tensor([[-108.5195]], grad_fn=<SubBackward0>)
[2022-11-06 00:18:51.285708] Process 3. Episode 23750, average_reward -0.072632
Episode 23750: Total Loss of tensor([[2.5599]], grad_fn=<SubBackward0>)
[2022-11-06 00:18:56.301766] Process 4. Episode 25350, average_reward -0.071637
Episode 25350: Total Loss of tensor([[161.4507]], grad_fn=<SubBackward0>)
[2022-11-06 00:19:17.127942] Process 0. Episode 23900, average_reward -0.070000
Episode 23900: Total Loss of tensor([[12.0622]], grad_fn=<SubBackward0>)
[2022-11-06 00:20:35.414214] Process 5. Episode 24050, average_reward -0.069771
Episode 24050: Total Loss of tensor([[5.6081]], grad_fn=<SubBackward0>)
[2022-11-06 00:20:40.497376] Process 1. Episode 23400, average_reward -0.069017
Episode 23400: Total Loss of tensor([[20.2662]], grad_fn=<SubBackward0>)
[2022-11-06 00:21:17.833440] Process 4. Episode 25400, average_reward -0.071654
Episode 25400: Total Loss of tensor([[9.3970]], grad_fn=<SubBackward0>)
[2022-11-06 00:21:18.603781] Process 3. Episode 23800, average_reward -0.072563
Episode 23800: Total Loss of tensor([[10.1619]], grad_fn=<SubBackward0>)
[2022-11-06 00:21:22.863190] Process 2. Episode 24350, average_reward -0.074374
Episode 24350: Total Loss of tensor([[24.3845]], grad_fn=<SubBackward0>)
[2022-11-06 00:21:53.712758] Process 0. Episode 23950, average_reward -0.069937
Episode 23950: Total Loss of tensor([[16.6786]], grad_fn=<SubBackward0>)
[2022-11-06 00:23:01.499062] Process 5. Episode 24100, average_reward -0.069668
Episode 24100: Total Loss of tensor([[-1.3316]], grad_fn=<SubBackward0>)
[2022-11-06 00:23:10.149342] Process 1. Episode 23450, average_reward -0.069211
Episode 23450: Total Loss of tensor([[-118.4848]], grad_fn=<SubBackward0>)
[2022-11-06 00:23:39.450896] Process 4. Episode 25450, average_reward -0.071631
Episode 25450: Total Loss of tensor([[-13.6531]], grad_fn=<SubBackward0>)
[2022-11-06 00:23:46.162000] Process 2. Episode 24400, average_reward -0.074385
Episode 24400: Total Loss of tensor([[-0.7926]], grad_fn=<SubBackward0>)
[2022-11-06 00:23:47.905615] Process 3. Episode 23850, average_reward -0.072495
Episode 23850: Total Loss of tensor([[9.2665]], grad_fn=<SubBackward0>)
[2022-11-06 00:24:29.073832] Process 0. Episode 24000, average_reward -0.069875
Episode 24000: Total Loss of tensor([[10.0029]], grad_fn=<SubBackward0>)
[2022-11-06 00:25:34.051055] Process 5. Episode 24150, average_reward -0.069731
Episode 24150: Total Loss of tensor([[7.9242]], grad_fn=<SubBackward0>)
[2022-11-06 00:25:35.590157] Process 1. Episode 23500, average_reward -0.069277
Episode 23500: Total Loss of tensor([[-23.9436]], grad_fn=<SubBackward0>)
[2022-11-06 00:25:58.802386] Process 4. Episode 25500, average_reward -0.071529
Episode 25500: Total Loss of tensor([[3.5221]], grad_fn=<SubBackward0>)
[2022-11-06 00:26:09.470708] Process 2. Episode 24450, average_reward -0.074274
Episode 24450: Total Loss of tensor([[-9.9495]], grad_fn=<SubBackward0>)
[2022-11-06 00:26:17.542809] Process 3. Episode 23900, average_reward -0.072510
Episode 23900: Total Loss of tensor([[-49.7566]], grad_fn=<SubBackward0>)
[2022-11-06 00:27:08.351256] Process 0. Episode 24050, average_reward -0.069771
Episode 24050: Total Loss of tensor([[7.2060]], grad_fn=<SubBackward0>)
[2022-11-06 00:28:00.821292] Process 5. Episode 24200, average_reward -0.069711
Episode 24200: Total Loss of tensor([[12.0201]], grad_fn=<SubBackward0>)
[2022-11-06 00:28:09.341549] Process 1. Episode 23550, average_reward -0.069299
Episode 23550: Total Loss of tensor([[0.7194]], grad_fn=<SubBackward0>)
[2022-11-06 00:28:21.211557] Process 4. Episode 25550, average_reward -0.071507
Episode 25550: Total Loss of tensor([[16.0847]], grad_fn=<SubBackward0>)
[2022-11-06 00:28:35.084986] Process 2. Episode 24500, average_reward -0.074367
Episode 24500: Total Loss of tensor([[-6.9374]], grad_fn=<SubBackward0>)
[2022-11-06 00:28:45.477749] Process 3. Episode 23950, average_reward -0.072526
Episode 23950: Total Loss of tensor([[13.8334]], grad_fn=<SubBackward0>)
[2022-11-06 00:29:35.070374] Process 0. Episode 24100, average_reward -0.069793
Episode 24100: Total Loss of tensor([[12.6174]], grad_fn=<SubBackward0>)
[2022-11-06 00:30:37.023477] Process 1. Episode 23600, average_reward -0.069195
Episode 23600: Total Loss of tensor([[3.1152]], grad_fn=<SubBackward0>)
[2022-11-06 00:30:37.452126] Process 5. Episode 24250, average_reward -0.069773
Episode 24250: Total Loss of tensor([[4.0891]], grad_fn=<SubBackward0>)
[2022-11-06 00:30:41.008681] Process 4. Episode 25600, average_reward -0.071445
Episode 25600: Total Loss of tensor([[-118.7336]], grad_fn=<SubBackward0>)
[2022-11-06 00:31:09.706336] Process 2. Episode 24550, average_reward -0.074338
Episode 24550: Total Loss of tensor([[2.1062]], grad_fn=<SubBackward0>)
[2022-11-06 00:31:14.062165] Process 3. Episode 24000, average_reward -0.072542
Episode 24000: Total Loss of tensor([[15.2065]], grad_fn=<SubBackward0>)
[2022-11-06 00:31:56.912476] Process 0. Episode 24150, average_reward -0.069814
Episode 24150: Total Loss of tensor([[-28.5982]], grad_fn=<SubBackward0>)
[2022-11-06 00:33:02.215338] Process 4. Episode 25650, average_reward -0.071618
Episode 25650: Total Loss of tensor([[-114.2533]], grad_fn=<SubBackward0>)
[2022-11-06 00:33:03.875610] Process 1. Episode 23650, average_reward -0.069133
Episode 23650: Total Loss of tensor([[12.5979]], grad_fn=<SubBackward0>)
[2022-11-06 00:33:09.030620] Process 5. Episode 24300, average_reward -0.069877
Episode 24300: Total Loss of tensor([[-27.2453]], grad_fn=<SubBackward0>)
[2022-11-06 00:33:35.982083] Process 2. Episode 24600, average_reward -0.074228
Episode 24600: Total Loss of tensor([[4.2096]], grad_fn=<SubBackward0>)
[2022-11-06 00:33:47.622905] Process 3. Episode 24050, average_reward -0.072516
Episode 24050: Total Loss of tensor([[5.8063]], grad_fn=<SubBackward0>)
[2022-11-06 00:34:21.958774] Process 0. Episode 24200, average_reward -0.069835
Episode 24200: Total Loss of tensor([[1.9995]], grad_fn=<SubBackward0>)
[2022-11-06 00:35:23.550962] Process 4. Episode 25700, average_reward -0.071479
Episode 25700: Total Loss of tensor([[-1.0965]], grad_fn=<SubBackward0>)
[2022-11-06 00:35:30.578906] Process 1. Episode 23700, average_reward -0.068987
Episode 23700: Total Loss of tensor([[6.3675]], grad_fn=<SubBackward0>)
[2022-11-06 00:35:49.690310] Process 5. Episode 24350, average_reward -0.069815
Episode 24350: Total Loss of tensor([[8.9006]], grad_fn=<SubBackward0>)
[2022-11-06 00:36:01.882203] Process 2. Episode 24650, average_reward -0.074158
Episode 24650: Total Loss of tensor([[-11.6330]], grad_fn=<SubBackward0>)
[2022-11-06 00:36:16.902071] Process 3. Episode 24100, average_reward -0.072490
Episode 24100: Total Loss of tensor([[16.7101]], grad_fn=<SubBackward0>)
[2022-11-06 00:36:47.619387] Process 0. Episode 24250, average_reward -0.069773
Episode 24250: Total Loss of tensor([[8.7319]], grad_fn=<SubBackward0>)
[2022-11-06 00:37:44.901348] Process 4. Episode 25750, average_reward -0.071534
Episode 25750: Total Loss of tensor([[-3.3013]], grad_fn=<SubBackward0>)
[2022-11-06 00:38:04.494347] Process 1. Episode 23750, average_reward -0.068968
Episode 23750: Total Loss of tensor([[-5.6668]], grad_fn=<SubBackward0>)
[2022-11-06 00:38:26.900103] Process 2. Episode 24700, average_reward -0.074008
Episode 24700: Total Loss of tensor([[-2.7694]], grad_fn=<SubBackward0>)
[2022-11-06 00:38:30.026084] Process 5. Episode 24400, average_reward -0.069795
Episode 24400: Total Loss of tensor([[6.7999]], grad_fn=<SubBackward0>)
[2022-11-06 00:38:49.486172] Process 3. Episode 24150, average_reward -0.072464
Episode 24150: Total Loss of tensor([[-48.6465]], grad_fn=<SubBackward0>)
[2022-11-06 00:39:11.627898] Process 0. Episode 24300, average_reward -0.069877
Episode 24300: Total Loss of tensor([[0.7650]], grad_fn=<SubBackward0>)
[2022-11-06 00:40:02.643300] Process 4. Episode 25800, average_reward -0.071434
Episode 25800: Total Loss of tensor([[2.6679]], grad_fn=<SubBackward0>)
[2022-11-06 00:40:34.621149] Process 1. Episode 23800, average_reward -0.068908
Episode 23800: Total Loss of tensor([[-2.6158]], grad_fn=<SubBackward0>)
[2022-11-06 00:40:55.839685] Process 2. Episode 24750, average_reward -0.073939
Episode 24750: Total Loss of tensor([[-5.1187]], grad_fn=<SubBackward0>)
[2022-11-06 00:40:59.957004] Process 5. Episode 24450, average_reward -0.069775
Episode 24450: Total Loss of tensor([[2.2344]], grad_fn=<SubBackward0>)
[2022-11-06 00:41:26.722395] Process 3. Episode 24200, average_reward -0.072562
Episode 24200: Total Loss of tensor([[15.9182]], grad_fn=<SubBackward0>)
[2022-11-06 00:41:47.933539] Process 0. Episode 24350, average_reward -0.069938
Episode 24350: Total Loss of tensor([[-3.4186]], grad_fn=<SubBackward0>)
[2022-11-06 00:42:21.704385] Process 4. Episode 25850, average_reward -0.071412
Episode 25850: Total Loss of tensor([[4.0524]], grad_fn=<SubBackward0>)
[2022-11-06 00:43:03.130985] Process 1. Episode 23850, average_reward -0.068931
Episode 23850: Total Loss of tensor([[5.4797]], grad_fn=<SubBackward0>)
[2022-11-06 00:43:21.497203] Process 2. Episode 24800, average_reward -0.073871
Episode 24800: Total Loss of tensor([[5.0615]], grad_fn=<SubBackward0>)
[2022-11-06 00:43:31.561182] Process 5. Episode 24500, average_reward -0.069673
Episode 24500: Total Loss of tensor([[16.9952]], grad_fn=<SubBackward0>)
[2022-11-06 00:43:53.822681] Process 3. Episode 24250, average_reward -0.072577
Episode 24250: Total Loss of tensor([[1.0920]], grad_fn=<SubBackward0>)
[2022-11-06 00:44:25.633755] Process 0. Episode 24400, average_reward -0.069959
Episode 24400: Total Loss of tensor([[-35.7977]], grad_fn=<SubBackward0>)
[2022-11-06 00:44:41.413052] Process 4. Episode 25900, average_reward -0.071429
Episode 25900: Total Loss of tensor([[-2.0922]], grad_fn=<SubBackward0>)
[2022-11-06 00:45:24.907946] Process 1. Episode 23900, average_reward -0.068954
Episode 23900: Total Loss of tensor([[-13.2705]], grad_fn=<SubBackward0>)
[2022-11-06 00:45:45.462855] Process 2. Episode 24850, average_reward -0.073924
Episode 24850: Total Loss of tensor([[13.5595]], grad_fn=<SubBackward0>)
[2022-11-06 00:45:58.662057] Process 5. Episode 24550, average_reward -0.069572
Episode 24550: Total Loss of tensor([[10.9981]], grad_fn=<SubBackward0>)
[2022-11-06 00:46:35.447748] Process 3. Episode 24300, average_reward -0.072510
Episode 24300: Total Loss of tensor([[6.4178]], grad_fn=<SubBackward0>)
[2022-11-06 00:46:58.409222] Process 0. Episode 24450, average_reward -0.069857
Episode 24450: Total Loss of tensor([[18.0092]], grad_fn=<SubBackward0>)
[2022-11-06 00:46:59.501589] Process 4. Episode 25950, average_reward -0.071445
Episode 25950: Total Loss of tensor([[9.3548]], grad_fn=<SubBackward0>)
[2022-11-06 00:47:46.348833] Process 1. Episode 23950, average_reward -0.068894
Episode 23950: Total Loss of tensor([[3.8820]], grad_fn=<SubBackward0>)
[2022-11-06 00:48:13.534710] Process 2. Episode 24900, average_reward -0.073936
Episode 24900: Total Loss of tensor([[-1.0468]], grad_fn=<SubBackward0>)
[2022-11-06 00:48:25.894516] Process 5. Episode 24600, average_reward -0.069634
Episode 24600: Total Loss of tensor([[97.0757]], grad_fn=<SubBackward0>)
[2022-11-06 00:49:14.738697] Process 3. Episode 24350, average_reward -0.072444
Episode 24350: Total Loss of tensor([[9.4936]], grad_fn=<SubBackward0>)
[2022-11-06 00:49:15.680429] Process 4. Episode 26000, average_reward -0.071385
Episode 26000: Total Loss of tensor([[-1.8078]], grad_fn=<SubBackward0>)
[2022-11-06 00:49:34.020318] Process 0. Episode 24500, average_reward -0.069878
Episode 24500: Total Loss of tensor([[1.1592]], grad_fn=<SubBackward0>)
[2022-11-06 00:50:19.568688] Process 1. Episode 24000, average_reward -0.068833
Episode 24000: Total Loss of tensor([[8.9997]], grad_fn=<SubBackward0>)
[2022-11-06 00:50:36.687503] Process 2. Episode 24950, average_reward -0.073908
Episode 24950: Total Loss of tensor([[8.1400]], grad_fn=<SubBackward0>)
[2022-11-06 00:50:56.642151] Process 5. Episode 24650, average_reward -0.069777
Episode 24650: Total Loss of tensor([[-2.9259]], grad_fn=<SubBackward0>)
[2022-11-06 00:51:41.960354] Process 4. Episode 26050, average_reward -0.071401
Episode 26050: Total Loss of tensor([[19.6113]], grad_fn=<SubBackward0>)
[2022-11-06 00:51:42.758457] Process 3. Episode 24400, average_reward -0.072459
Episode 24400: Total Loss of tensor([[9.6579]], grad_fn=<SubBackward0>)
[2022-11-06 00:52:05.922193] Process 0. Episode 24550, average_reward -0.069898
Episode 24550: Total Loss of tensor([[39.7739]], grad_fn=<SubBackward0>)
[2022-11-06 00:52:46.380768] Process 1. Episode 24050, average_reward -0.068815
Episode 24050: Total Loss of tensor([[0.2604]], grad_fn=<SubBackward0>)
[2022-11-06 00:53:01.230146] Process 2. Episode 25000, average_reward -0.073920
Episode 25000: Total Loss of tensor([[7.7203]], grad_fn=<SubBackward0>)
[2022-11-06 00:53:22.802322] Process 5. Episode 24700, average_reward -0.069919
Episode 24700: Total Loss of tensor([[151.6343]], grad_fn=<SubBackward0>)
[2022-11-06 00:54:04.969935] Process 4. Episode 26100, average_reward -0.071494
Episode 26100: Total Loss of tensor([[-11.0430]], grad_fn=<SubBackward0>)
[2022-11-06 00:54:15.805457] Process 3. Episode 24450, average_reward -0.072474
Episode 24450: Total Loss of tensor([[12.0418]], grad_fn=<SubBackward0>)
[2022-11-06 00:54:37.753324] Process 0. Episode 24600, average_reward -0.069919
Episode 24600: Total Loss of tensor([[5.3485]], grad_fn=<SubBackward0>)
[2022-11-06 00:55:18.216384] Process 1. Episode 24100, average_reward -0.068797
Episode 24100: Total Loss of tensor([[6.7562]], grad_fn=<SubBackward0>)
[2022-11-06 00:55:22.582393] Process 2. Episode 25050, average_reward -0.074012
Episode 25050: Total Loss of tensor([[-7.2729]], grad_fn=<SubBackward0>)
[2022-11-06 00:55:45.017834] Process 5. Episode 24750, average_reward -0.069899
Episode 24750: Total Loss of tensor([[19.2986]], grad_fn=<SubBackward0>)
[2022-11-06 00:56:25.529465] Process 4. Episode 26150, average_reward -0.071511
Episode 26150: Total Loss of tensor([[-4.2317]], grad_fn=<SubBackward0>)
[2022-11-06 00:56:57.828952] Process 3. Episode 24500, average_reward -0.072490
Episode 24500: Total Loss of tensor([[17.2928]], grad_fn=<SubBackward0>)
[2022-11-06 00:57:08.801561] Process 0. Episode 24650, average_reward -0.069899
Episode 24650: Total Loss of tensor([[-0.5852]], grad_fn=<SubBackward0>)
[2022-11-06 00:57:43.575661] Process 1. Episode 24150, average_reward -0.068820
Episode 24150: Total Loss of tensor([[6.2415]], grad_fn=<SubBackward0>)
[2022-11-06 00:57:43.899610] Process 2. Episode 25100, average_reward -0.074064
Episode 25100: Total Loss of tensor([[6.9478]], grad_fn=<SubBackward0>)
[2022-11-06 00:58:08.853463] Process 5. Episode 24800, average_reward -0.069960
Episode 24800: Total Loss of tensor([[11.0389]], grad_fn=<SubBackward0>)
[2022-11-06 00:58:46.407685] Process 4. Episode 26200, average_reward -0.071527
Episode 26200: Total Loss of tensor([[5.0256]], grad_fn=<SubBackward0>)
[2022-11-06 00:59:31.479176] Process 3. Episode 24550, average_reward -0.072424
Episode 24550: Total Loss of tensor([[-0.7412]], grad_fn=<SubBackward0>)
[2022-11-06 00:59:48.982936] Process 0. Episode 24700, average_reward -0.069960
Episode 24700: Total Loss of tensor([[-36.1998]], grad_fn=<SubBackward0>)
[2022-11-06 01:00:02.561011] Process 2. Episode 25150, average_reward -0.074155
Episode 25150: Total Loss of tensor([[-3.8446]], grad_fn=<SubBackward0>)
[2022-11-06 01:00:11.481611] Process 1. Episode 24200, average_reward -0.068719
Episode 24200: Total Loss of tensor([[5.1182]], grad_fn=<SubBackward0>)
[2022-11-06 01:00:45.176796] Process 5. Episode 24850, average_reward -0.070141
Episode 24850: Total Loss of tensor([[14.4424]], grad_fn=<SubBackward0>)
[2022-11-06 01:01:04.009617] Process 4. Episode 26250, average_reward -0.071581
Episode 26250: Total Loss of tensor([[18.7494]], grad_fn=<SubBackward0>)
[2022-11-06 01:02:01.716699] Process 3. Episode 24600, average_reward -0.072439
Episode 24600: Total Loss of tensor([[8.2871]], grad_fn=<SubBackward0>)
[2022-11-06 01:02:21.393435] Process 2. Episode 25200, average_reward -0.074167
Episode 25200: Total Loss of tensor([[12.0241]], grad_fn=<SubBackward0>)
[2022-11-06 01:02:31.858089] Process 0. Episode 24750, average_reward -0.069939
Episode 24750: Total Loss of tensor([[5.9129]], grad_fn=<SubBackward0>)
[2022-11-06 01:02:42.703765] Process 1. Episode 24250, average_reward -0.068742
Episode 24250: Total Loss of tensor([[-91.3530]], grad_fn=<SubBackward0>)
[2022-11-06 01:03:16.158625] Process 5. Episode 24900, average_reward -0.070120
Episode 24900: Total Loss of tensor([[3.5270]], grad_fn=<SubBackward0>)
[2022-11-06 01:03:23.576064] Process 4. Episode 26300, average_reward -0.071597
Episode 26300: Total Loss of tensor([[2.4817]], grad_fn=<SubBackward0>)
[2022-11-06 01:04:31.303191] Process 3. Episode 24650, average_reward -0.072454
Episode 24650: Total Loss of tensor([[13.8466]], grad_fn=<SubBackward0>)
[2022-11-06 01:04:48.155394] Process 2. Episode 25250, average_reward -0.074099
Episode 25250: Total Loss of tensor([[-1.4810]], grad_fn=<SubBackward0>)
[2022-11-06 01:05:07.721112] Process 0. Episode 24800, average_reward -0.069919
Episode 24800: Total Loss of tensor([[3.5651]], grad_fn=<SubBackward0>)
[2022-11-06 01:05:27.484915] Process 1. Episode 24300, average_reward -0.068683
Episode 24300: Total Loss of tensor([[21.2368]], grad_fn=<SubBackward0>)
[2022-11-06 01:05:39.461957] Process 5. Episode 24950, average_reward -0.070100
Episode 24950: Total Loss of tensor([[7.8751]], grad_fn=<SubBackward0>)
[2022-11-06 01:05:43.404913] Process 4. Episode 26350, average_reward -0.071537
Episode 26350: Total Loss of tensor([[2.0981]], grad_fn=<SubBackward0>)
[2022-11-06 01:06:53.058054] Process 3. Episode 24700, average_reward -0.072551
Episode 24700: Total Loss of tensor([[-3.4874]], grad_fn=<SubBackward0>)
[2022-11-06 01:07:18.167638] Process 2. Episode 25300, average_reward -0.074071
Episode 25300: Total Loss of tensor([[8.3360]], grad_fn=<SubBackward0>)
[2022-11-06 01:07:40.451625] Process 0. Episode 24850, average_reward -0.069940
Episode 24850: Total Loss of tensor([[11.3840]], grad_fn=<SubBackward0>)
[2022-11-06 01:07:59.705821] Process 1. Episode 24350, average_reward -0.068542
Episode 24350: Total Loss of tensor([[15.5583]], grad_fn=<SubBackward0>)
[2022-11-06 01:08:05.152420] Process 4. Episode 26400, average_reward -0.071553
Episode 26400: Total Loss of tensor([[7.1651]], grad_fn=<SubBackward0>)
[2022-11-06 01:08:05.642472] Process 5. Episode 25000, average_reward -0.070040
Episode 25000: Total Loss of tensor([[2.8759]], grad_fn=<SubBackward0>)
[2022-11-06 01:09:19.352093] Process 3. Episode 24750, average_reward -0.072485
Episode 24750: Total Loss of tensor([[7.6188]], grad_fn=<SubBackward0>)
[2022-11-06 01:09:42.319946] Process 2. Episode 25350, average_reward -0.074201
Episode 25350: Total Loss of tensor([[-33.9428]], grad_fn=<SubBackward0>)
[2022-11-06 01:10:16.998418] Process 0. Episode 24900, average_reward -0.069920
Episode 24900: Total Loss of tensor([[9.5260]], grad_fn=<SubBackward0>)
[2022-11-06 01:10:31.394789] Process 4. Episode 26450, average_reward -0.071531
Episode 26450: Total Loss of tensor([[0.5817]], grad_fn=<SubBackward0>)
[2022-11-06 01:10:38.608502] Process 1. Episode 24400, average_reward -0.068566
Episode 24400: Total Loss of tensor([[-22.9764]], grad_fn=<SubBackward0>)
[2022-11-06 01:10:42.714626] Process 5. Episode 25050, average_reward -0.069940
Episode 25050: Total Loss of tensor([[1.2127]], grad_fn=<SubBackward0>)
[2022-11-06 01:11:40.592103] Process 3. Episode 24800, average_reward -0.072500
Episode 24800: Total Loss of tensor([[-14.5296]], grad_fn=<SubBackward0>)
[2022-11-06 01:12:05.108405] Process 2. Episode 25400, average_reward -0.074252
Episode 25400: Total Loss of tensor([[-34.9772]], grad_fn=<SubBackward0>)
[2022-11-06 01:12:49.401070] Process 0. Episode 24950, average_reward -0.069900
Episode 24950: Total Loss of tensor([[11.5348]], grad_fn=<SubBackward0>)
[2022-11-06 01:12:54.004836] Process 4. Episode 26500, average_reward -0.071472
Episode 26500: Total Loss of tensor([[10.9865]], grad_fn=<SubBackward0>)
[2022-11-06 01:13:11.477698] Process 5. Episode 25100, average_reward -0.069880
Episode 25100: Total Loss of tensor([[1.3092]], grad_fn=<SubBackward0>)
[2022-11-06 01:13:21.333491] Process 1. Episode 24450, average_reward -0.068466
Episode 24450: Total Loss of tensor([[-0.1611]], grad_fn=<SubBackward0>)
[2022-11-06 01:14:06.748969] Process 3. Episode 24850, average_reward -0.072475
Episode 24850: Total Loss of tensor([[10.2152]], grad_fn=<SubBackward0>)
[2022-11-06 01:14:26.994884] Process 2. Episode 25450, average_reward -0.074224
Episode 25450: Total Loss of tensor([[1.4124]], grad_fn=<SubBackward0>)
[2022-11-06 01:15:16.098404] Process 4. Episode 26550, average_reward -0.071450
Episode 26550: Total Loss of tensor([[-24.2075]], grad_fn=<SubBackward0>)
[2022-11-06 01:15:19.462466] Process 0. Episode 25000, average_reward -0.069800
Episode 25000: Total Loss of tensor([[6.9139]], grad_fn=<SubBackward0>)
[2022-11-06 01:15:36.882753] Process 5. Episode 25150, average_reward -0.069980
Episode 25150: Total Loss of tensor([[12.0695]], grad_fn=<SubBackward0>)
[2022-11-06 01:16:02.025767] Process 1. Episode 24500, average_reward -0.068449
Episode 24500: Total Loss of tensor([[-0.0925]], grad_fn=<SubBackward0>)
[2022-11-06 01:16:41.281085] Process 3. Episode 24900, average_reward -0.072490
Episode 24900: Total Loss of tensor([[6.5857]], grad_fn=<SubBackward0>)
[2022-11-06 01:16:46.454581] Process 2. Episode 25500, average_reward -0.074196
Episode 25500: Total Loss of tensor([[-3.2143]], grad_fn=<SubBackward0>)
[2022-11-06 01:17:35.939382] Process 4. Episode 26600, average_reward -0.071391
Episode 26600: Total Loss of tensor([[10.8603]], grad_fn=<SubBackward0>)
[2022-11-06 01:18:02.816052] Process 0. Episode 25050, average_reward -0.069741
Episode 25050: Total Loss of tensor([[-5.7247]], grad_fn=<SubBackward0>)
[2022-11-06 01:18:03.445741] Process 5. Episode 25200, average_reward -0.070000
Episode 25200: Total Loss of tensor([[8.6084]], grad_fn=<SubBackward0>)
[2022-11-06 01:18:29.608941] Process 1. Episode 24550, average_reward -0.068432
Episode 24550: Total Loss of tensor([[1.9281]], grad_fn=<SubBackward0>)
[2022-11-06 01:19:16.770141] Process 3. Episode 24950, average_reward -0.072665
Episode 24950: Total Loss of tensor([[3.5223]], grad_fn=<SubBackward0>)
[2022-11-06 01:19:16.781853] Process 2. Episode 25550, average_reward -0.074129
Episode 25550: Total Loss of tensor([[0.3192]], grad_fn=<SubBackward0>)
[2022-11-06 01:19:57.642916] Process 4. Episode 26650, average_reward -0.071332
Episode 26650: Total Loss of tensor([[4.5830]], grad_fn=<SubBackward0>)
[2022-11-06 01:20:34.594581] Process 5. Episode 25250, average_reward -0.069941
Episode 25250: Total Loss of tensor([[7.5479]], grad_fn=<SubBackward0>)
[2022-11-06 01:20:36.454058] Process 0. Episode 25100, average_reward -0.069602
Episode 25100: Total Loss of tensor([[5.9452]], grad_fn=<SubBackward0>)
[2022-11-06 01:20:56.969711] Process 1. Episode 24600, average_reward -0.068496
Episode 24600: Total Loss of tensor([[-82.3032]], grad_fn=<SubBackward0>)
[2022-11-06 01:21:38.134112] Process 2. Episode 25600, average_reward -0.074102
Episode 25600: Total Loss of tensor([[11.9280]], grad_fn=<SubBackward0>)
[2022-11-06 01:21:53.805468] Process 3. Episode 25000, average_reward -0.072600
Episode 25000: Total Loss of tensor([[16.0646]], grad_fn=<SubBackward0>)
[2022-11-06 01:22:21.574900] Process 4. Episode 26700, average_reward -0.071386
Episode 26700: Total Loss of tensor([[1.0870]], grad_fn=<SubBackward0>)
[2022-11-06 01:23:05.195005] Process 5. Episode 25300, average_reward -0.069881
Episode 25300: Total Loss of tensor([[12.2187]], grad_fn=<SubBackward0>)
[2022-11-06 01:23:05.699926] Process 0. Episode 25150, average_reward -0.069583
Episode 25150: Total Loss of tensor([[4.3496]], grad_fn=<SubBackward0>)
[2022-11-06 01:23:29.931907] Process 1. Episode 24650, average_reward -0.068479
Episode 24650: Total Loss of tensor([[2.3701]], grad_fn=<SubBackward0>)
[2022-11-06 01:23:56.485080] Process 2. Episode 25650, average_reward -0.074191
Episode 25650: Total Loss of tensor([[7.0702]], grad_fn=<SubBackward0>)
[2022-11-06 01:24:20.009663] Process 3. Episode 25050, average_reward -0.072695
Episode 25050: Total Loss of tensor([[12.4312]], grad_fn=<SubBackward0>)
[2022-11-06 01:24:43.755073] Process 4. Episode 26750, average_reward -0.071364
Episode 26750: Total Loss of tensor([[1.5888]], grad_fn=<SubBackward0>)
[2022-11-06 01:25:32.217161] Process 0. Episode 25200, average_reward -0.069603
Episode 25200: Total Loss of tensor([[-8.4924]], grad_fn=<SubBackward0>)
[2022-11-06 01:25:35.931998] Process 5. Episode 25350, average_reward -0.069783
Episode 25350: Total Loss of tensor([[0.3881]], grad_fn=<SubBackward0>)
[2022-11-06 01:26:08.682713] Process 1. Episode 24700, average_reward -0.068340
Episode 24700: Total Loss of tensor([[14.0669]], grad_fn=<SubBackward0>)
[2022-11-06 01:26:17.527783] Process 2. Episode 25700, average_reward -0.074163
Episode 25700: Total Loss of tensor([[4.2047]], grad_fn=<SubBackward0>)
[2022-11-06 01:26:51.029241] Process 3. Episode 25100, average_reward -0.072629
Episode 25100: Total Loss of tensor([[13.5360]], grad_fn=<SubBackward0>)
[2022-11-06 01:27:06.036153] Process 4. Episode 26800, average_reward -0.071306
Episode 26800: Total Loss of tensor([[-4.6504]], grad_fn=<SubBackward0>)
[2022-11-06 01:27:59.249425] Process 0. Episode 25250, average_reward -0.069663
Episode 25250: Total Loss of tensor([[13.4616]], grad_fn=<SubBackward0>)
[2022-11-06 01:28:03.341085] Process 5. Episode 25400, average_reward -0.069882
Episode 25400: Total Loss of tensor([[7.4162]], grad_fn=<SubBackward0>)
[2022-11-06 01:28:39.579078] Process 1. Episode 24750, average_reward -0.068364
Episode 24750: Total Loss of tensor([[-118.9769]], grad_fn=<SubBackward0>)
[2022-11-06 01:28:47.015198] Process 2. Episode 25750, average_reward -0.074136
Episode 25750: Total Loss of tensor([[7.8074]], grad_fn=<SubBackward0>)
[2022-11-06 01:29:18.037892] Process 3. Episode 25150, average_reward -0.072565
Episode 25150: Total Loss of tensor([[11.8512]], grad_fn=<SubBackward0>)
[2022-11-06 01:29:26.397869] Process 4. Episode 26850, average_reward -0.071285
Episode 26850: Total Loss of tensor([[9.7637]], grad_fn=<SubBackward0>)
[2022-11-06 01:30:23.751620] Process 0. Episode 25300, average_reward -0.069605
Episode 25300: Total Loss of tensor([[13.0564]], grad_fn=<SubBackward0>)
[2022-11-06 01:30:36.730808] Process 5. Episode 25450, average_reward -0.069941
Episode 25450: Total Loss of tensor([[4.5670]], grad_fn=<SubBackward0>)
[2022-11-06 01:31:05.191621] Process 1. Episode 24800, average_reward -0.068387
Episode 24800: Total Loss of tensor([[2.1452]], grad_fn=<SubBackward0>)
[2022-11-06 01:31:11.536273] Process 2. Episode 25800, average_reward -0.074147
Episode 25800: Total Loss of tensor([[6.1334]], grad_fn=<SubBackward0>)
[2022-11-06 01:31:48.734490] Process 4. Episode 26900, average_reward -0.071301
Episode 26900: Total Loss of tensor([[8.8812]], grad_fn=<SubBackward0>)
[2022-11-06 01:31:52.421892] Process 3. Episode 25200, average_reward -0.072659
Episode 25200: Total Loss of tensor([[11.0682]], grad_fn=<SubBackward0>)
[2022-11-06 01:32:56.486268] Process 0. Episode 25350, average_reward -0.069625
Episode 25350: Total Loss of tensor([[-21.0466]], grad_fn=<SubBackward0>)
[2022-11-06 01:32:59.097500] Process 5. Episode 25500, average_reward -0.069922
Episode 25500: Total Loss of tensor([[11.7274]], grad_fn=<SubBackward0>)
[2022-11-06 01:33:36.996924] Process 2. Episode 25850, average_reward -0.074197
Episode 25850: Total Loss of tensor([[2.6230]], grad_fn=<SubBackward0>)
[2022-11-06 01:33:43.900908] Process 1. Episode 24850, average_reward -0.068531
Episode 24850: Total Loss of tensor([[8.1768]], grad_fn=<SubBackward0>)
[2022-11-06 01:34:12.061939] Process 4. Episode 26950, average_reward -0.071280
Episode 26950: Total Loss of tensor([[2.1593]], grad_fn=<SubBackward0>)
[2022-11-06 01:34:21.022081] Process 3. Episode 25250, average_reward -0.072554
Episode 25250: Total Loss of tensor([[1.0287]], grad_fn=<SubBackward0>)
[2022-11-06 01:35:26.009702] Process 5. Episode 25550, average_reward -0.069824
Episode 25550: Total Loss of tensor([[5.8834]], grad_fn=<SubBackward0>)
[2022-11-06 01:35:28.604328] Process 0. Episode 25400, average_reward -0.069528
Episode 25400: Total Loss of tensor([[13.3363]], grad_fn=<SubBackward0>)
[2022-11-06 01:36:01.536899] Process 2. Episode 25900, average_reward -0.074170
Episode 25900: Total Loss of tensor([[11.4180]], grad_fn=<SubBackward0>)
[2022-11-06 01:36:18.142636] Process 1. Episode 24900, average_reward -0.068434
Episode 24900: Total Loss of tensor([[5.1544]], grad_fn=<SubBackward0>)
[2022-11-06 01:36:31.917906] Process 4. Episode 27000, average_reward -0.071259
Episode 27000: Total Loss of tensor([[2.3430]], grad_fn=<SubBackward0>)
[2022-11-06 01:36:46.772103] Process 3. Episode 25300, average_reward -0.072451
Episode 25300: Total Loss of tensor([[12.1238]], grad_fn=<SubBackward0>)
[2022-11-06 01:37:50.746986] Process 5. Episode 25600, average_reward -0.069844
Episode 25600: Total Loss of tensor([[10.6034]], grad_fn=<SubBackward0>)
[2022-11-06 01:38:07.973205] Process 0. Episode 25450, average_reward -0.069548
Episode 25450: Total Loss of tensor([[14.5818]], grad_fn=<SubBackward0>)
[2022-11-06 01:38:22.496208] Process 2. Episode 25950, average_reward -0.074181
Episode 25950: Total Loss of tensor([[-17.6657]], grad_fn=<SubBackward0>)
[2022-11-06 01:38:50.942607] Process 4. Episode 27050, average_reward -0.071165
Episode 27050: Total Loss of tensor([[7.6629]], grad_fn=<SubBackward0>)
[2022-11-06 01:38:57.833601] Process 1. Episode 24950, average_reward -0.068497
Episode 24950: Total Loss of tensor([[5.0702]], grad_fn=<SubBackward0>)
[2022-11-06 01:39:10.390133] Process 3. Episode 25350, average_reward -0.072584
Episode 25350: Total Loss of tensor([[23.7116]], grad_fn=<SubBackward0>)
[2022-11-06 01:40:29.654847] Process 5. Episode 25650, average_reward -0.069825
Episode 25650: Total Loss of tensor([[4.8162]], grad_fn=<SubBackward0>)
[2022-11-06 01:40:34.035990] Process 0. Episode 25500, average_reward -0.069529
Episode 25500: Total Loss of tensor([[3.0624]], grad_fn=<SubBackward0>)
[2022-11-06 01:40:49.074019] Process 2. Episode 26000, average_reward -0.074115
Episode 26000: Total Loss of tensor([[1.7092]], grad_fn=<SubBackward0>)
[2022-11-06 01:41:12.130611] Process 4. Episode 27100, average_reward -0.071218
Episode 27100: Total Loss of tensor([[10.3202]], grad_fn=<SubBackward0>)
[2022-11-06 01:41:34.245565] Process 3. Episode 25400, average_reward -0.072717
Episode 25400: Total Loss of tensor([[12.1889]], grad_fn=<SubBackward0>)
[2022-11-06 01:41:38.017228] Process 1. Episode 25000, average_reward -0.068480
Episode 25000: Total Loss of tensor([[3.9430]], grad_fn=<SubBackward0>)
[2022-11-06 01:42:56.224942] Process 5. Episode 25700, average_reward -0.069728
Episode 25700: Total Loss of tensor([[12.2521]], grad_fn=<SubBackward0>)
[2022-11-06 01:43:06.172715] Process 0. Episode 25550, average_reward -0.069628
Episode 25550: Total Loss of tensor([[5.6778]], grad_fn=<SubBackward0>)
[2022-11-06 01:43:13.507543] Process 2. Episode 26050, average_reward -0.074127
Episode 26050: Total Loss of tensor([[5.5107]], grad_fn=<SubBackward0>)
[2022-11-06 01:43:34.594184] Process 4. Episode 27150, average_reward -0.071271
Episode 27150: Total Loss of tensor([[8.2392]], grad_fn=<SubBackward0>)
[2022-11-06 01:43:57.909041] Process 3. Episode 25450, average_reward -0.072652
Episode 25450: Total Loss of tensor([[-1.3805]], grad_fn=<SubBackward0>)
[2022-11-06 01:44:04.497547] Process 1. Episode 25050, average_reward -0.068503
Episode 25050: Total Loss of tensor([[8.2821]], grad_fn=<SubBackward0>)
[2022-11-06 01:45:21.692759] Process 5. Episode 25750, average_reward -0.069748
Episode 25750: Total Loss of tensor([[22.2668]], grad_fn=<SubBackward0>)
[2022-11-06 01:45:34.114984] Process 0. Episode 25600, average_reward -0.069766
Episode 25600: Total Loss of tensor([[13.0766]], grad_fn=<SubBackward0>)
[2022-11-06 01:45:39.667458] Process 2. Episode 26100, average_reward -0.074023
Episode 26100: Total Loss of tensor([[3.2946]], grad_fn=<SubBackward0>)
[2022-11-06 01:45:55.532979] Process 4. Episode 27200, average_reward -0.071287
Episode 27200: Total Loss of tensor([[-21.5189]], grad_fn=<SubBackward0>)
[2022-11-06 01:46:24.140556] Process 3. Episode 25500, average_reward -0.072627
Episode 25500: Total Loss of tensor([[8.6964]], grad_fn=<SubBackward0>)
[2022-11-06 01:46:46.099223] Process 1. Episode 25100, average_reward -0.068486
Episode 25100: Total Loss of tensor([[26.7703]], grad_fn=<SubBackward0>)
[2022-11-06 01:47:51.548326] Process 5. Episode 25800, average_reward -0.069845
Episode 25800: Total Loss of tensor([[17.1128]], grad_fn=<SubBackward0>)
[2022-11-06 01:47:57.153548] Process 2. Episode 26150, average_reward -0.074149
Episode 26150: Total Loss of tensor([[3.4737]], grad_fn=<SubBackward0>)
[2022-11-06 01:47:58.936192] Process 0. Episode 25650, average_reward -0.069825
Episode 25650: Total Loss of tensor([[17.9277]], grad_fn=<SubBackward0>)
[2022-11-06 01:48:13.687532] Process 4. Episode 27250, average_reward -0.071266
Episode 27250: Total Loss of tensor([[12.0602]], grad_fn=<SubBackward0>)
[2022-11-06 01:48:49.779335] Process 3. Episode 25550, average_reward -0.072642
Episode 25550: Total Loss of tensor([[-5.3673]], grad_fn=<SubBackward0>)
[2022-11-06 01:49:23.602040] Process 1. Episode 25150, average_reward -0.068429
Episode 25150: Total Loss of tensor([[-113.0347]], grad_fn=<SubBackward0>)
[2022-11-06 01:50:14.836661] Process 5. Episode 25850, average_reward -0.069787
Episode 25850: Total Loss of tensor([[12.5977]], grad_fn=<SubBackward0>)
[2022-11-06 01:50:21.928861] Process 2. Episode 26200, average_reward -0.074237
Episode 26200: Total Loss of tensor([[1.3787]], grad_fn=<SubBackward0>)
[2022-11-06 01:50:27.080010] Process 0. Episode 25700, average_reward -0.069805
Episode 25700: Total Loss of tensor([[-101.5240]], grad_fn=<SubBackward0>)
[2022-11-06 01:50:34.402013] Process 4. Episode 27300, average_reward -0.071245
Episode 27300: Total Loss of tensor([[-86.6883]], grad_fn=<SubBackward0>)
[2022-11-06 01:51:21.663304] Process 3. Episode 25600, average_reward -0.072617
Episode 25600: Total Loss of tensor([[17.5536]], grad_fn=<SubBackward0>)
[2022-11-06 01:51:50.218925] Process 1. Episode 25200, average_reward -0.068333
Episode 25200: Total Loss of tensor([[12.4765]], grad_fn=<SubBackward0>)
[2022-11-06 01:52:42.920095] Process 5. Episode 25900, average_reward -0.069807
Episode 25900: Total Loss of tensor([[2.4136]], grad_fn=<SubBackward0>)
[2022-11-06 01:52:50.334872] Process 2. Episode 26250, average_reward -0.074286
Episode 26250: Total Loss of tensor([[6.7206]], grad_fn=<SubBackward0>)
[2022-11-06 01:52:55.106912] Process 4. Episode 27350, average_reward -0.071225
Episode 27350: Total Loss of tensor([[4.2930]], grad_fn=<SubBackward0>)
[2022-11-06 01:53:00.162646] Process 0. Episode 25750, average_reward -0.069825
Episode 25750: Total Loss of tensor([[7.7118]], grad_fn=<SubBackward0>)
[2022-11-06 01:53:50.384546] Process 3. Episode 25650, average_reward -0.072593
Episode 25650: Total Loss of tensor([[2.7121]], grad_fn=<SubBackward0>)
[2022-11-06 01:54:23.544565] Process 1. Episode 25250, average_reward -0.068277
Episode 25250: Total Loss of tensor([[1.1149]], grad_fn=<SubBackward0>)
[2022-11-06 01:55:07.675788] Process 5. Episode 25950, average_reward -0.069750
Episode 25950: Total Loss of tensor([[17.5210]], grad_fn=<SubBackward0>)
[2022-11-06 01:55:16.979548] Process 4. Episode 27400, average_reward -0.071277
Episode 27400: Total Loss of tensor([[-35.0746]], grad_fn=<SubBackward0>)
[2022-11-06 01:55:19.669011] Process 2. Episode 26300, average_reward -0.074259
Episode 26300: Total Loss of tensor([[7.3126]], grad_fn=<SubBackward0>)
[2022-11-06 01:55:24.552475] Process 0. Episode 25800, average_reward -0.069729
Episode 25800: Total Loss of tensor([[4.1359]], grad_fn=<SubBackward0>)
[2022-11-06 01:56:21.588697] Process 3. Episode 25700, average_reward -0.072568
Episode 25700: Total Loss of tensor([[0.1857]], grad_fn=<SubBackward0>)
[2022-11-06 01:56:45.345261] Process 1. Episode 25300, average_reward -0.068300
Episode 25300: Total Loss of tensor([[21.8357]], grad_fn=<SubBackward0>)
[2022-11-06 01:57:37.635540] Process 5. Episode 26000, average_reward -0.069769
Episode 26000: Total Loss of tensor([[7.9137]], grad_fn=<SubBackward0>)
[2022-11-06 01:57:45.377851] Process 4. Episode 27450, average_reward -0.071257
Episode 27450: Total Loss of tensor([[3.5107]], grad_fn=<SubBackward0>)
[2022-11-06 01:57:49.362856] Process 2. Episode 26350, average_reward -0.074307
Episode 26350: Total Loss of tensor([[3.0744]], grad_fn=<SubBackward0>)
[2022-11-06 01:57:58.171609] Process 0. Episode 25850, average_reward -0.069710
Episode 25850: Total Loss of tensor([[5.7680]], grad_fn=<SubBackward0>)
[2022-11-06 01:58:46.766523] Process 3. Episode 25750, average_reward -0.072505
Episode 25750: Total Loss of tensor([[-2.2655]], grad_fn=<SubBackward0>)
[2022-11-06 01:59:17.232693] Process 1. Episode 25350, average_reward -0.068245
Episode 25350: Total Loss of tensor([[0.6215]], grad_fn=<SubBackward0>)
[2022-11-06 02:00:05.279845] Process 4. Episode 27500, average_reward -0.071345
Episode 27500: Total Loss of tensor([[2.6097]], grad_fn=<SubBackward0>)
[2022-11-06 02:00:10.551083] Process 5. Episode 26050, average_reward -0.069789
Episode 26050: Total Loss of tensor([[6.7463]], grad_fn=<SubBackward0>)
[2022-11-06 02:00:13.283989] Process 2. Episode 26400, average_reward -0.074280
Episode 26400: Total Loss of tensor([[5.9826]], grad_fn=<SubBackward0>)
[2022-11-06 02:00:33.238038] Process 0. Episode 25900, average_reward -0.069691
Episode 25900: Total Loss of tensor([[8.0457]], grad_fn=<SubBackward0>)
[2022-11-06 02:01:23.013268] Process 3. Episode 25800, average_reward -0.072481
Episode 25800: Total Loss of tensor([[14.9023]], grad_fn=<SubBackward0>)
[2022-11-06 02:01:45.753010] Process 1. Episode 25400, average_reward -0.068307
Episode 25400: Total Loss of tensor([[-107.6466]], grad_fn=<SubBackward0>)
[2022-11-06 02:02:24.077492] Process 4. Episode 27550, average_reward -0.071397
Episode 27550: Total Loss of tensor([[1.7717]], grad_fn=<SubBackward0>)
[2022-11-06 02:02:38.819443] Process 5. Episode 26100, average_reward -0.069808
Episode 26100: Total Loss of tensor([[6.4113]], grad_fn=<SubBackward0>)
[2022-11-06 02:02:39.845875] Process 2. Episode 26450, average_reward -0.074253
Episode 26450: Total Loss of tensor([[14.8781]], grad_fn=<SubBackward0>)
[2022-11-06 02:03:00.929364] Process 0. Episode 25950, average_reward -0.069634
Episode 25950: Total Loss of tensor([[6.7394]], grad_fn=<SubBackward0>)
[2022-11-06 02:03:59.396806] Process 3. Episode 25850, average_reward -0.072495
Episode 25850: Total Loss of tensor([[4.4644]], grad_fn=<SubBackward0>)
[2022-11-06 02:04:21.823289] Process 1. Episode 25450, average_reward -0.068291
Episode 25450: Total Loss of tensor([[17.7265]], grad_fn=<SubBackward0>)
[2022-11-06 02:04:42.673963] Process 4. Episode 27600, average_reward -0.071486
Episode 27600: Total Loss of tensor([[15.3696]], grad_fn=<SubBackward0>)
[2022-11-06 02:04:59.760553] Process 2. Episode 26500, average_reward -0.074264
Episode 26500: Total Loss of tensor([[17.0003]], grad_fn=<SubBackward0>)
[2022-11-06 02:05:04.304441] Process 5. Episode 26150, average_reward -0.069904
Episode 26150: Total Loss of tensor([[11.7467]], grad_fn=<SubBackward0>)
[2022-11-06 02:05:43.614228] Process 0. Episode 26000, average_reward -0.069654
Episode 26000: Total Loss of tensor([[3.1012]], grad_fn=<SubBackward0>)
[2022-11-06 02:06:38.591700] Process 3. Episode 25900, average_reward -0.072548
Episode 25900: Total Loss of tensor([[14.3485]], grad_fn=<SubBackward0>)
[2022-11-06 02:06:56.055351] Process 1. Episode 25500, average_reward -0.068275
Episode 25500: Total Loss of tensor([[-57.5466]], grad_fn=<SubBackward0>)
[2022-11-06 02:07:03.153184] Process 4. Episode 27650, average_reward -0.071573
Episode 27650: Total Loss of tensor([[-1.4616]], grad_fn=<SubBackward0>)
[2022-11-06 02:07:20.566681] Process 2. Episode 26550, average_reward -0.074388
Episode 26550: Total Loss of tensor([[11.4488]], grad_fn=<SubBackward0>)
[2022-11-06 02:07:28.324083] Process 5. Episode 26200, average_reward -0.069924
Episode 26200: Total Loss of tensor([[-9.3296]], grad_fn=<SubBackward0>)
[2022-11-06 02:08:16.744806] Process 0. Episode 26050, average_reward -0.069712
Episode 26050: Total Loss of tensor([[5.0769]], grad_fn=<SubBackward0>)
[2022-11-06 02:09:05.182016] Process 3. Episode 25950, average_reward -0.072601
Episode 25950: Total Loss of tensor([[0.3250]], grad_fn=<SubBackward0>)
[2022-11-06 02:09:23.724051] Process 4. Episode 27700, average_reward -0.071552
Episode 27700: Total Loss of tensor([[10.3198]], grad_fn=<SubBackward0>)
[2022-11-06 02:09:34.005713] Process 1. Episode 25550, average_reward -0.068415
Episode 25550: Total Loss of tensor([[9.3143]], grad_fn=<SubBackward0>)
[2022-11-06 02:09:43.022810] Process 2. Episode 26600, average_reward -0.074248
Episode 26600: Total Loss of tensor([[7.1902]], grad_fn=<SubBackward0>)
[2022-11-06 02:10:01.385421] Process 5. Episode 26250, average_reward -0.069905
Episode 26250: Total Loss of tensor([[-2.4375]], grad_fn=<SubBackward0>)
[2022-11-06 02:10:49.599185] Process 0. Episode 26100, average_reward -0.069732
Episode 26100: Total Loss of tensor([[-2.2008]], grad_fn=<SubBackward0>)
[2022-11-06 02:11:37.604123] Process 3. Episode 26000, average_reward -0.072654
Episode 26000: Total Loss of tensor([[-0.0870]], grad_fn=<SubBackward0>)
[2022-11-06 02:11:40.936341] Process 4. Episode 27750, average_reward -0.071459
Episode 27750: Total Loss of tensor([[12.3994]], grad_fn=<SubBackward0>)
[2022-11-06 02:12:05.109823] Process 2. Episode 26650, average_reward -0.074371
Episode 26650: Total Loss of tensor([[-124.1380]], grad_fn=<SubBackward0>)
[2022-11-06 02:12:05.208178] Process 1. Episode 25600, average_reward -0.068437
Episode 25600: Total Loss of tensor([[-65.0901]], grad_fn=<SubBackward0>)
[2022-11-06 02:12:40.758426] Process 5. Episode 26300, average_reward -0.069886
Episode 26300: Total Loss of tensor([[-0.3492]], grad_fn=<SubBackward0>)
[2022-11-06 02:13:21.946656] Process 0. Episode 26150, average_reward -0.069751
Episode 26150: Total Loss of tensor([[6.0831]], grad_fn=<SubBackward0>)
[2022-11-06 02:14:00.337316] Process 4. Episode 27800, average_reward -0.071439
Episode 27800: Total Loss of tensor([[15.9254]], grad_fn=<SubBackward0>)
[2022-11-06 02:14:20.320349] Process 3. Episode 26050, average_reward -0.072706
Episode 26050: Total Loss of tensor([[13.2054]], grad_fn=<SubBackward0>)
[2022-11-06 02:14:30.454761] Process 2. Episode 26700, average_reward -0.074419
Episode 26700: Total Loss of tensor([[15.8080]], grad_fn=<SubBackward0>)
[2022-11-06 02:14:36.538025] Process 1. Episode 25650, average_reward -0.068343
Episode 25650: Total Loss of tensor([[18.3045]], grad_fn=<SubBackward0>)
[2022-11-06 02:15:09.415824] Process 5. Episode 26350, average_reward -0.069867
Episode 26350: Total Loss of tensor([[-132.1554]], grad_fn=<SubBackward0>)
[2022-11-06 02:16:00.510785] Process 0. Episode 26200, average_reward -0.069771
Episode 26200: Total Loss of tensor([[-103.4713]], grad_fn=<SubBackward0>)
[2022-11-06 02:16:21.826646] Process 4. Episode 27850, average_reward -0.071490
Episode 27850: Total Loss of tensor([[10.3162]], grad_fn=<SubBackward0>)
[2022-11-06 02:16:50.196824] Process 2. Episode 26750, average_reward -0.074467
Episode 26750: Total Loss of tensor([[7.6946]], grad_fn=<SubBackward0>)
[2022-11-06 02:17:05.088519] Process 3. Episode 26100, average_reward -0.072644
Episode 26100: Total Loss of tensor([[6.0492]], grad_fn=<SubBackward0>)
[2022-11-06 02:17:14.901330] Process 1. Episode 25700, average_reward -0.068288
Episode 25700: Total Loss of tensor([[1.6403]], grad_fn=<SubBackward0>)
[2022-11-06 02:17:35.036535] Process 5. Episode 26400, average_reward -0.069886
Episode 26400: Total Loss of tensor([[0.9812]], grad_fn=<SubBackward0>)
[2022-11-06 02:18:41.426525] Process 4. Episode 27900, average_reward -0.071505
Episode 27900: Total Loss of tensor([[8.9988]], grad_fn=<SubBackward0>)
[2022-11-06 02:18:42.864681] Process 0. Episode 26250, average_reward -0.069867
Episode 26250: Total Loss of tensor([[7.3431]], grad_fn=<SubBackward0>)
[2022-11-06 02:19:17.487789] Process 2. Episode 26800, average_reward -0.074440
Episode 26800: Total Loss of tensor([[-1.8983]], grad_fn=<SubBackward0>)
[2022-11-06 02:19:36.686388] Process 3. Episode 26150, average_reward -0.072581
Episode 26150: Total Loss of tensor([[23.0807]], grad_fn=<SubBackward0>)
[2022-11-06 02:19:43.636966] Process 1. Episode 25750, average_reward -0.068350
Episode 25750: Total Loss of tensor([[15.3168]], grad_fn=<SubBackward0>)
[2022-11-06 02:20:16.615894] Process 5. Episode 26450, average_reward -0.069830
Episode 26450: Total Loss of tensor([[14.5317]], grad_fn=<SubBackward0>)
[2022-11-06 02:21:03.710111] Process 4. Episode 27950, average_reward -0.071556
Episode 27950: Total Loss of tensor([[22.4262]], grad_fn=<SubBackward0>)
[2022-11-06 02:21:12.263749] Process 0. Episode 26300, average_reward -0.069962
Episode 26300: Total Loss of tensor([[18.5062]], grad_fn=<SubBackward0>)
[2022-11-06 02:21:38.267633] Process 2. Episode 26850, average_reward -0.074488
Episode 26850: Total Loss of tensor([[10.9382]], grad_fn=<SubBackward0>)
[2022-11-06 02:22:07.666369] Process 1. Episode 25800, average_reward -0.068411
Episode 25800: Total Loss of tensor([[9.3137]], grad_fn=<SubBackward0>)
[2022-11-06 02:22:19.061751] Process 3. Episode 26200, average_reward -0.072443
Episode 26200: Total Loss of tensor([[17.8678]], grad_fn=<SubBackward0>)
[2022-11-06 02:22:42.654571] Process 5. Episode 26500, average_reward -0.069849
Episode 26500: Total Loss of tensor([[7.4855]], grad_fn=<SubBackward0>)
[2022-11-06 02:23:23.341201] Process 4. Episode 28000, average_reward -0.071464
Episode 28000: Total Loss of tensor([[-4.6369]], grad_fn=<SubBackward0>)
[2022-11-06 02:23:33.803228] Process 0. Episode 26350, average_reward -0.069905
Episode 26350: Total Loss of tensor([[12.3046]], grad_fn=<SubBackward0>)
[2022-11-06 02:23:54.696047] Process 2. Episode 26900, average_reward -0.074461
Episode 26900: Total Loss of tensor([[12.0977]], grad_fn=<SubBackward0>)
[2022-11-06 02:24:47.866914] Process 1. Episode 25850, average_reward -0.068472
Episode 25850: Total Loss of tensor([[1.1908]], grad_fn=<SubBackward0>)
[2022-11-06 02:24:59.673057] Process 3. Episode 26250, average_reward -0.072495
Episode 26250: Total Loss of tensor([[7.5667]], grad_fn=<SubBackward0>)
[2022-11-06 02:25:21.303570] Process 5. Episode 26550, average_reward -0.069793
Episode 26550: Total Loss of tensor([[1.2582]], grad_fn=<SubBackward0>)
[2022-11-06 02:25:43.013763] Process 4. Episode 28050, average_reward -0.071408
Episode 28050: Total Loss of tensor([[7.0784]], grad_fn=<SubBackward0>)
[2022-11-06 02:26:01.819868] Process 0. Episode 26400, average_reward -0.069848
Episode 26400: Total Loss of tensor([[3.1890]], grad_fn=<SubBackward0>)
[2022-11-06 02:26:22.324728] Process 2. Episode 26950, average_reward -0.074471
Episode 26950: Total Loss of tensor([[2.2628]], grad_fn=<SubBackward0>)
[2022-11-06 02:27:25.511523] Process 1. Episode 25900, average_reward -0.068494
Episode 25900: Total Loss of tensor([[-129.3811]], grad_fn=<SubBackward0>)
[2022-11-06 02:27:26.118206] Process 3. Episode 26300, average_reward -0.072471
Episode 26300: Total Loss of tensor([[-2.2851]], grad_fn=<SubBackward0>)
[2022-11-06 02:27:55.494608] Process 5. Episode 26600, average_reward -0.069812
Episode 26600: Total Loss of tensor([[10.9185]], grad_fn=<SubBackward0>)
[2022-11-06 02:28:02.602544] Process 4. Episode 28100, average_reward -0.071317
Episode 28100: Total Loss of tensor([[8.0066]], grad_fn=<SubBackward0>)
[2022-11-06 02:28:30.022721] Process 0. Episode 26450, average_reward -0.069754
Episode 26450: Total Loss of tensor([[9.0128]], grad_fn=<SubBackward0>)
[2022-11-06 02:28:59.370684] Process 2. Episode 27000, average_reward -0.074593
Episode 27000: Total Loss of tensor([[5.9636]], grad_fn=<SubBackward0>)
[2022-11-06 02:29:52.708978] Process 3. Episode 26350, average_reward -0.072600
Episode 26350: Total Loss of tensor([[-1.1048]], grad_fn=<SubBackward0>)
[2022-11-06 02:29:54.426868] Process 1. Episode 25950, average_reward -0.068555
Episode 25950: Total Loss of tensor([[16.9101]], grad_fn=<SubBackward0>)
[2022-11-06 02:30:19.707935] Process 5. Episode 26650, average_reward -0.069981
Episode 26650: Total Loss of tensor([[10.5744]], grad_fn=<SubBackward0>)
[2022-11-06 02:30:23.948973] Process 4. Episode 28150, average_reward -0.071332
Episode 28150: Total Loss of tensor([[18.6531]], grad_fn=<SubBackward0>)
[2022-11-06 02:30:58.813399] Process 0. Episode 26500, average_reward -0.069774
Episode 26500: Total Loss of tensor([[11.2964]], grad_fn=<SubBackward0>)
[2022-11-06 02:31:25.417233] Process 2. Episode 27050, average_reward -0.074492
Episode 27050: Total Loss of tensor([[6.0347]], grad_fn=<SubBackward0>)
[2022-11-06 02:32:26.347090] Process 1. Episode 26000, average_reward -0.068577
Episode 26000: Total Loss of tensor([[-11.5378]], grad_fn=<SubBackward0>)
[2022-11-06 02:32:30.020513] Process 3. Episode 26400, average_reward -0.072689
Episode 26400: Total Loss of tensor([[-58.7180]], grad_fn=<SubBackward0>)
[2022-11-06 02:32:40.338145] Process 4. Episode 28200, average_reward -0.071348
Episode 28200: Total Loss of tensor([[4.3096]], grad_fn=<SubBackward0>)
[2022-11-06 02:32:48.647499] Process 5. Episode 26700, average_reward -0.069963
Episode 26700: Total Loss of tensor([[15.1894]], grad_fn=<SubBackward0>)
[2022-11-06 02:33:23.460108] Process 0. Episode 26550, average_reward -0.069755
Episode 26550: Total Loss of tensor([[11.4267]], grad_fn=<SubBackward0>)
[2022-11-06 02:34:01.258075] Process 2. Episode 27100, average_reward -0.074391
Episode 27100: Total Loss of tensor([[1.2317]], grad_fn=<SubBackward0>)
[2022-11-06 02:34:50.989835] Process 1. Episode 26050, average_reward -0.068714
Episode 26050: Total Loss of tensor([[14.1525]], grad_fn=<SubBackward0>)
[2022-11-06 02:34:59.551152] Process 4. Episode 28250, average_reward -0.071327
Episode 28250: Total Loss of tensor([[-7.3857]], grad_fn=<SubBackward0>)
[2022-11-06 02:35:02.348032] Process 3. Episode 26450, average_reward -0.072703
Episode 26450: Total Loss of tensor([[21.1468]], grad_fn=<SubBackward0>)
[2022-11-06 02:35:15.023510] Process 5. Episode 26750, average_reward -0.069981
Episode 26750: Total Loss of tensor([[2.3638]], grad_fn=<SubBackward0>)
[2022-11-06 02:35:57.314466] Process 0. Episode 26600, average_reward -0.069737
Episode 26600: Total Loss of tensor([[3.2654]], grad_fn=<SubBackward0>)
[2022-11-06 02:36:30.845638] Process 2. Episode 27150, average_reward -0.074328
Episode 27150: Total Loss of tensor([[-3.3173]], grad_fn=<SubBackward0>)
[2022-11-06 02:37:16.982189] Process 1. Episode 26100, average_reward -0.068774
Episode 26100: Total Loss of tensor([[-1.7764]], grad_fn=<SubBackward0>)
[2022-11-06 02:37:18.260634] Process 4. Episode 28300, average_reward -0.071378
Episode 28300: Total Loss of tensor([[2.9210]], grad_fn=<SubBackward0>)
[2022-11-06 02:37:34.006124] Process 3. Episode 26500, average_reward -0.072642
Episode 26500: Total Loss of tensor([[-133.2253]], grad_fn=<SubBackward0>)
[2022-11-06 02:37:40.712454] Process 5. Episode 26800, average_reward -0.070037
Episode 26800: Total Loss of tensor([[16.2481]], grad_fn=<SubBackward0>)
[2022-11-06 02:38:29.468506] Process 0. Episode 26650, average_reward -0.069681
Episode 26650: Total Loss of tensor([[18.5290]], grad_fn=<SubBackward0>)
[2022-11-06 02:39:01.224679] Process 2. Episode 27200, average_reward -0.074338
Episode 27200: Total Loss of tensor([[25.8946]], grad_fn=<SubBackward0>)
[2022-11-06 02:39:38.542935] Process 4. Episode 28350, average_reward -0.071429
Episode 28350: Total Loss of tensor([[19.0824]], grad_fn=<SubBackward0>)
[2022-11-06 02:39:53.002693] Process 1. Episode 26150, average_reward -0.068757
Episode 26150: Total Loss of tensor([[16.4303]], grad_fn=<SubBackward0>)
[2022-11-06 02:39:59.834805] Process 3. Episode 26550, average_reward -0.072731
Episode 26550: Total Loss of tensor([[21.1401]], grad_fn=<SubBackward0>)
[2022-11-06 02:40:10.223397] Process 5. Episode 26850, average_reward -0.070056
Episode 26850: Total Loss of tensor([[-4.3554]], grad_fn=<SubBackward0>)
[2022-11-06 02:41:01.301909] Process 0. Episode 26700, average_reward -0.069663
Episode 26700: Total Loss of tensor([[12.4547]], grad_fn=<SubBackward0>)
[2022-11-06 02:41:40.493408] Process 2. Episode 27250, average_reward -0.074349
Episode 27250: Total Loss of tensor([[7.4806]], grad_fn=<SubBackward0>)
[2022-11-06 02:41:58.110451] Process 4. Episode 28400, average_reward -0.071479
Episode 28400: Total Loss of tensor([[8.5359]], grad_fn=<SubBackward0>)
[2022-11-06 02:42:33.554403] Process 3. Episode 26600, average_reward -0.072857
Episode 26600: Total Loss of tensor([[14.2448]], grad_fn=<SubBackward0>)
[2022-11-06 02:42:34.709988] Process 5. Episode 26900, average_reward -0.070149
Episode 26900: Total Loss of tensor([[18.4083]], grad_fn=<SubBackward0>)
[2022-11-06 02:42:34.982605] Process 1. Episode 26200, average_reward -0.068740
Episode 26200: Total Loss of tensor([[12.8721]], grad_fn=<SubBackward0>)
[2022-11-06 02:43:29.820586] Process 0. Episode 26750, average_reward -0.069645
Episode 26750: Total Loss of tensor([[-4.1498]], grad_fn=<SubBackward0>)
[2022-11-06 02:44:14.019453] Process 4. Episode 28450, average_reward -0.071424
Episode 28450: Total Loss of tensor([[12.5029]], grad_fn=<SubBackward0>)
[2022-11-06 02:44:18.490326] Process 2. Episode 27300, average_reward -0.074322
Episode 27300: Total Loss of tensor([[18.1745]], grad_fn=<SubBackward0>)
[2022-11-06 02:44:57.519193] Process 5. Episode 26950, average_reward -0.070204
Episode 26950: Total Loss of tensor([[-21.9434]], grad_fn=<SubBackward0>)
[2022-11-06 02:45:07.628649] Process 3. Episode 26650, average_reward -0.072983
Episode 26650: Total Loss of tensor([[3.8369]], grad_fn=<SubBackward0>)
[2022-11-06 02:45:16.661006] Process 1. Episode 26250, average_reward -0.068876
Episode 26250: Total Loss of tensor([[20.2667]], grad_fn=<SubBackward0>)
[2022-11-06 02:45:58.002673] Process 0. Episode 26800, average_reward -0.069627
Episode 26800: Total Loss of tensor([[1.2932]], grad_fn=<SubBackward0>)
[2022-11-06 02:46:33.788600] Process 4. Episode 28500, average_reward -0.071439
Episode 28500: Total Loss of tensor([[-6.4775]], grad_fn=<SubBackward0>)
[2022-11-06 02:46:53.011880] Process 2. Episode 27350, average_reward -0.074442
Episode 27350: Total Loss of tensor([[13.8239]], grad_fn=<SubBackward0>)
[2022-11-06 02:47:21.880532] Process 5. Episode 27000, average_reward -0.070148
Episode 27000: Total Loss of tensor([[-2.4557]], grad_fn=<SubBackward0>)
[2022-11-06 02:47:35.324492] Process 3. Episode 26700, average_reward -0.072959
Episode 26700: Total Loss of tensor([[-26.3558]], grad_fn=<SubBackward0>)
[2022-11-06 02:48:00.005632] Process 1. Episode 26300, average_reward -0.068745
Episode 26300: Total Loss of tensor([[5.5354]], grad_fn=<SubBackward0>)
[2022-11-06 02:48:23.836425] Process 0. Episode 26850, average_reward -0.069609
Episode 26850: Total Loss of tensor([[15.7909]], grad_fn=<SubBackward0>)
[2022-11-06 02:48:51.554056] Process 4. Episode 28550, average_reward -0.071454
Episode 28550: Total Loss of tensor([[22.5666]], grad_fn=<SubBackward0>)
[2022-11-06 02:49:20.792761] Process 2. Episode 27400, average_reward -0.074453
Episode 27400: Total Loss of tensor([[13.4967]], grad_fn=<SubBackward0>)
[2022-11-06 02:49:43.467832] Process 5. Episode 27050, average_reward -0.070240
Episode 27050: Total Loss of tensor([[-0.5795]], grad_fn=<SubBackward0>)
[2022-11-06 02:50:00.025706] Process 3. Episode 26750, average_reward -0.072935
Episode 26750: Total Loss of tensor([[14.2641]], grad_fn=<SubBackward0>)
[2022-11-06 02:50:43.007667] Process 1. Episode 26350, average_reward -0.068729
Episode 26350: Total Loss of tensor([[2.4117]], grad_fn=<SubBackward0>)
[2022-11-06 02:51:08.340717] Process 0. Episode 26900, average_reward -0.069703
Episode 26900: Total Loss of tensor([[-5.7663]], grad_fn=<SubBackward0>)
[2022-11-06 02:51:11.093555] Process 4. Episode 28600, average_reward -0.071399
Episode 28600: Total Loss of tensor([[12.9576]], grad_fn=<SubBackward0>)
[2022-11-06 02:51:44.522666] Process 2. Episode 27450, average_reward -0.074426
Episode 27450: Total Loss of tensor([[-3.8612]], grad_fn=<SubBackward0>)
[2022-11-06 02:52:04.837182] Process 5. Episode 27100, average_reward -0.070221
Episode 27100: Total Loss of tensor([[6.6017]], grad_fn=<SubBackward0>)
[2022-11-06 02:52:26.701537] Process 3. Episode 26800, average_reward -0.072910
Episode 26800: Total Loss of tensor([[8.9000]], grad_fn=<SubBackward0>)
[2022-11-06 02:53:21.008775] Process 1. Episode 26400, average_reward -0.068712
Episode 26400: Total Loss of tensor([[12.5605]], grad_fn=<SubBackward0>)
[2022-11-06 02:53:29.465547] Process 4. Episode 28650, average_reward -0.071344
Episode 28650: Total Loss of tensor([[3.5605]], grad_fn=<SubBackward0>)
[2022-11-06 02:53:37.807990] Process 0. Episode 26950, average_reward -0.069759
Episode 26950: Total Loss of tensor([[16.5780]], grad_fn=<SubBackward0>)
[2022-11-06 02:54:16.817703] Process 2. Episode 27500, average_reward -0.074473
Episode 27500: Total Loss of tensor([[6.1304]], grad_fn=<SubBackward0>)
[2022-11-06 02:54:30.477148] Process 5. Episode 27150, average_reward -0.070387
Episode 27150: Total Loss of tensor([[3.1899]], grad_fn=<SubBackward0>)
[2022-11-06 02:54:51.038887] Process 3. Episode 26850, average_reward -0.072924
Episode 26850: Total Loss of tensor([[-2.1256]], grad_fn=<SubBackward0>)
[2022-11-06 02:55:49.949704] Process 4. Episode 28700, average_reward -0.071254
Episode 28700: Total Loss of tensor([[14.8967]], grad_fn=<SubBackward0>)
[2022-11-06 02:55:54.108981] Process 1. Episode 26450, average_reward -0.068733
Episode 26450: Total Loss of tensor([[-2.6041]], grad_fn=<SubBackward0>)
[2022-11-06 02:56:15.343081] Process 0. Episode 27000, average_reward -0.069667
Episode 27000: Total Loss of tensor([[28.6803]], grad_fn=<SubBackward0>)
[2022-11-06 02:56:45.988368] Process 2. Episode 27550, average_reward -0.074592
Episode 27550: Total Loss of tensor([[9.1488]], grad_fn=<SubBackward0>)
[2022-11-06 02:56:53.072489] Process 5. Episode 27200, average_reward -0.070368
Episode 27200: Total Loss of tensor([[-7.2276]], grad_fn=<SubBackward0>)
[2022-11-06 02:57:15.710646] Process 3. Episode 26900, average_reward -0.072900
Episode 26900: Total Loss of tensor([[5.2768]], grad_fn=<SubBackward0>)
[2022-11-06 02:58:08.790936] Process 4. Episode 28750, average_reward -0.071270
Episode 28750: Total Loss of tensor([[11.1348]], grad_fn=<SubBackward0>)
[2022-11-06 02:58:22.365722] Process 1. Episode 26500, average_reward -0.068906
Episode 26500: Total Loss of tensor([[20.4859]], grad_fn=<SubBackward0>)
[2022-11-06 02:58:48.169690] Process 0. Episode 27050, average_reward -0.069612
Episode 27050: Total Loss of tensor([[-73.9644]], grad_fn=<SubBackward0>)
[2022-11-06 02:59:14.990406] Process 2. Episode 27600, average_reward -0.074565
Episode 27600: Total Loss of tensor([[14.3552]], grad_fn=<SubBackward0>)
[2022-11-06 02:59:15.920475] Process 5. Episode 27250, average_reward -0.070385
Episode 27250: Total Loss of tensor([[-4.3739]], grad_fn=<SubBackward0>)
[2022-11-06 02:59:54.862407] Process 3. Episode 26950, average_reward -0.072913
Episode 26950: Total Loss of tensor([[-33.3901]], grad_fn=<SubBackward0>)
[2022-11-06 03:00:27.555727] Process 4. Episode 28800, average_reward -0.071215
Episode 28800: Total Loss of tensor([[6.6526]], grad_fn=<SubBackward0>)
[2022-11-06 03:00:52.696639] Process 1. Episode 26550, average_reward -0.068927
Episode 26550: Total Loss of tensor([[6.2020]], grad_fn=<SubBackward0>)
[2022-11-06 03:01:20.368558] Process 0. Episode 27100, average_reward -0.069705
Episode 27100: Total Loss of tensor([[9.6229]], grad_fn=<SubBackward0>)
[2022-11-06 03:01:39.370126] Process 5. Episode 27300, average_reward -0.070366
Episode 27300: Total Loss of tensor([[9.7925]], grad_fn=<SubBackward0>)
[2022-11-06 03:01:43.057350] Process 2. Episode 27650, average_reward -0.074647
Episode 27650: Total Loss of tensor([[8.3300]], grad_fn=<SubBackward0>)
[2022-11-06 03:02:21.400064] Process 3. Episode 27000, average_reward -0.072852
Episode 27000: Total Loss of tensor([[16.5464]], grad_fn=<SubBackward0>)
[2022-11-06 03:02:48.375086] Process 4. Episode 28850, average_reward -0.071231
Episode 28850: Total Loss of tensor([[-112.9204]], grad_fn=<SubBackward0>)
[2022-11-06 03:03:30.665087] Process 1. Episode 26600, average_reward -0.068985
Episode 26600: Total Loss of tensor([[-6.2217]], grad_fn=<SubBackward0>)
[2022-11-06 03:03:58.749085] Process 0. Episode 27150, average_reward -0.069613
Episode 27150: Total Loss of tensor([[-2.6924]], grad_fn=<SubBackward0>)
[2022-11-06 03:04:06.853514] Process 2. Episode 27700, average_reward -0.074621
Episode 27700: Total Loss of tensor([[12.2535]], grad_fn=<SubBackward0>)
[2022-11-06 03:04:07.164573] Process 5. Episode 27350, average_reward -0.070347
Episode 27350: Total Loss of tensor([[14.4447]], grad_fn=<SubBackward0>)
[2022-11-06 03:04:46.592994] Process 3. Episode 27050, average_reward -0.072828
Episode 27050: Total Loss of tensor([[4.2940]], grad_fn=<SubBackward0>)
[2022-11-06 03:05:07.064050] Process 4. Episode 28900, average_reward -0.071142
Episode 28900: Total Loss of tensor([[13.3743]], grad_fn=<SubBackward0>)
[2022-11-06 03:06:09.827206] Process 1. Episode 26650, average_reward -0.068968
Episode 26650: Total Loss of tensor([[2.9538]], grad_fn=<SubBackward0>)
[2022-11-06 03:06:31.183153] Process 2. Episode 27750, average_reward -0.074667
Episode 27750: Total Loss of tensor([[3.7488]], grad_fn=<SubBackward0>)
[2022-11-06 03:06:32.726292] Process 0. Episode 27200, average_reward -0.069559
Episode 27200: Total Loss of tensor([[5.3077]], grad_fn=<SubBackward0>)
[2022-11-06 03:06:36.336040] Process 5. Episode 27400, average_reward -0.070365
Episode 27400: Total Loss of tensor([[-113.8518]], grad_fn=<SubBackward0>)
[2022-11-06 03:07:14.047021] Process 3. Episode 27100, average_reward -0.072731
Episode 27100: Total Loss of tensor([[15.4350]], grad_fn=<SubBackward0>)
[2022-11-06 03:07:27.413922] Process 4. Episode 28950, average_reward -0.071088
Episode 28950: Total Loss of tensor([[-58.0570]], grad_fn=<SubBackward0>)
[2022-11-06 03:08:43.659624] Process 1. Episode 26700, average_reward -0.068989
Episode 26700: Total Loss of tensor([[-1.1302]], grad_fn=<SubBackward0>)
[2022-11-06 03:08:53.132468] Process 2. Episode 27800, average_reward -0.074748
Episode 27800: Total Loss of tensor([[-46.9175]], grad_fn=<SubBackward0>)
[2022-11-06 03:09:00.283512] Process 5. Episode 27450, average_reward -0.070455
Episode 27450: Total Loss of tensor([[-0.0831]], grad_fn=<SubBackward0>)
[2022-11-06 03:09:05.657189] Process 0. Episode 27250, average_reward -0.069578
Episode 27250: Total Loss of tensor([[7.5581]], grad_fn=<SubBackward0>)
[2022-11-06 03:09:48.022913] Process 4. Episode 29000, average_reward -0.071138
Episode 29000: Total Loss of tensor([[14.2268]], grad_fn=<SubBackward0>)
[2022-11-06 03:09:51.852920] Process 3. Episode 27150, average_reward -0.072707
Episode 27150: Total Loss of tensor([[17.1017]], grad_fn=<SubBackward0>)
[2022-11-06 03:11:14.013135] Process 2. Episode 27850, average_reward -0.074829
Episode 27850: Total Loss of tensor([[-123.9670]], grad_fn=<SubBackward0>)
[2022-11-06 03:11:22.559424] Process 1. Episode 26750, average_reward -0.069047
Episode 26750: Total Loss of tensor([[11.4306]], grad_fn=<SubBackward0>)
[2022-11-06 03:11:24.017893] Process 5. Episode 27500, average_reward -0.070509
Episode 27500: Total Loss of tensor([[-8.9098]], grad_fn=<SubBackward0>)
[2022-11-06 03:11:33.182082] Process 0. Episode 27300, average_reward -0.069670
Episode 27300: Total Loss of tensor([[5.3875]], grad_fn=<SubBackward0>)
[2022-11-06 03:12:07.672708] Process 4. Episode 29050, average_reward -0.071153
Episode 29050: Total Loss of tensor([[9.6127]], grad_fn=<SubBackward0>)
[2022-11-06 03:12:20.198021] Process 3. Episode 27200, average_reward -0.072610
Episode 27200: Total Loss of tensor([[6.0829]], grad_fn=<SubBackward0>)
[2022-11-06 03:13:46.075050] Process 2. Episode 27900, average_reward -0.074767
Episode 27900: Total Loss of tensor([[14.9953]], grad_fn=<SubBackward0>)
[2022-11-06 03:13:47.509170] Process 5. Episode 27550, average_reward -0.070526
Episode 27550: Total Loss of tensor([[3.3204]], grad_fn=<SubBackward0>)
[2022-11-06 03:14:01.187617] Process 1. Episode 26800, average_reward -0.069030
Episode 26800: Total Loss of tensor([[14.5299]], grad_fn=<SubBackward0>)
[2022-11-06 03:14:04.178326] Process 0. Episode 27350, average_reward -0.069689
Episode 27350: Total Loss of tensor([[-3.4949]], grad_fn=<SubBackward0>)
[2022-11-06 03:14:29.701203] Process 4. Episode 29100, average_reward -0.071134
Episode 29100: Total Loss of tensor([[4.0223]], grad_fn=<SubBackward0>)
[2022-11-06 03:14:47.461298] Process 3. Episode 27250, average_reward -0.072587
Episode 27250: Total Loss of tensor([[15.8993]], grad_fn=<SubBackward0>)
[2022-11-06 03:16:11.411625] Process 5. Episode 27600, average_reward -0.070652
Episode 27600: Total Loss of tensor([[1.9161]], grad_fn=<SubBackward0>)
[2022-11-06 03:16:21.617685] Process 2. Episode 27950, average_reward -0.074812
Episode 27950: Total Loss of tensor([[-109.8778]], grad_fn=<SubBackward0>)
[2022-11-06 03:16:25.404645] Process 0. Episode 27400, average_reward -0.069635
Episode 27400: Total Loss of tensor([[14.4069]], grad_fn=<SubBackward0>)
[2022-11-06 03:16:36.483533] Process 1. Episode 26850, average_reward -0.069050
Episode 26850: Total Loss of tensor([[2.1556]], grad_fn=<SubBackward0>)
[2022-11-06 03:16:49.507760] Process 4. Episode 29150, average_reward -0.071149
Episode 29150: Total Loss of tensor([[6.4097]], grad_fn=<SubBackward0>)
[2022-11-06 03:17:17.170490] Process 3. Episode 27300, average_reward -0.072527
Episode 27300: Total Loss of tensor([[13.4861]], grad_fn=<SubBackward0>)
[2022-11-06 03:18:49.096451] Process 2. Episode 28000, average_reward -0.074786
Episode 28000: Total Loss of tensor([[13.3807]], grad_fn=<SubBackward0>)
[2022-11-06 03:18:50.569710] Process 0. Episode 27450, average_reward -0.069690
Episode 27450: Total Loss of tensor([[3.3198]], grad_fn=<SubBackward0>)
[2022-11-06 03:18:56.340711] Process 5. Episode 27650, average_reward -0.070669
Episode 27650: Total Loss of tensor([[-4.6522]], grad_fn=<SubBackward0>)
[2022-11-06 03:19:04.460877] Process 1. Episode 26900, average_reward -0.069033
Episode 26900: Total Loss of tensor([[5.4387]], grad_fn=<SubBackward0>)
[2022-11-06 03:19:12.250945] Process 4. Episode 29200, average_reward -0.071130
Episode 29200: Total Loss of tensor([[13.4316]], grad_fn=<SubBackward0>)
[2022-11-06 03:19:50.266715] Process 3. Episode 27350, average_reward -0.072505
Episode 27350: Total Loss of tensor([[5.6756]], grad_fn=<SubBackward0>)
[2022-11-06 03:21:11.486734] Process 2. Episode 28050, average_reward -0.074724
Episode 28050: Total Loss of tensor([[-2.2813]], grad_fn=<SubBackward0>)
[2022-11-06 03:21:14.843040] Process 0. Episode 27500, average_reward -0.069673
Episode 27500: Total Loss of tensor([[8.6424]], grad_fn=<SubBackward0>)
[2022-11-06 03:21:21.225232] Process 5. Episode 27700, average_reward -0.070578
Episode 27700: Total Loss of tensor([[0.5099]], grad_fn=<SubBackward0>)
[2022-11-06 03:21:33.123858] Process 1. Episode 26950, average_reward -0.069017
Episode 26950: Total Loss of tensor([[2.5747]], grad_fn=<SubBackward0>)
[2022-11-06 03:21:36.344505] Process 4. Episode 29250, average_reward -0.071214
Episode 29250: Total Loss of tensor([[2.1998]], grad_fn=<SubBackward0>)
[2022-11-06 03:22:35.702934] Process 3. Episode 27400, average_reward -0.072445
Episode 27400: Total Loss of tensor([[7.6511]], grad_fn=<SubBackward0>)
[2022-11-06 03:23:34.186598] Process 2. Episode 28100, average_reward -0.074662
Episode 28100: Total Loss of tensor([[18.9088]], grad_fn=<SubBackward0>)
[2022-11-06 03:23:48.251567] Process 0. Episode 27550, average_reward -0.069764
Episode 27550: Total Loss of tensor([[7.2351]], grad_fn=<SubBackward0>)
[2022-11-06 03:23:53.184113] Process 4. Episode 29300, average_reward -0.071331
Episode 29300: Total Loss of tensor([[16.5758]], grad_fn=<SubBackward0>)
[2022-11-06 03:23:53.677815] Process 5. Episode 27750, average_reward -0.070595
Episode 27750: Total Loss of tensor([[7.1688]], grad_fn=<SubBackward0>)
[2022-11-06 03:24:10.049806] Process 1. Episode 27000, average_reward -0.068963
Episode 27000: Total Loss of tensor([[4.4962]], grad_fn=<SubBackward0>)
[2022-11-06 03:25:13.850499] Process 3. Episode 27450, average_reward -0.072495
Episode 27450: Total Loss of tensor([[-14.3525]], grad_fn=<SubBackward0>)
[2022-11-06 03:25:59.683258] Process 2. Episode 28150, average_reward -0.074636
Episode 28150: Total Loss of tensor([[-6.9393]], grad_fn=<SubBackward0>)
[2022-11-06 03:26:10.551268] Process 4. Episode 29350, average_reward -0.071278
Episode 29350: Total Loss of tensor([[0.0677]], grad_fn=<SubBackward0>)
[2022-11-06 03:26:20.848467] Process 5. Episode 27800, average_reward -0.070504
Episode 27800: Total Loss of tensor([[-4.5392]], grad_fn=<SubBackward0>)
[2022-11-06 03:26:26.653216] Process 0. Episode 27600, average_reward -0.069710
Episode 27600: Total Loss of tensor([[5.0125]], grad_fn=<SubBackward0>)
[2022-11-06 03:26:48.425543] Process 1. Episode 27050, average_reward -0.068983
Episode 27050: Total Loss of tensor([[-5.4772]], grad_fn=<SubBackward0>)
[2022-11-06 03:27:47.339311] Process 3. Episode 27500, average_reward -0.072473
Episode 27500: Total Loss of tensor([[3.4820]], grad_fn=<SubBackward0>)
[2022-11-06 03:28:27.951378] Process 4. Episode 29400, average_reward -0.071361
Episode 29400: Total Loss of tensor([[-0.1569]], grad_fn=<SubBackward0>)
[2022-11-06 03:28:29.886367] Process 2. Episode 28200, average_reward -0.074716
Episode 28200: Total Loss of tensor([[0.5525]], grad_fn=<SubBackward0>)
[2022-11-06 03:28:43.136367] Process 5. Episode 27850, average_reward -0.070413
Episode 27850: Total Loss of tensor([[10.4605]], grad_fn=<SubBackward0>)
[2022-11-06 03:29:03.430857] Process 0. Episode 27650, average_reward -0.069693
Episode 27650: Total Loss of tensor([[-6.8071]], grad_fn=<SubBackward0>)
[2022-11-06 03:29:18.419151] Process 1. Episode 27100, average_reward -0.068893
Episode 27100: Total Loss of tensor([[1.4234]], grad_fn=<SubBackward0>)
[2022-11-06 03:30:19.237113] Process 3. Episode 27550, average_reward -0.072486
Episode 27550: Total Loss of tensor([[-2.8575]], grad_fn=<SubBackward0>)
[2022-11-06 03:30:50.435479] Process 4. Episode 29450, average_reward -0.071273
Episode 29450: Total Loss of tensor([[-123.1876]], grad_fn=<SubBackward0>)
[2022-11-06 03:31:00.512689] Process 2. Episode 28250, average_reward -0.074726
Episode 28250: Total Loss of tensor([[9.5943]], grad_fn=<SubBackward0>)
[2022-11-06 03:31:08.056128] Process 5. Episode 27900, average_reward -0.070287
Episode 27900: Total Loss of tensor([[2.4830]], grad_fn=<SubBackward0>)
[2022-11-06 03:31:42.755035] Process 0. Episode 27700, average_reward -0.069675
Episode 27700: Total Loss of tensor([[14.0705]], grad_fn=<SubBackward0>)
[2022-11-06 03:31:49.750278] Process 1. Episode 27150, average_reward -0.068913
Episode 27150: Total Loss of tensor([[9.6243]], grad_fn=<SubBackward0>)
[2022-11-06 03:32:43.233407] Process 3. Episode 27600, average_reward -0.072464
Episode 27600: Total Loss of tensor([[8.0105]], grad_fn=<SubBackward0>)
[2022-11-06 03:33:12.481857] Process 4. Episode 29500, average_reward -0.071254
Episode 29500: Total Loss of tensor([[6.0006]], grad_fn=<SubBackward0>)
[2022-11-06 03:33:32.881602] Process 5. Episode 27950, average_reward -0.070376
Episode 27950: Total Loss of tensor([[-54.2742]], grad_fn=<SubBackward0>)
[2022-11-06 03:33:42.497913] Process 2. Episode 28300, average_reward -0.074770
Episode 28300: Total Loss of tensor([[15.4104]], grad_fn=<SubBackward0>)
[2022-11-06 03:34:13.976180] Process 0. Episode 27750, average_reward -0.069658
Episode 27750: Total Loss of tensor([[15.4361]], grad_fn=<SubBackward0>)
[2022-11-06 03:34:18.314936] Process 1. Episode 27200, average_reward -0.068934
Episode 27200: Total Loss of tensor([[11.2747]], grad_fn=<SubBackward0>)
[2022-11-06 03:35:10.578993] Process 3. Episode 27650, average_reward -0.072586
Episode 27650: Total Loss of tensor([[22.0163]], grad_fn=<SubBackward0>)
[2022-11-06 03:35:31.310706] Process 4. Episode 29550, average_reward -0.071269
Episode 29550: Total Loss of tensor([[3.7642]], grad_fn=<SubBackward0>)
[2022-11-06 03:35:56.398508] Process 5. Episode 28000, average_reward -0.070357
Episode 28000: Total Loss of tensor([[1.9205]], grad_fn=<SubBackward0>)
[2022-11-06 03:36:24.797243] Process 2. Episode 28350, average_reward -0.074709
Episode 28350: Total Loss of tensor([[-127.3853]], grad_fn=<SubBackward0>)
[2022-11-06 03:36:37.189607] Process 0. Episode 27800, average_reward -0.069640
Episode 27800: Total Loss of tensor([[0.6570]], grad_fn=<SubBackward0>)
[2022-11-06 03:36:50.867516] Process 1. Episode 27250, average_reward -0.068991
Episode 27250: Total Loss of tensor([[-2.1955]], grad_fn=<SubBackward0>)
[2022-11-06 03:37:45.130981] Process 3. Episode 27700, average_reward -0.072563
Episode 27700: Total Loss of tensor([[-3.6206]], grad_fn=<SubBackward0>)
[2022-11-06 03:37:51.697511] Process 4. Episode 29600, average_reward -0.071351
Episode 29600: Total Loss of tensor([[9.7091]], grad_fn=<SubBackward0>)
[2022-11-06 03:38:18.149522] Process 5. Episode 28050, average_reward -0.070446
Episode 28050: Total Loss of tensor([[20.4064]], grad_fn=<SubBackward0>)
[2022-11-06 03:38:58.663637] Process 2. Episode 28400, average_reward -0.074789
Episode 28400: Total Loss of tensor([[7.5218]], grad_fn=<SubBackward0>)
[2022-11-06 03:39:11.083604] Process 0. Episode 27850, average_reward -0.069623
Episode 27850: Total Loss of tensor([[-0.3519]], grad_fn=<SubBackward0>)
[2022-11-06 03:39:24.607117] Process 1. Episode 27300, average_reward -0.069048
Episode 27300: Total Loss of tensor([[16.6129]], grad_fn=<SubBackward0>)
[2022-11-06 03:40:09.843914] Process 3. Episode 27750, average_reward -0.072541
Episode 27750: Total Loss of tensor([[-2.0551]], grad_fn=<SubBackward0>)
[2022-11-06 03:40:15.186670] Process 4. Episode 29650, average_reward -0.071332
Episode 29650: Total Loss of tensor([[4.0685]], grad_fn=<SubBackward0>)
[2022-11-06 03:40:43.124558] Process 5. Episode 28100, average_reward -0.070463
Episode 28100: Total Loss of tensor([[-9.4791]], grad_fn=<SubBackward0>)
[2022-11-06 03:41:29.605129] Process 2. Episode 28450, average_reward -0.074728
Episode 28450: Total Loss of tensor([[2.4918]], grad_fn=<SubBackward0>)
[2022-11-06 03:41:47.343142] Process 0. Episode 27900, average_reward -0.069677
Episode 27900: Total Loss of tensor([[-122.0110]], grad_fn=<SubBackward0>)
[2022-11-06 03:41:58.983507] Process 1. Episode 27350, average_reward -0.068995
Episode 27350: Total Loss of tensor([[9.9194]], grad_fn=<SubBackward0>)
[2022-11-06 03:42:35.695480] Process 4. Episode 29700, average_reward -0.071347
Episode 29700: Total Loss of tensor([[10.6999]], grad_fn=<SubBackward0>)
[2022-11-06 03:42:39.268035] Process 3. Episode 27800, average_reward -0.072626
Episode 27800: Total Loss of tensor([[-127.6876]], grad_fn=<SubBackward0>)
[2022-11-06 03:43:11.145701] Process 5. Episode 28150, average_reward -0.070409
Episode 28150: Total Loss of tensor([[2.8690]], grad_fn=<SubBackward0>)
[2022-11-06 03:44:06.975320] Process 2. Episode 28500, average_reward -0.074632
Episode 28500: Total Loss of tensor([[7.1218]], grad_fn=<SubBackward0>)
[2022-11-06 03:44:29.478146] Process 0. Episode 27950, average_reward -0.069660
Episode 27950: Total Loss of tensor([[6.4606]], grad_fn=<SubBackward0>)
[2022-11-06 03:44:29.535365] Process 1. Episode 27400, average_reward -0.068978
Episode 27400: Total Loss of tensor([[9.7079]], grad_fn=<SubBackward0>)
[2022-11-06 03:44:55.619530] Process 4. Episode 29750, average_reward -0.071395
Episode 29750: Total Loss of tensor([[7.1823]], grad_fn=<SubBackward0>)
[2022-11-06 03:45:05.295139] Process 3. Episode 27850, average_reward -0.072603
Episode 27850: Total Loss of tensor([[0.3592]], grad_fn=<SubBackward0>)
[2022-11-06 03:45:35.758028] Process 5. Episode 28200, average_reward -0.070426
Episode 28200: Total Loss of tensor([[9.9220]], grad_fn=<SubBackward0>)
[2022-11-06 03:46:45.068041] Process 2. Episode 28550, average_reward -0.074781
Episode 28550: Total Loss of tensor([[-2.3748]], grad_fn=<SubBackward0>)
[2022-11-06 03:46:57.259629] Process 0. Episode 28000, average_reward -0.069714
Episode 28000: Total Loss of tensor([[-0.1035]], grad_fn=<SubBackward0>)
[2022-11-06 03:47:05.056026] Process 1. Episode 27450, average_reward -0.068925
Episode 27450: Total Loss of tensor([[16.1298]], grad_fn=<SubBackward0>)
[2022-11-06 03:47:17.969017] Process 4. Episode 29800, average_reward -0.071376
Episode 29800: Total Loss of tensor([[19.1495]], grad_fn=<SubBackward0>)
[2022-11-06 03:47:42.808376] Process 3. Episode 27900, average_reward -0.072581
Episode 27900: Total Loss of tensor([[-6.5920]], grad_fn=<SubBackward0>)
[2022-11-06 03:48:02.636183] Process 5. Episode 28250, average_reward -0.070407
Episode 28250: Total Loss of tensor([[12.5235]], grad_fn=<SubBackward0>)
[2022-11-06 03:49:14.806098] Process 2. Episode 28600, average_reward -0.074825
Episode 28600: Total Loss of tensor([[-80.7663]], grad_fn=<SubBackward0>)
[2022-11-06 03:49:37.632424] Process 0. Episode 28050, average_reward -0.069733
Episode 28050: Total Loss of tensor([[-111.7082]], grad_fn=<SubBackward0>)
[2022-11-06 03:49:37.662081] Process 4. Episode 29850, average_reward -0.071290
Episode 29850: Total Loss of tensor([[3.6631]], grad_fn=<SubBackward0>)
[2022-11-06 03:49:41.643343] Process 1. Episode 27500, average_reward -0.068873
Episode 27500: Total Loss of tensor([[-4.2632]], grad_fn=<SubBackward0>)
[2022-11-06 03:50:23.916247] Process 3. Episode 27950, average_reward -0.072665
Episode 27950: Total Loss of tensor([[7.9335]], grad_fn=<SubBackward0>)
[2022-11-06 03:50:25.080804] Process 5. Episode 28300, average_reward -0.070459
Episode 28300: Total Loss of tensor([[6.2717]], grad_fn=<SubBackward0>)
[2022-11-06 03:51:34.660250] Process 2. Episode 28650, average_reward -0.074729
Episode 28650: Total Loss of tensor([[15.8446]], grad_fn=<SubBackward0>)
[2022-11-06 03:52:00.492927] Process 4. Episode 29900, average_reward -0.071237
Episode 29900: Total Loss of tensor([[-1.9366]], grad_fn=<SubBackward0>)
[2022-11-06 03:52:13.492175] Process 0. Episode 28100, average_reward -0.069786
Episode 28100: Total Loss of tensor([[4.5324]], grad_fn=<SubBackward0>)
[2022-11-06 03:52:20.854900] Process 1. Episode 27550, average_reward -0.068893
Episode 27550: Total Loss of tensor([[10.4863]], grad_fn=<SubBackward0>)
[2022-11-06 03:52:53.905882] Process 5. Episode 28350, average_reward -0.070547
Episode 28350: Total Loss of tensor([[-130.9666]], grad_fn=<SubBackward0>)
[2022-11-06 03:52:54.137835] Process 3. Episode 28000, average_reward -0.072643
Episode 28000: Total Loss of tensor([[9.9344]], grad_fn=<SubBackward0>)
[2022-11-06 03:54:07.894986] Process 2. Episode 28700, average_reward -0.074669
Episode 28700: Total Loss of tensor([[9.7700]], grad_fn=<SubBackward0>)
[2022-11-06 03:54:22.077056] Process 4. Episode 29950, average_reward -0.071152
Episode 29950: Total Loss of tensor([[7.8948]], grad_fn=<SubBackward0>)
[2022-11-06 03:54:37.915337] Process 0. Episode 28150, average_reward -0.069769
Episode 28150: Total Loss of tensor([[-1.5753]], grad_fn=<SubBackward0>)
[2022-11-06 03:54:45.914678] Process 1. Episode 27600, average_reward -0.068986
Episode 27600: Total Loss of tensor([[-30.6553]], grad_fn=<SubBackward0>)
[2022-11-06 03:55:25.800518] Process 5. Episode 28400, average_reward -0.070458
Episode 28400: Total Loss of tensor([[5.9158]], grad_fn=<SubBackward0>)
[2022-11-06 03:55:37.355029] Process 3. Episode 28050, average_reward -0.072620
Episode 28050: Total Loss of tensor([[-2.2328]], grad_fn=<SubBackward0>)
[2022-11-06 03:56:32.190211] Process 2. Episode 28750, average_reward -0.074713
Episode 28750: Total Loss of tensor([[7.4464]], grad_fn=<SubBackward0>)
[2022-11-06 03:56:45.060595] Process 4. Episode 30000, average_reward -0.071100
Episode 30000: Total Loss of tensor([[7.2279]], grad_fn=<SubBackward0>)
[2022-11-06 03:57:05.592061] Process 0. Episode 28200, average_reward -0.069787
Episode 28200: Total Loss of tensor([[0.9097]], grad_fn=<SubBackward0>)
[2022-11-06 03:57:18.286325] Process 1. Episode 27650, average_reward -0.068933
Episode 27650: Total Loss of tensor([[9.4875]], grad_fn=<SubBackward0>)
[2022-11-06 03:57:52.213888] Process 5. Episode 28450, average_reward -0.070404
Episode 28450: Total Loss of tensor([[2.6170]], grad_fn=<SubBackward0>)
[2022-11-06 03:58:04.471809] Process 3. Episode 28100, average_reward -0.072669
Episode 28100: Total Loss of tensor([[-100.8906]], grad_fn=<SubBackward0>)
[2022-11-06 03:59:01.509098] Process 2. Episode 28800, average_reward -0.074653
Episode 28800: Total Loss of tensor([[7.6763]], grad_fn=<SubBackward0>)
[2022-11-06 03:59:04.674226] Process 4. Episode 30050, average_reward -0.071048
Episode 30050: Total Loss of tensor([[3.4747]], grad_fn=<SubBackward0>)
[2022-11-06 03:59:34.809385] Process 0. Episode 28250, average_reward -0.069805
Episode 28250: Total Loss of tensor([[13.4611]], grad_fn=<SubBackward0>)
[2022-11-06 03:59:56.819713] Process 1. Episode 27700, average_reward -0.068845
Episode 27700: Total Loss of tensor([[2.8418]], grad_fn=<SubBackward0>)
[2022-11-06 04:00:15.712226] Process 5. Episode 28500, average_reward -0.070491
Episode 28500: Total Loss of tensor([[13.3346]], grad_fn=<SubBackward0>)
[2022-11-06 04:00:30.421706] Process 3. Episode 28150, average_reward -0.072824
Episode 28150: Total Loss of tensor([[-123.8410]], grad_fn=<SubBackward0>)
[2022-11-06 04:01:23.140252] Process 4. Episode 30100, average_reward -0.071163
Episode 30100: Total Loss of tensor([[-2.2988]], grad_fn=<SubBackward0>)
[2022-11-06 04:01:45.518684] Process 2. Episode 28850, average_reward -0.074558
Episode 28850: Total Loss of tensor([[5.8023]], grad_fn=<SubBackward0>)
[2022-11-06 04:02:09.697814] Process 0. Episode 28300, average_reward -0.069753
Episode 28300: Total Loss of tensor([[9.6646]], grad_fn=<SubBackward0>)
[2022-11-06 04:02:34.999386] Process 5. Episode 28550, average_reward -0.070473
Episode 28550: Total Loss of tensor([[-15.3304]], grad_fn=<SubBackward0>)
[2022-11-06 04:02:35.322705] Process 1. Episode 27750, average_reward -0.068865
Episode 27750: Total Loss of tensor([[14.7171]], grad_fn=<SubBackward0>)
[2022-11-06 04:03:11.413018] Process 3. Episode 28200, average_reward -0.072730
Episode 28200: Total Loss of tensor([[7.4376]], grad_fn=<SubBackward0>)
[2022-11-06 04:03:40.564718] Process 4. Episode 30150, average_reward -0.071211
Episode 30150: Total Loss of tensor([[3.4989]], grad_fn=<SubBackward0>)
[2022-11-06 04:04:25.070029] Process 2. Episode 28900, average_reward -0.074671
Episode 28900: Total Loss of tensor([[-3.7275]], grad_fn=<SubBackward0>)
[2022-11-06 04:04:34.857477] Process 0. Episode 28350, average_reward -0.069700
Episode 28350: Total Loss of tensor([[1.7134]], grad_fn=<SubBackward0>)
[2022-11-06 04:04:58.923660] Process 5. Episode 28600, average_reward -0.070455
Episode 28600: Total Loss of tensor([[3.9109]], grad_fn=<SubBackward0>)
[2022-11-06 04:05:16.334119] Process 1. Episode 27800, average_reward -0.068777
Episode 27800: Total Loss of tensor([[7.7342]], grad_fn=<SubBackward0>)
[2022-11-06 04:05:37.159938] Process 3. Episode 28250, average_reward -0.072779
Episode 28250: Total Loss of tensor([[6.6233]], grad_fn=<SubBackward0>)
[2022-11-06 04:06:01.570442] Process 4. Episode 30200, average_reward -0.071126
Episode 30200: Total Loss of tensor([[-4.8527]], grad_fn=<SubBackward0>)
[2022-11-06 04:06:59.390860] Process 2. Episode 28950, average_reward -0.074611
Episode 28950: Total Loss of tensor([[5.4795]], grad_fn=<SubBackward0>)
[2022-11-06 04:07:01.300892] Process 0. Episode 28400, average_reward -0.069718
Episode 28400: Total Loss of tensor([[0.3735]], grad_fn=<SubBackward0>)
[2022-11-06 04:07:21.325314] Process 5. Episode 28650, average_reward -0.070366
Episode 28650: Total Loss of tensor([[-0.8727]], grad_fn=<SubBackward0>)
[2022-11-06 04:07:57.169270] Process 1. Episode 27850, average_reward -0.068725
Episode 27850: Total Loss of tensor([[8.7216]], grad_fn=<SubBackward0>)
[2022-11-06 04:08:02.852928] Process 3. Episode 28300, average_reward -0.072756
Episode 28300: Total Loss of tensor([[11.4832]], grad_fn=<SubBackward0>)
[2022-11-06 04:08:22.617633] Process 4. Episode 30250, average_reward -0.071207
Episode 30250: Total Loss of tensor([[2.3856]], grad_fn=<SubBackward0>)
[2022-11-06 04:09:29.838382] Process 2. Episode 29000, average_reward -0.074586
Episode 29000: Total Loss of tensor([[4.0833]], grad_fn=<SubBackward0>)
[2022-11-06 04:09:35.947235] Process 0. Episode 28450, average_reward -0.069807
Episode 28450: Total Loss of tensor([[-0.5689]], grad_fn=<SubBackward0>)
[2022-11-06 04:09:44.099230] Process 5. Episode 28700, average_reward -0.070383
Episode 28700: Total Loss of tensor([[11.3402]], grad_fn=<SubBackward0>)
[2022-11-06 04:10:27.996131] Process 3. Episode 28350, average_reward -0.072734
Episode 28350: Total Loss of tensor([[4.4603]], grad_fn=<SubBackward0>)
[2022-11-06 04:10:39.360074] Process 1. Episode 27900, average_reward -0.068817
Episode 27900: Total Loss of tensor([[-34.7668]], grad_fn=<SubBackward0>)
[2022-11-06 04:10:46.436813] Process 4. Episode 30300, average_reward -0.071221
Episode 30300: Total Loss of tensor([[-14.1702]], grad_fn=<SubBackward0>)
[2022-11-06 04:11:54.956934] Process 2. Episode 29050, average_reward -0.074664
Episode 29050: Total Loss of tensor([[18.9561]], grad_fn=<SubBackward0>)
[2022-11-06 04:12:05.426726] Process 0. Episode 28500, average_reward -0.069789
Episode 28500: Total Loss of tensor([[-9.3282]], grad_fn=<SubBackward0>)
[2022-11-06 04:12:07.876432] Process 5. Episode 28750, average_reward -0.070296
Episode 28750: Total Loss of tensor([[18.5502]], grad_fn=<SubBackward0>)
[2022-11-06 04:12:55.150444] Process 3. Episode 28400, average_reward -0.072746
Episode 28400: Total Loss of tensor([[24.9989]], grad_fn=<SubBackward0>)
[2022-11-06 04:13:10.080860] Process 4. Episode 30350, average_reward -0.071170
Episode 30350: Total Loss of tensor([[17.2636]], grad_fn=<SubBackward0>)
[2022-11-06 04:13:27.063547] Process 1. Episode 27950, average_reward -0.069016
Episode 27950: Total Loss of tensor([[-1.7550]], grad_fn=<SubBackward0>)
[2022-11-06 04:14:16.145756] Process 2. Episode 29100, average_reward -0.074811
Episode 29100: Total Loss of tensor([[-4.9759]], grad_fn=<SubBackward0>)
[2022-11-06 04:14:29.148059] Process 5. Episode 28800, average_reward -0.070347
Episode 28800: Total Loss of tensor([[-80.5806]], grad_fn=<SubBackward0>)
[2022-11-06 04:14:38.139351] Process 0. Episode 28550, average_reward -0.069807
Episode 28550: Total Loss of tensor([[24.8057]], grad_fn=<SubBackward0>)
[2022-11-06 04:15:20.049307] Process 3. Episode 28450, average_reward -0.072794
Episode 28450: Total Loss of tensor([[1.9679]], grad_fn=<SubBackward0>)
[2022-11-06 04:15:31.641040] Process 4. Episode 30400, average_reward -0.071151
Episode 30400: Total Loss of tensor([[13.6098]], grad_fn=<SubBackward0>)
[2022-11-06 04:16:02.175590] Process 1. Episode 28000, average_reward -0.069107
Episode 28000: Total Loss of tensor([[4.5588]], grad_fn=<SubBackward0>)
[2022-11-06 04:16:40.262650] Process 2. Episode 29150, average_reward -0.074786
Episode 29150: Total Loss of tensor([[20.3227]], grad_fn=<SubBackward0>)
[2022-11-06 04:16:50.188925] Process 5. Episode 28850, average_reward -0.070295
Episode 28850: Total Loss of tensor([[11.8144]], grad_fn=<SubBackward0>)
[2022-11-06 04:17:12.397918] Process 0. Episode 28600, average_reward -0.069755
Episode 28600: Total Loss of tensor([[3.3055]], grad_fn=<SubBackward0>)
[2022-11-06 04:17:46.492923] Process 3. Episode 28500, average_reward -0.072877
Episode 28500: Total Loss of tensor([[8.2041]], grad_fn=<SubBackward0>)
[2022-11-06 04:17:55.840352] Process 4. Episode 30450, average_reward -0.071232
Episode 30450: Total Loss of tensor([[-10.2553]], grad_fn=<SubBackward0>)
[2022-11-06 04:18:43.166230] Process 1. Episode 28050, average_reward -0.069127
Episode 28050: Total Loss of tensor([[-3.5797]], grad_fn=<SubBackward0>)
[2022-11-06 04:19:06.009161] Process 2. Episode 29200, average_reward -0.074658
Episode 29200: Total Loss of tensor([[8.6008]], grad_fn=<SubBackward0>)
[2022-11-06 04:19:14.598299] Process 5. Episode 28900, average_reward -0.070208
Episode 28900: Total Loss of tensor([[6.3528]], grad_fn=<SubBackward0>)
[2022-11-06 04:19:47.072788] Process 0. Episode 28650, average_reward -0.069668
Episode 28650: Total Loss of tensor([[13.7692]], grad_fn=<SubBackward0>)
[2022-11-06 04:20:15.606283] Process 3. Episode 28550, average_reward -0.072925
Episode 28550: Total Loss of tensor([[11.9055]], grad_fn=<SubBackward0>)
[2022-11-06 04:20:17.621588] Process 4. Episode 30500, average_reward -0.071279
Episode 30500: Total Loss of tensor([[5.0237]], grad_fn=<SubBackward0>)
[2022-11-06 04:21:26.540178] Process 2. Episode 29250, average_reward -0.074735
Episode 29250: Total Loss of tensor([[3.0365]], grad_fn=<SubBackward0>)
[2022-11-06 04:21:29.039127] Process 1. Episode 28100, average_reward -0.069110
Episode 28100: Total Loss of tensor([[-84.9509]], grad_fn=<SubBackward0>)
[2022-11-06 04:21:46.190083] Process 5. Episode 28950, average_reward -0.070259
Episode 28950: Total Loss of tensor([[-105.8749]], grad_fn=<SubBackward0>)
[2022-11-06 04:22:17.853323] Process 0. Episode 28700, average_reward -0.069617
Episode 28700: Total Loss of tensor([[-0.6514]], grad_fn=<SubBackward0>)
[2022-11-06 04:22:37.631719] Process 3. Episode 28600, average_reward -0.072832
Episode 28600: Total Loss of tensor([[9.2658]], grad_fn=<SubBackward0>)
[2022-11-06 04:22:39.640577] Process 4. Episode 30550, average_reward -0.071391
Episode 30550: Total Loss of tensor([[1.9227]], grad_fn=<SubBackward0>)
[2022-11-06 04:23:52.608428] Process 2. Episode 29300, average_reward -0.074744
Episode 29300: Total Loss of tensor([[-103.0066]], grad_fn=<SubBackward0>)
[2022-11-06 04:24:08.922733] Process 5. Episode 29000, average_reward -0.070276
Episode 29000: Total Loss of tensor([[-0.7614]], grad_fn=<SubBackward0>)
[2022-11-06 04:24:14.556024] Process 1. Episode 28150, average_reward -0.069165
Episode 28150: Total Loss of tensor([[5.9075]], grad_fn=<SubBackward0>)
[2022-11-06 04:24:50.469370] Process 0. Episode 28750, average_reward -0.069670
Episode 28750: Total Loss of tensor([[4.2037]], grad_fn=<SubBackward0>)
[2022-11-06 04:24:59.328053] Process 4. Episode 30600, average_reward -0.071471
Episode 30600: Total Loss of tensor([[11.6092]], grad_fn=<SubBackward0>)
[2022-11-06 04:25:05.720782] Process 3. Episode 28650, average_reward -0.072914
Episode 28650: Total Loss of tensor([[10.7452]], grad_fn=<SubBackward0>)
[2022-11-06 04:26:15.903939] Process 2. Episode 29350, average_reward -0.074719
Episode 29350: Total Loss of tensor([[2.3349]], grad_fn=<SubBackward0>)
[2022-11-06 04:26:36.472325] Process 5. Episode 29050, average_reward -0.070224
Episode 29050: Total Loss of tensor([[-6.6103]], grad_fn=<SubBackward0>)
[2022-11-06 04:26:41.120536] Process 1. Episode 28200, average_reward -0.069220
Episode 28200: Total Loss of tensor([[7.7988]], grad_fn=<SubBackward0>)
[2022-11-06 04:27:17.812781] Process 0. Episode 28800, average_reward -0.069757
Episode 28800: Total Loss of tensor([[-14.0742]], grad_fn=<SubBackward0>)
[2022-11-06 04:27:22.660860] Process 4. Episode 30650, average_reward -0.071452
Episode 30650: Total Loss of tensor([[12.4964]], grad_fn=<SubBackward0>)
[2022-11-06 04:27:35.101431] Process 3. Episode 28700, average_reward -0.072927
Episode 28700: Total Loss of tensor([[13.6257]], grad_fn=<SubBackward0>)
[2022-11-06 04:28:37.380243] Process 2. Episode 29400, average_reward -0.074830
Episode 29400: Total Loss of tensor([[14.9208]], grad_fn=<SubBackward0>)
[2022-11-06 04:29:03.955028] Process 5. Episode 29100, average_reward -0.070344
Episode 29100: Total Loss of tensor([[17.7688]], grad_fn=<SubBackward0>)
[2022-11-06 04:29:20.806964] Process 1. Episode 28250, average_reward -0.069239
Episode 28250: Total Loss of tensor([[16.3595]], grad_fn=<SubBackward0>)
[2022-11-06 04:29:43.462179] Process 0. Episode 28850, average_reward -0.069740
Episode 28850: Total Loss of tensor([[7.7496]], grad_fn=<SubBackward0>)
[2022-11-06 04:29:46.102263] Process 4. Episode 30700, average_reward -0.071433
Episode 30700: Total Loss of tensor([[7.4691]], grad_fn=<SubBackward0>)
[2022-11-06 04:30:02.216082] Process 3. Episode 28750, average_reward -0.072939
Episode 28750: Total Loss of tensor([[-1.3502]], grad_fn=<SubBackward0>)
[2022-11-06 04:31:07.281684] Process 2. Episode 29450, average_reward -0.074873
Episode 29450: Total Loss of tensor([[13.7512]], grad_fn=<SubBackward0>)
[2022-11-06 04:31:31.718711] Process 5. Episode 29150, average_reward -0.070292
Episode 29150: Total Loss of tensor([[-8.7362]], grad_fn=<SubBackward0>)
[2022-11-06 04:31:52.588234] Process 1. Episode 28300, average_reward -0.069187
Episode 28300: Total Loss of tensor([[-1.2940]], grad_fn=<SubBackward0>)
[2022-11-06 04:32:07.290834] Process 4. Episode 30750, average_reward -0.071447
Episode 30750: Total Loss of tensor([[10.6124]], grad_fn=<SubBackward0>)
[2022-11-06 04:32:16.509877] Process 0. Episode 28900, average_reward -0.069723
Episode 28900: Total Loss of tensor([[23.0950]], grad_fn=<SubBackward0>)
[2022-11-06 04:32:23.498473] Process 3. Episode 28800, average_reward -0.072986
Episode 28800: Total Loss of tensor([[16.6699]], grad_fn=<SubBackward0>)
[2022-11-06 04:33:32.389422] Process 2. Episode 29500, average_reward -0.074949
Episode 29500: Total Loss of tensor([[11.0375]], grad_fn=<SubBackward0>)
[2022-11-06 04:33:58.579818] Process 5. Episode 29200, average_reward -0.070308
Episode 29200: Total Loss of tensor([[8.4192]], grad_fn=<SubBackward0>)
[2022-11-06 04:34:27.414088] Process 4. Episode 30800, average_reward -0.071494
Episode 30800: Total Loss of tensor([[7.6041]], grad_fn=<SubBackward0>)
[2022-11-06 04:34:29.584206] Process 1. Episode 28350, average_reward -0.069242
Episode 28350: Total Loss of tensor([[-0.1798]], grad_fn=<SubBackward0>)
[2022-11-06 04:34:44.691615] Process 3. Episode 28850, average_reward -0.072964
Episode 28850: Total Loss of tensor([[-96.9631]], grad_fn=<SubBackward0>)
[2022-11-06 04:34:49.164364] Process 0. Episode 28950, average_reward -0.069706
Episode 28950: Total Loss of tensor([[-9.7116]], grad_fn=<SubBackward0>)
[2022-11-06 04:36:02.122376] Process 2. Episode 29550, average_reward -0.074992
Episode 29550: Total Loss of tensor([[-2.0367]], grad_fn=<SubBackward0>)
[2022-11-06 04:36:25.426933] Process 5. Episode 29250, average_reward -0.070256
Episode 29250: Total Loss of tensor([[-42.0042]], grad_fn=<SubBackward0>)
[2022-11-06 04:36:47.218161] Process 4. Episode 30850, average_reward -0.071572
Episode 30850: Total Loss of tensor([[12.2493]], grad_fn=<SubBackward0>)
[2022-11-06 04:37:06.742353] Process 3. Episode 28900, average_reward -0.072872
Episode 28900: Total Loss of tensor([[-1.0818]], grad_fn=<SubBackward0>)
[2022-11-06 04:37:09.294677] Process 1. Episode 28400, average_reward -0.069120
Episode 28400: Total Loss of tensor([[-4.9969]], grad_fn=<SubBackward0>)
[2022-11-06 04:37:26.203512] Process 0. Episode 29000, average_reward -0.069793
Episode 29000: Total Loss of tensor([[13.8854]], grad_fn=<SubBackward0>)
[2022-11-06 04:38:29.914045] Process 2. Episode 29600, average_reward -0.075000
Episode 29600: Total Loss of tensor([[12.7931]], grad_fn=<SubBackward0>)
[2022-11-06 04:38:47.626842] Process 5. Episode 29300, average_reward -0.070307
Episode 29300: Total Loss of tensor([[8.0326]], grad_fn=<SubBackward0>)
[2022-11-06 04:39:05.436209] Process 4. Episode 30900, average_reward -0.071553
Episode 30900: Total Loss of tensor([[-0.5434]], grad_fn=<SubBackward0>)
[2022-11-06 04:39:38.439498] Process 3. Episode 28950, average_reward -0.072953
Episode 28950: Total Loss of tensor([[7.6799]], grad_fn=<SubBackward0>)
[2022-11-06 04:39:47.089495] Process 1. Episode 28450, average_reward -0.069069
Episode 28450: Total Loss of tensor([[16.4394]], grad_fn=<SubBackward0>)
[2022-11-06 04:40:07.608309] Process 0. Episode 29050, average_reward -0.069845
Episode 29050: Total Loss of tensor([[13.9533]], grad_fn=<SubBackward0>)
[2022-11-06 04:40:53.927579] Process 2. Episode 29650, average_reward -0.074975
Episode 29650: Total Loss of tensor([[-21.1259]], grad_fn=<SubBackward0>)
[2022-11-06 04:41:17.843499] Process 5. Episode 29350, average_reward -0.070256
Episode 29350: Total Loss of tensor([[11.6198]], grad_fn=<SubBackward0>)
[2022-11-06 04:41:25.091143] Process 4. Episode 30950, average_reward -0.071567
Episode 30950: Total Loss of tensor([[1.3498]], grad_fn=<SubBackward0>)
[2022-11-06 04:42:01.588791] Process 3. Episode 29000, average_reward -0.072966
Episode 29000: Total Loss of tensor([[14.3750]], grad_fn=<SubBackward0>)
[2022-11-06 04:42:25.323416] Process 1. Episode 28500, average_reward -0.069158
Episode 28500: Total Loss of tensor([[11.9441]], grad_fn=<SubBackward0>)
[2022-11-06 04:42:42.307214] Process 0. Episode 29100, average_reward -0.069897
Episode 29100: Total Loss of tensor([[-29.2298]], grad_fn=<SubBackward0>)
[2022-11-06 04:43:22.140916] Process 2. Episode 29700, average_reward -0.075017
Episode 29700: Total Loss of tensor([[-6.1729]], grad_fn=<SubBackward0>)
[2022-11-06 04:43:44.089492] Process 4. Episode 31000, average_reward -0.071516
Episode 31000: Total Loss of tensor([[-61.2820]], grad_fn=<SubBackward0>)
[2022-11-06 04:43:58.466285] Process 5. Episode 29400, average_reward -0.070238
Episode 29400: Total Loss of tensor([[5.0625]], grad_fn=<SubBackward0>)
[2022-11-06 04:44:25.382607] Process 3. Episode 29050, average_reward -0.073012
Episode 29050: Total Loss of tensor([[11.1373]], grad_fn=<SubBackward0>)
[2022-11-06 04:45:03.173008] Process 1. Episode 28550, average_reward -0.069177
Episode 28550: Total Loss of tensor([[-122.0634]], grad_fn=<SubBackward0>)
[2022-11-06 04:45:11.255367] Process 0. Episode 29150, average_reward -0.069949
Episode 29150: Total Loss of tensor([[6.3085]], grad_fn=<SubBackward0>)
[2022-11-06 04:45:47.307488] Process 2. Episode 29750, average_reward -0.074958
Episode 29750: Total Loss of tensor([[17.7561]], grad_fn=<SubBackward0>)
[2022-11-06 04:46:06.233217] Process 4. Episode 31050, average_reward -0.071594
Episode 31050: Total Loss of tensor([[-30.3618]], grad_fn=<SubBackward0>)
[2022-11-06 04:46:30.022368] Process 5. Episode 29450, average_reward -0.070289
Episode 29450: Total Loss of tensor([[3.1723]], grad_fn=<SubBackward0>)
[2022-11-06 04:46:45.606234] Process 3. Episode 29100, average_reward -0.072955
Episode 29100: Total Loss of tensor([[12.8759]], grad_fn=<SubBackward0>)
[2022-11-06 04:47:31.138946] Process 1. Episode 28600, average_reward -0.069161
Episode 28600: Total Loss of tensor([[10.3690]], grad_fn=<SubBackward0>)
[2022-11-06 04:47:39.705665] Process 0. Episode 29200, average_reward -0.070034
Episode 29200: Total Loss of tensor([[-82.3155]], grad_fn=<SubBackward0>)
[2022-11-06 04:48:10.340635] Process 2. Episode 29800, average_reward -0.075034
Episode 29800: Total Loss of tensor([[6.4865]], grad_fn=<SubBackward0>)
[2022-11-06 04:48:31.986379] Process 4. Episode 31100, average_reward -0.071576
Episode 31100: Total Loss of tensor([[13.0860]], grad_fn=<SubBackward0>)
[2022-11-06 04:48:55.224839] Process 5. Episode 29500, average_reward -0.070271
Episode 29500: Total Loss of tensor([[5.4664]], grad_fn=<SubBackward0>)
[2022-11-06 04:49:12.424891] Process 3. Episode 29150, average_reward -0.072864
Episode 29150: Total Loss of tensor([[6.6660]], grad_fn=<SubBackward0>)
[2022-11-06 04:49:56.729436] Process 1. Episode 28650, average_reward -0.069110
Episode 28650: Total Loss of tensor([[-21.8087]], grad_fn=<SubBackward0>)
[2022-11-06 04:50:07.952137] Process 0. Episode 29250, average_reward -0.069949
Episode 29250: Total Loss of tensor([[5.5709]], grad_fn=<SubBackward0>)
[2022-11-06 04:50:34.110914] Process 2. Episode 29850, average_reward -0.075142
Episode 29850: Total Loss of tensor([[-39.1113]], grad_fn=<SubBackward0>)
[2022-11-06 04:50:53.665121] Process 4. Episode 31150, average_reward -0.071557
Episode 31150: Total Loss of tensor([[18.1542]], grad_fn=<SubBackward0>)
[2022-11-06 04:51:30.683901] Process 5. Episode 29550, average_reward -0.070254
Episode 29550: Total Loss of tensor([[20.9446]], grad_fn=<SubBackward0>)
[2022-11-06 04:51:37.038789] Process 3. Episode 29200, average_reward -0.072842
Episode 29200: Total Loss of tensor([[21.8012]], grad_fn=<SubBackward0>)
[2022-11-06 04:52:39.701722] Process 1. Episode 28700, average_reward -0.069129
Episode 28700: Total Loss of tensor([[2.0257]], grad_fn=<SubBackward0>)
[2022-11-06 04:52:43.713327] Process 0. Episode 29300, average_reward -0.069966
Episode 29300: Total Loss of tensor([[10.9399]], grad_fn=<SubBackward0>)
[2022-11-06 04:52:51.790934] Process 2. Episode 29900, average_reward -0.075151
Episode 29900: Total Loss of tensor([[13.2896]], grad_fn=<SubBackward0>)
[2022-11-06 04:53:15.279493] Process 4. Episode 31200, average_reward -0.071506
Episode 31200: Total Loss of tensor([[15.9977]], grad_fn=<SubBackward0>)
[2022-11-06 04:53:59.386435] Process 3. Episode 29250, average_reward -0.072821
Episode 29250: Total Loss of tensor([[10.5199]], grad_fn=<SubBackward0>)
[2022-11-06 04:54:08.713193] Process 5. Episode 29600, average_reward -0.070203
Episode 29600: Total Loss of tensor([[-0.9992]], grad_fn=<SubBackward0>)
[2022-11-06 04:55:13.740987] Process 0. Episode 29350, average_reward -0.070051
Episode 29350: Total Loss of tensor([[-115.2226]], grad_fn=<SubBackward0>)
[2022-11-06 04:55:16.607127] Process 1. Episode 28750, average_reward -0.069078
Episode 28750: Total Loss of tensor([[17.8300]], grad_fn=<SubBackward0>)
[2022-11-06 04:55:19.065253] Process 2. Episode 29950, average_reward -0.075292
Episode 29950: Total Loss of tensor([[9.3014]], grad_fn=<SubBackward0>)
[2022-11-06 04:55:43.452439] Process 4. Episode 31250, average_reward -0.071456
Episode 31250: Total Loss of tensor([[10.0872]], grad_fn=<SubBackward0>)
[2022-11-06 04:56:25.158461] Process 3. Episode 29300, average_reward -0.072867
Episode 29300: Total Loss of tensor([[-14.7665]], grad_fn=<SubBackward0>)
[2022-11-06 04:56:44.033374] Process 5. Episode 29650, average_reward -0.070253
Episode 29650: Total Loss of tensor([[0.7201]], grad_fn=<SubBackward0>)
[2022-11-06 04:57:43.592468] Process 2. Episode 30000, average_reward -0.075233
Episode 30000: Total Loss of tensor([[8.7612]], grad_fn=<SubBackward0>)
[2022-11-06 04:57:44.102516] Process 0. Episode 29400, average_reward -0.070000
Episode 29400: Total Loss of tensor([[10.6838]], grad_fn=<SubBackward0>)
[2022-11-06 04:57:52.739367] Process 1. Episode 28800, average_reward -0.069062
Episode 28800: Total Loss of tensor([[5.6407]], grad_fn=<SubBackward0>)
[2022-11-06 04:58:05.548863] Process 4. Episode 31300, average_reward -0.071438
Episode 31300: Total Loss of tensor([[-12.8034]], grad_fn=<SubBackward0>)
[2022-11-06 04:58:47.813926] Process 3. Episode 29350, average_reward -0.072879
Episode 29350: Total Loss of tensor([[6.6682]], grad_fn=<SubBackward0>)
[2022-11-06 04:59:16.653178] Process 5. Episode 29700, average_reward -0.070236
Episode 29700: Total Loss of tensor([[0.9931]], grad_fn=<SubBackward0>)
[2022-11-06 05:00:03.999127] Process 2. Episode 30050, average_reward -0.075175
Episode 30050: Total Loss of tensor([[-0.1033]], grad_fn=<SubBackward0>)
[2022-11-06 05:00:09.707324] Process 0. Episode 29450, average_reward -0.069983
Episode 29450: Total Loss of tensor([[1.9181]], grad_fn=<SubBackward0>)
[2022-11-06 05:00:28.249653] Process 4. Episode 31350, average_reward -0.071356
Episode 31350: Total Loss of tensor([[3.4697]], grad_fn=<SubBackward0>)
[2022-11-06 05:00:33.748573] Process 1. Episode 28850, average_reward -0.069081
Episode 28850: Total Loss of tensor([[2.7279]], grad_fn=<SubBackward0>)
[2022-11-06 05:01:09.793370] Process 3. Episode 29400, average_reward -0.072891
Episode 29400: Total Loss of tensor([[8.8818]], grad_fn=<SubBackward0>)
[2022-11-06 05:01:44.005001] Process 5. Episode 29750, average_reward -0.070185
Episode 29750: Total Loss of tensor([[12.2514]], grad_fn=<SubBackward0>)
[2022-11-06 05:02:28.720738] Process 2. Episode 30100, average_reward -0.075216
Episode 30100: Total Loss of tensor([[12.9717]], grad_fn=<SubBackward0>)
[2022-11-06 05:02:37.334451] Process 0. Episode 29500, average_reward -0.069932
Episode 29500: Total Loss of tensor([[13.6147]], grad_fn=<SubBackward0>)
[2022-11-06 05:02:51.476182] Process 4. Episode 31400, average_reward -0.071274
Episode 31400: Total Loss of tensor([[2.7340]], grad_fn=<SubBackward0>)
[2022-11-06 05:03:06.144057] Process 1. Episode 28900, average_reward -0.069100
Episode 28900: Total Loss of tensor([[8.8127]], grad_fn=<SubBackward0>)
[2022-11-06 05:03:31.120441] Process 3. Episode 29450, average_reward -0.072903
Episode 29450: Total Loss of tensor([[-43.3243]], grad_fn=<SubBackward0>)
[2022-11-06 05:04:04.981422] Process 5. Episode 29800, average_reward -0.070201
Episode 29800: Total Loss of tensor([[-5.2118]], grad_fn=<SubBackward0>)
[2022-11-06 05:05:06.733228] Process 2. Episode 30150, average_reward -0.075224
Episode 30150: Total Loss of tensor([[1.0767]], grad_fn=<SubBackward0>)
[2022-11-06 05:05:11.418427] Process 4. Episode 31450, average_reward -0.071320
Episode 31450: Total Loss of tensor([[3.0019]], grad_fn=<SubBackward0>)
[2022-11-06 05:05:11.563118] Process 0. Episode 29550, average_reward -0.069949
Episode 29550: Total Loss of tensor([[13.4334]], grad_fn=<SubBackward0>)
[2022-11-06 05:05:41.852130] Process 1. Episode 28950, average_reward -0.069154
Episode 28950: Total Loss of tensor([[-7.5166]], grad_fn=<SubBackward0>)
[2022-11-06 05:05:52.661686] Process 3. Episode 29500, average_reward -0.072949
Episode 29500: Total Loss of tensor([[3.6864]], grad_fn=<SubBackward0>)
[2022-11-06 05:06:40.178640] Process 5. Episode 29850, average_reward -0.070184
Episode 29850: Total Loss of tensor([[0.7036]], grad_fn=<SubBackward0>)
[2022-11-06 05:07:31.019480] Process 4. Episode 31500, average_reward -0.071333
Episode 31500: Total Loss of tensor([[9.5874]], grad_fn=<SubBackward0>)
[2022-11-06 05:07:32.153354] Process 2. Episode 30200, average_reward -0.075199
Episode 30200: Total Loss of tensor([[-3.2271]], grad_fn=<SubBackward0>)
[2022-11-06 05:07:53.351040] Process 0. Episode 29600, average_reward -0.069932
Episode 29600: Total Loss of tensor([[6.5973]], grad_fn=<SubBackward0>)
[2022-11-06 05:08:13.964997] Process 3. Episode 29550, average_reward -0.072893
Episode 29550: Total Loss of tensor([[7.3023]], grad_fn=<SubBackward0>)
[2022-11-06 05:08:17.944140] Process 1. Episode 29000, average_reward -0.069069
Episode 29000: Total Loss of tensor([[7.5965]], grad_fn=<SubBackward0>)
[2022-11-06 05:09:11.964001] Process 5. Episode 29900, average_reward -0.070134
Episode 29900: Total Loss of tensor([[6.6725]], grad_fn=<SubBackward0>)
[2022-11-06 05:09:49.999207] Process 4. Episode 31550, average_reward -0.071347
Episode 31550: Total Loss of tensor([[3.6885]], grad_fn=<SubBackward0>)
[2022-11-06 05:10:03.533640] Process 2. Episode 30250, average_reward -0.075207
Episode 30250: Total Loss of tensor([[-0.9286]], grad_fn=<SubBackward0>)
[2022-11-06 05:10:21.447040] Process 0. Episode 29650, average_reward -0.069882
Episode 29650: Total Loss of tensor([[3.5582]], grad_fn=<SubBackward0>)
[2022-11-06 05:10:40.778773] Process 3. Episode 29600, average_reward -0.072905
Episode 29600: Total Loss of tensor([[6.2820]], grad_fn=<SubBackward0>)
[2022-11-06 05:11:02.784390] Process 1. Episode 29050, average_reward -0.069053
Episode 29050: Total Loss of tensor([[5.0820]], grad_fn=<SubBackward0>)
[2022-11-06 05:11:46.116424] Process 5. Episode 29950, average_reward -0.070117
Episode 29950: Total Loss of tensor([[4.3584]], grad_fn=<SubBackward0>)
[2022-11-06 05:12:12.104859] Process 4. Episode 31600, average_reward -0.071266
Episode 31600: Total Loss of tensor([[6.3363]], grad_fn=<SubBackward0>)
[2022-11-06 05:12:38.739301] Process 2. Episode 30300, average_reward -0.075116
Episode 30300: Total Loss of tensor([[14.1746]], grad_fn=<SubBackward0>)
[2022-11-06 05:13:02.921413] Process 0. Episode 29700, average_reward -0.069865
Episode 29700: Total Loss of tensor([[-6.3627]], grad_fn=<SubBackward0>)
[2022-11-06 05:13:03.975476] Process 3. Episode 29650, average_reward -0.072917
Episode 29650: Total Loss of tensor([[6.0926]], grad_fn=<SubBackward0>)
[2022-11-06 05:13:49.261492] Process 1. Episode 29100, average_reward -0.069038
Episode 29100: Total Loss of tensor([[-24.0065]], grad_fn=<SubBackward0>)
[2022-11-06 05:14:07.286992] Process 5. Episode 30000, average_reward -0.070133
Episode 30000: Total Loss of tensor([[16.2785]], grad_fn=<SubBackward0>)
[2022-11-06 05:14:33.544553] Process 4. Episode 31650, average_reward -0.071248
Episode 31650: Total Loss of tensor([[7.1468]], grad_fn=<SubBackward0>)
[2022-11-06 05:15:11.465595] Process 2. Episode 30350, average_reward -0.075091
Episode 30350: Total Loss of tensor([[8.5221]], grad_fn=<SubBackward0>)
[2022-11-06 05:15:31.125552] Process 3. Episode 29700, average_reward -0.072929
Episode 29700: Total Loss of tensor([[9.6955]], grad_fn=<SubBackward0>)
[2022-11-06 05:15:46.866439] Process 0. Episode 29750, average_reward -0.069882
Episode 29750: Total Loss of tensor([[0.2254]], grad_fn=<SubBackward0>)
[2022-11-06 05:16:22.517408] Process 1. Episode 29150, average_reward -0.069022
Episode 29150: Total Loss of tensor([[3.1229]], grad_fn=<SubBackward0>)
[2022-11-06 05:16:37.645488] Process 5. Episode 30050, average_reward -0.070116
Episode 30050: Total Loss of tensor([[8.4454]], grad_fn=<SubBackward0>)
[2022-11-06 05:16:53.984894] Process 4. Episode 31700, average_reward -0.071262
Episode 31700: Total Loss of tensor([[5.5616]], grad_fn=<SubBackward0>)
[2022-11-06 05:17:47.236532] Process 2. Episode 30400, average_reward -0.075099
Episode 30400: Total Loss of tensor([[-96.0779]], grad_fn=<SubBackward0>)
[2022-11-06 05:17:59.944618] Process 3. Episode 29750, average_reward -0.072941
Episode 29750: Total Loss of tensor([[25.1513]], grad_fn=<SubBackward0>)
[2022-11-06 05:18:15.456558] Process 0. Episode 29800, average_reward -0.069933
Episode 29800: Total Loss of tensor([[-37.2424]], grad_fn=<SubBackward0>)
[2022-11-06 05:18:51.810404] Process 1. Episode 29200, average_reward -0.069247
Episode 29200: Total Loss of tensor([[11.5286]], grad_fn=<SubBackward0>)
[2022-11-06 05:19:12.628042] Process 5. Episode 30100, average_reward -0.070033
Episode 30100: Total Loss of tensor([[-57.9813]], grad_fn=<SubBackward0>)
[2022-11-06 05:19:14.995578] Process 4. Episode 31750, average_reward -0.071402
Episode 31750: Total Loss of tensor([[15.4983]], grad_fn=<SubBackward0>)
[2022-11-06 05:20:11.048688] Process 2. Episode 30450, average_reward -0.075008
Episode 30450: Total Loss of tensor([[25.8701]], grad_fn=<SubBackward0>)
[2022-11-06 05:20:32.612525] Process 3. Episode 29800, average_reward -0.072953
Episode 29800: Total Loss of tensor([[14.8860]], grad_fn=<SubBackward0>)
[2022-11-06 05:20:44.266394] Process 0. Episode 29850, average_reward -0.069983
Episode 29850: Total Loss of tensor([[10.7925]], grad_fn=<SubBackward0>)
[2022-11-06 05:21:36.466182] Process 1. Episode 29250, average_reward -0.069368
Episode 29250: Total Loss of tensor([[10.1337]], grad_fn=<SubBackward0>)
[2022-11-06 05:21:37.717287] Process 4. Episode 31800, average_reward -0.071384
Episode 31800: Total Loss of tensor([[8.1229]], grad_fn=<SubBackward0>)
[2022-11-06 05:21:42.380750] Process 5. Episode 30150, average_reward -0.069917
Episode 30150: Total Loss of tensor([[3.3048]], grad_fn=<SubBackward0>)
[2022-11-06 05:22:38.529778] Process 2. Episode 30500, average_reward -0.074984
Episode 30500: Total Loss of tensor([[8.2802]], grad_fn=<SubBackward0>)
[2022-11-06 05:22:55.686618] Process 3. Episode 29850, average_reward -0.072965
Episode 29850: Total Loss of tensor([[26.1979]], grad_fn=<SubBackward0>)
[2022-11-06 05:23:19.532971] Process 0. Episode 29900, average_reward -0.070067
Episode 29900: Total Loss of tensor([[-1.1156]], grad_fn=<SubBackward0>)
[2022-11-06 05:24:01.195175] Process 4. Episode 31850, average_reward -0.071429
Episode 31850: Total Loss of tensor([[11.6746]], grad_fn=<SubBackward0>)
[2022-11-06 05:24:12.148800] Process 5. Episode 30200, average_reward -0.069934
Episode 30200: Total Loss of tensor([[10.6509]], grad_fn=<SubBackward0>)
[2022-11-06 05:24:16.715734] Process 1. Episode 29300, average_reward -0.069420
Episode 29300: Total Loss of tensor([[3.0003]], grad_fn=<SubBackward0>)
[2022-11-06 05:25:03.322810] Process 2. Episode 30550, average_reward -0.075090
Episode 30550: Total Loss of tensor([[16.2044]], grad_fn=<SubBackward0>)
[2022-11-06 05:25:22.315323] Process 3. Episode 29900, average_reward -0.073010
Episode 29900: Total Loss of tensor([[17.2103]], grad_fn=<SubBackward0>)
[2022-11-06 05:25:46.778556] Process 0. Episode 29950, average_reward -0.070083
Episode 29950: Total Loss of tensor([[10.1741]], grad_fn=<SubBackward0>)
[2022-11-06 05:26:26.396064] Process 4. Episode 31900, average_reward -0.071411
Episode 31900: Total Loss of tensor([[8.2721]], grad_fn=<SubBackward0>)
[2022-11-06 05:26:41.841688] Process 5. Episode 30250, average_reward -0.069917
Episode 30250: Total Loss of tensor([[10.8857]], grad_fn=<SubBackward0>)
[2022-11-06 05:26:49.439549] Process 1. Episode 29350, average_reward -0.069404
Episode 29350: Total Loss of tensor([[19.5796]], grad_fn=<SubBackward0>)
[2022-11-06 05:27:37.950725] Process 2. Episode 30600, average_reward -0.075098
Episode 30600: Total Loss of tensor([[12.1075]], grad_fn=<SubBackward0>)
[2022-11-06 05:28:02.514656] Process 3. Episode 29950, average_reward -0.072988
Episode 29950: Total Loss of tensor([[6.4247]], grad_fn=<SubBackward0>)
[2022-11-06 05:28:12.005728] Process 0. Episode 30000, average_reward -0.070200
Episode 30000: Total Loss of tensor([[-82.5244]], grad_fn=<SubBackward0>)
[2022-11-06 05:28:50.372990] Process 4. Episode 31950, average_reward -0.071393
Episode 31950: Total Loss of tensor([[9.1185]], grad_fn=<SubBackward0>)
[2022-11-06 05:29:06.479761] Process 5. Episode 30300, average_reward -0.069868
Episode 30300: Total Loss of tensor([[6.5959]], grad_fn=<SubBackward0>)
[2022-11-06 05:29:17.820751] Process 1. Episode 29400, average_reward -0.069354
Episode 29400: Total Loss of tensor([[4.6510]], grad_fn=<SubBackward0>)
[2022-11-06 05:30:11.746761] Process 2. Episode 30650, average_reward -0.075171
Episode 30650: Total Loss of tensor([[10.0292]], grad_fn=<SubBackward0>)
[2022-11-06 05:30:24.783048] Process 3. Episode 30000, average_reward -0.072900
Episode 30000: Total Loss of tensor([[7.0774]], grad_fn=<SubBackward0>)
[2022-11-06 05:30:52.332568] Process 0. Episode 30050, average_reward -0.070216
Episode 30050: Total Loss of tensor([[-59.2569]], grad_fn=<SubBackward0>)
[2022-11-06 05:31:09.557526] Process 4. Episode 32000, average_reward -0.071406
Episode 32000: Total Loss of tensor([[-2.1486]], grad_fn=<SubBackward0>)
[2022-11-06 05:31:37.706900] Process 5. Episode 30350, average_reward -0.069885
Episode 30350: Total Loss of tensor([[1.1013]], grad_fn=<SubBackward0>)
[2022-11-06 05:32:02.593815] Process 1. Episode 29450, average_reward -0.069338
Episode 29450: Total Loss of tensor([[-5.6945]], grad_fn=<SubBackward0>)
[2022-11-06 05:32:39.961083] Process 2. Episode 30700, average_reward -0.075179
Episode 30700: Total Loss of tensor([[2.9377]], grad_fn=<SubBackward0>)
[2022-11-06 05:32:46.810209] Process 3. Episode 30050, average_reward -0.073012
Episode 30050: Total Loss of tensor([[-111.9676]], grad_fn=<SubBackward0>)
[2022-11-06 05:33:31.839408] Process 0. Episode 30100, average_reward -0.070266
Episode 30100: Total Loss of tensor([[-13.1191]], grad_fn=<SubBackward0>)
[2022-11-06 05:33:34.047873] Process 4. Episode 32050, average_reward -0.071451
Episode 32050: Total Loss of tensor([[14.2441]], grad_fn=<SubBackward0>)
[2022-11-06 05:34:07.801464] Process 5. Episode 30400, average_reward -0.069803
Episode 30400: Total Loss of tensor([[15.9011]], grad_fn=<SubBackward0>)
[2022-11-06 05:34:49.547224] Process 1. Episode 29500, average_reward -0.069288
Episode 29500: Total Loss of tensor([[3.7090]], grad_fn=<SubBackward0>)
[2022-11-06 05:35:05.316552] Process 2. Episode 30750, average_reward -0.075252
Episode 30750: Total Loss of tensor([[10.4520]], grad_fn=<SubBackward0>)
[2022-11-06 05:35:10.413989] Process 3. Episode 30100, average_reward -0.073056
Episode 30100: Total Loss of tensor([[-10.0035]], grad_fn=<SubBackward0>)
[2022-11-06 05:35:57.015405] Process 4. Episode 32100, average_reward -0.071371
Episode 32100: Total Loss of tensor([[-1.9801]], grad_fn=<SubBackward0>)
[2022-11-06 05:36:14.237602] Process 0. Episode 30150, average_reward -0.070182
Episode 30150: Total Loss of tensor([[10.1140]], grad_fn=<SubBackward0>)
[2022-11-06 05:36:42.378589] Process 5. Episode 30450, average_reward -0.069852
Episode 30450: Total Loss of tensor([[2.0963]], grad_fn=<SubBackward0>)
[2022-11-06 05:37:29.264682] Process 1. Episode 29550, average_reward -0.069272
Episode 29550: Total Loss of tensor([[14.4153]], grad_fn=<SubBackward0>)
[2022-11-06 05:37:30.536621] Process 2. Episode 30800, average_reward -0.075195
Episode 30800: Total Loss of tensor([[-2.3107]], grad_fn=<SubBackward0>)
[2022-11-06 05:37:37.167089] Process 3. Episode 30150, average_reward -0.072968
Episode 30150: Total Loss of tensor([[10.8255]], grad_fn=<SubBackward0>)
[2022-11-06 05:38:18.158087] Process 4. Episode 32150, average_reward -0.071446
Episode 32150: Total Loss of tensor([[16.6706]], grad_fn=<SubBackward0>)
[2022-11-06 05:38:57.329568] Process 0. Episode 30200, average_reward -0.070331
Episode 30200: Total Loss of tensor([[14.4220]], grad_fn=<SubBackward0>)
[2022-11-06 05:39:14.550420] Process 5. Episode 30500, average_reward -0.070000
Episode 30500: Total Loss of tensor([[23.7748]], grad_fn=<SubBackward0>)
[2022-11-06 05:39:51.192111] Process 2. Episode 30850, average_reward -0.075138
Episode 30850: Total Loss of tensor([[7.1276]], grad_fn=<SubBackward0>)
[2022-11-06 05:40:03.129378] Process 3. Episode 30200, average_reward -0.073013
Episode 30200: Total Loss of tensor([[12.8734]], grad_fn=<SubBackward0>)
[2022-11-06 05:40:06.591185] Process 1. Episode 29600, average_reward -0.069324
Episode 29600: Total Loss of tensor([[7.9735]], grad_fn=<SubBackward0>)
[2022-11-06 05:40:41.802358] Process 4. Episode 32200, average_reward -0.071584
Episode 32200: Total Loss of tensor([[6.8018]], grad_fn=<SubBackward0>)
[2022-11-06 05:41:32.294775] Process 0. Episode 30250, average_reward -0.070248
Episode 30250: Total Loss of tensor([[0.6429]], grad_fn=<SubBackward0>)
[2022-11-06 05:41:43.469313] Process 5. Episode 30550, average_reward -0.069984
Episode 30550: Total Loss of tensor([[-101.0905]], grad_fn=<SubBackward0>)
[2022-11-06 05:42:23.617815] Process 2. Episode 30900, average_reward -0.075178
Episode 30900: Total Loss of tensor([[-121.6225]], grad_fn=<SubBackward0>)
[2022-11-06 05:42:35.507971] Process 3. Episode 30250, average_reward -0.072992
Episode 30250: Total Loss of tensor([[-45.0363]], grad_fn=<SubBackward0>)
[2022-11-06 05:42:45.990491] Process 1. Episode 29650, average_reward -0.069376
Episode 29650: Total Loss of tensor([[-33.3740]], grad_fn=<SubBackward0>)
[2022-11-06 05:43:02.331363] Process 4. Episode 32250, average_reward -0.071690
Episode 32250: Total Loss of tensor([[-75.6970]], grad_fn=<SubBackward0>)
[2022-11-06 05:44:03.298155] Process 5. Episode 30600, average_reward -0.070000
Episode 30600: Total Loss of tensor([[4.9906]], grad_fn=<SubBackward0>)
[2022-11-06 05:44:11.573362] Process 0. Episode 30300, average_reward -0.070198
Episode 30300: Total Loss of tensor([[2.2227]], grad_fn=<SubBackward0>)
[2022-11-06 05:44:55.519537] Process 2. Episode 30950, average_reward -0.075218
Episode 30950: Total Loss of tensor([[-19.7856]], grad_fn=<SubBackward0>)
[2022-11-06 05:45:12.492677] Process 3. Episode 30300, average_reward -0.072937
Episode 30300: Total Loss of tensor([[16.1730]], grad_fn=<SubBackward0>)
[2022-11-06 05:45:16.091056] Process 1. Episode 29700, average_reward -0.069327
Episode 29700: Total Loss of tensor([[3.8073]], grad_fn=<SubBackward0>)
[2022-11-06 05:45:23.232499] Process 4. Episode 32300, average_reward -0.071672
Episode 32300: Total Loss of tensor([[-63.5086]], grad_fn=<SubBackward0>)
[2022-11-06 05:46:32.463683] Process 5. Episode 30650, average_reward -0.069886
Episode 30650: Total Loss of tensor([[5.7711]], grad_fn=<SubBackward0>)
[2022-11-06 05:46:47.233400] Process 0. Episode 30350, average_reward -0.070214
Episode 30350: Total Loss of tensor([[12.2248]], grad_fn=<SubBackward0>)
[2022-11-06 05:47:32.586418] Process 2. Episode 31000, average_reward -0.075161
Episode 31000: Total Loss of tensor([[-4.1786]], grad_fn=<SubBackward0>)
[2022-11-06 05:47:40.030462] Process 3. Episode 30350, average_reward -0.072916
Episode 30350: Total Loss of tensor([[14.9188]], grad_fn=<SubBackward0>)
[2022-11-06 05:47:43.553018] Process 1. Episode 29750, average_reward -0.069244
Episode 29750: Total Loss of tensor([[-0.1912]], grad_fn=<SubBackward0>)
[2022-11-06 05:47:43.999996] Process 4. Episode 32350, average_reward -0.071623
Episode 32350: Total Loss of tensor([[6.5629]], grad_fn=<SubBackward0>)
[2022-11-06 05:49:00.214921] Process 5. Episode 30700, average_reward -0.069837
Episode 30700: Total Loss of tensor([[12.2493]], grad_fn=<SubBackward0>)
[2022-11-06 05:49:29.173892] Process 0. Episode 30400, average_reward -0.070132
Episode 30400: Total Loss of tensor([[6.9031]], grad_fn=<SubBackward0>)
[2022-11-06 05:50:01.416867] Process 4. Episode 32400, average_reward -0.071605
Episode 32400: Total Loss of tensor([[3.7361]], grad_fn=<SubBackward0>)
[2022-11-06 05:50:06.578768] Process 2. Episode 31050, average_reward -0.075169
Episode 31050: Total Loss of tensor([[7.3027]], grad_fn=<SubBackward0>)
[2022-11-06 05:50:10.410993] Process 3. Episode 30400, average_reward -0.072993
Episode 30400: Total Loss of tensor([[-105.8167]], grad_fn=<SubBackward0>)
[2022-11-06 05:50:22.338140] Process 1. Episode 29800, average_reward -0.069195
Episode 29800: Total Loss of tensor([[4.9853]], grad_fn=<SubBackward0>)
[2022-11-06 05:51:20.041412] Process 5. Episode 30750, average_reward -0.069854
Episode 30750: Total Loss of tensor([[4.5609]], grad_fn=<SubBackward0>)
[2022-11-06 05:51:57.639628] Process 0. Episode 30450, average_reward -0.070148
Episode 30450: Total Loss of tensor([[7.8137]], grad_fn=<SubBackward0>)
[2022-11-06 05:52:21.815859] Process 4. Episode 32450, average_reward -0.071525
Episode 32450: Total Loss of tensor([[12.2755]], grad_fn=<SubBackward0>)
[2022-11-06 05:52:44.522460] Process 2. Episode 31100, average_reward -0.075177
Episode 31100: Total Loss of tensor([[-28.5681]], grad_fn=<SubBackward0>)
[2022-11-06 05:52:47.151173] Process 3. Episode 30450, average_reward -0.072939
Episode 30450: Total Loss of tensor([[-0.6309]], grad_fn=<SubBackward0>)
[2022-11-06 05:52:53.127740] Process 1. Episode 29850, average_reward -0.069112
Episode 29850: Total Loss of tensor([[-12.9807]], grad_fn=<SubBackward0>)
[2022-11-06 05:53:54.417396] Process 5. Episode 30800, average_reward -0.069805
Episode 30800: Total Loss of tensor([[8.5850]], grad_fn=<SubBackward0>)
[2022-11-06 05:54:29.840990] Process 0. Episode 30500, average_reward -0.070197
Episode 30500: Total Loss of tensor([[6.5505]], grad_fn=<SubBackward0>)
[2022-11-06 05:54:42.412910] Process 4. Episode 32500, average_reward -0.071569
Episode 32500: Total Loss of tensor([[17.5552]], grad_fn=<SubBackward0>)
[2022-11-06 05:55:15.141385] Process 3. Episode 30500, average_reward -0.072885
Episode 30500: Total Loss of tensor([[5.5587]], grad_fn=<SubBackward0>)
[2022-11-06 05:55:23.191580] Process 1. Episode 29900, average_reward -0.069164
Episode 29900: Total Loss of tensor([[18.7091]], grad_fn=<SubBackward0>)
[2022-11-06 05:55:29.371560] Process 2. Episode 31150, average_reward -0.075217
Episode 31150: Total Loss of tensor([[10.1631]], grad_fn=<SubBackward0>)
[2022-11-06 05:56:22.406872] Process 5. Episode 30850, average_reward -0.069789
Episode 30850: Total Loss of tensor([[0.5027]], grad_fn=<SubBackward0>)
[2022-11-06 05:57:06.387135] Process 4. Episode 32550, average_reward -0.071582
Episode 32550: Total Loss of tensor([[13.0996]], grad_fn=<SubBackward0>)
[2022-11-06 05:57:09.836212] Process 0. Episode 30550, average_reward -0.070147
Episode 30550: Total Loss of tensor([[-115.6345]], grad_fn=<SubBackward0>)
[2022-11-06 05:57:39.979178] Process 3. Episode 30550, average_reward -0.072864
Episode 30550: Total Loss of tensor([[2.1104]], grad_fn=<SubBackward0>)
[2022-11-06 05:57:51.296310] Process 2. Episode 31200, average_reward -0.075128
Episode 31200: Total Loss of tensor([[3.0758]], grad_fn=<SubBackward0>)
[2022-11-06 05:57:57.908603] Process 1. Episode 29950, average_reward -0.069182
Episode 29950: Total Loss of tensor([[7.1189]], grad_fn=<SubBackward0>)
[2022-11-06 05:58:51.077870] Process 5. Episode 30900, average_reward -0.069871
Episode 30900: Total Loss of tensor([[-125.8724]], grad_fn=<SubBackward0>)
[2022-11-06 05:59:30.373778] Process 4. Episode 32600, average_reward -0.071534
Episode 32600: Total Loss of tensor([[0.3395]], grad_fn=<SubBackward0>)
[2022-11-06 05:59:50.705865] Process 0. Episode 30600, average_reward -0.070196
Episode 30600: Total Loss of tensor([[5.8681]], grad_fn=<SubBackward0>)
[2022-11-06 06:00:05.708128] Process 3. Episode 30600, average_reward -0.072876
Episode 30600: Total Loss of tensor([[5.1709]], grad_fn=<SubBackward0>)
[2022-11-06 06:00:16.066321] Process 2. Episode 31250, average_reward -0.075104
Episode 31250: Total Loss of tensor([[13.6919]], grad_fn=<SubBackward0>)
[2022-11-06 06:00:39.027360] Process 1. Episode 30000, average_reward -0.069067
Episode 30000: Total Loss of tensor([[5.4526]], grad_fn=<SubBackward0>)
[2022-11-06 06:01:18.553493] Process 5. Episode 30950, average_reward -0.069952
Episode 30950: Total Loss of tensor([[11.1236]], grad_fn=<SubBackward0>)
[2022-11-06 06:01:53.144431] Process 4. Episode 32650, average_reward -0.071577
Episode 32650: Total Loss of tensor([[5.2186]], grad_fn=<SubBackward0>)
[2022-11-06 06:02:17.180222] Process 0. Episode 30650, average_reward -0.070147
Episode 30650: Total Loss of tensor([[6.3178]], grad_fn=<SubBackward0>)
[2022-11-06 06:02:31.090630] Process 3. Episode 30650, average_reward -0.072822
Episode 30650: Total Loss of tensor([[9.5570]], grad_fn=<SubBackward0>)
[2022-11-06 06:02:46.058388] Process 2. Episode 31300, average_reward -0.075176
Episode 31300: Total Loss of tensor([[2.1089]], grad_fn=<SubBackward0>)
[2022-11-06 06:03:15.578117] Process 1. Episode 30050, average_reward -0.069018
Episode 30050: Total Loss of tensor([[-0.9432]], grad_fn=<SubBackward0>)
[2022-11-06 06:03:58.405801] Process 5. Episode 31000, average_reward -0.069903
Episode 31000: Total Loss of tensor([[-0.9333]], grad_fn=<SubBackward0>)
[2022-11-06 06:04:12.347443] Process 4. Episode 32700, average_reward -0.071621
Episode 32700: Total Loss of tensor([[6.3261]], grad_fn=<SubBackward0>)
[2022-11-06 06:04:58.307688] Process 0. Episode 30700, average_reward -0.070130
Episode 30700: Total Loss of tensor([[-0.2466]], grad_fn=<SubBackward0>)
[2022-11-06 06:05:02.388896] Process 3. Episode 30700, average_reward -0.072834
Episode 30700: Total Loss of tensor([[-2.6734]], grad_fn=<SubBackward0>)
[2022-11-06 06:05:17.480411] Process 2. Episode 31350, average_reward -0.075215
Episode 31350: Total Loss of tensor([[-1.5741]], grad_fn=<SubBackward0>)
[2022-11-06 06:05:51.911214] Process 1. Episode 30100, average_reward -0.069103
Episode 30100: Total Loss of tensor([[-18.0113]], grad_fn=<SubBackward0>)
[2022-11-06 06:06:31.650142] Process 4. Episode 32750, average_reward -0.071695
Episode 32750: Total Loss of tensor([[4.7478]], grad_fn=<SubBackward0>)
[2022-11-06 06:06:32.687343] Process 5. Episode 31050, average_reward -0.069823
Episode 31050: Total Loss of tensor([[-119.1966]], grad_fn=<SubBackward0>)
[2022-11-06 06:07:25.977913] Process 3. Episode 30750, average_reward -0.072748
Episode 30750: Total Loss of tensor([[-2.0027]], grad_fn=<SubBackward0>)
[2022-11-06 06:07:34.756075] Process 0. Episode 30750, average_reward -0.070114
Episode 30750: Total Loss of tensor([[18.7952]], grad_fn=<SubBackward0>)
[2022-11-06 06:07:53.533424] Process 2. Episode 31400, average_reward -0.075159
Episode 31400: Total Loss of tensor([[-4.0244]], grad_fn=<SubBackward0>)
[2022-11-06 06:08:20.683189] Process 1. Episode 30150, average_reward -0.069055
Episode 30150: Total Loss of tensor([[-15.9225]], grad_fn=<SubBackward0>)
[2022-11-06 06:08:53.321187] Process 4. Episode 32800, average_reward -0.071677
Episode 32800: Total Loss of tensor([[1.5707]], grad_fn=<SubBackward0>)
[2022-11-06 06:09:11.757781] Process 5. Episode 31100, average_reward -0.069839
Episode 31100: Total Loss of tensor([[5.3708]], grad_fn=<SubBackward0>)
[2022-11-06 06:09:49.943349] Process 3. Episode 30800, average_reward -0.072630
Episode 30800: Total Loss of tensor([[0.5140]], grad_fn=<SubBackward0>)
[2022-11-06 06:10:13.569066] Process 0. Episode 30800, average_reward -0.070162
Episode 30800: Total Loss of tensor([[12.5857]], grad_fn=<SubBackward0>)
[2022-11-06 06:10:20.417797] Process 2. Episode 31450, average_reward -0.075167
Episode 31450: Total Loss of tensor([[-3.5606]], grad_fn=<SubBackward0>)
[2022-11-06 06:10:49.708871] Process 1. Episode 30200, average_reward -0.069040
Episode 30200: Total Loss of tensor([[12.9813]], grad_fn=<SubBackward0>)
[2022-11-06 06:11:23.419524] Process 4. Episode 32850, average_reward -0.071689
Episode 32850: Total Loss of tensor([[11.0078]], grad_fn=<SubBackward0>)
[2022-11-06 06:11:32.906187] Process 5. Episode 31150, average_reward -0.069791
Episode 31150: Total Loss of tensor([[-0.7536]], grad_fn=<SubBackward0>)
[2022-11-06 06:12:13.221178] Process 3. Episode 30850, average_reward -0.072577
Episode 30850: Total Loss of tensor([[6.4352]], grad_fn=<SubBackward0>)
[2022-11-06 06:12:48.241957] Process 0. Episode 30850, average_reward -0.070276
Episode 30850: Total Loss of tensor([[-17.5525]], grad_fn=<SubBackward0>)
[2022-11-06 06:12:56.384553] Process 2. Episode 31500, average_reward -0.075111
Episode 31500: Total Loss of tensor([[-6.6365]], grad_fn=<SubBackward0>)
[2022-11-06 06:13:19.990485] Process 1. Episode 30250, average_reward -0.069091
Episode 30250: Total Loss of tensor([[-52.6292]], grad_fn=<SubBackward0>)
[2022-11-06 06:13:46.534015] Process 4. Episode 32900, average_reward -0.071641
Episode 32900: Total Loss of tensor([[-2.8870]], grad_fn=<SubBackward0>)
[2022-11-06 06:13:57.926255] Process 5. Episode 31200, average_reward -0.069840
Episode 31200: Total Loss of tensor([[9.1976]], grad_fn=<SubBackward0>)
[2022-11-06 06:14:47.350587] Process 3. Episode 30900, average_reward -0.072686
Episode 30900: Total Loss of tensor([[-5.3123]], grad_fn=<SubBackward0>)
[2022-11-06 06:15:21.696046] Process 0. Episode 30900, average_reward -0.070227
Episode 30900: Total Loss of tensor([[-127.6496]], grad_fn=<SubBackward0>)
[2022-11-06 06:15:23.061838] Process 2. Episode 31550, average_reward -0.075055
Episode 31550: Total Loss of tensor([[1.6656]], grad_fn=<SubBackward0>)
[2022-11-06 06:15:55.385788] Process 1. Episode 30300, average_reward -0.069043
Episode 30300: Total Loss of tensor([[1.3266]], grad_fn=<SubBackward0>)
[2022-11-06 06:16:08.826642] Process 4. Episode 32950, average_reward -0.071654
Episode 32950: Total Loss of tensor([[-124.0081]], grad_fn=<SubBackward0>)
[2022-11-06 06:16:24.984695] Process 5. Episode 31250, average_reward -0.069760
Episode 31250: Total Loss of tensor([[16.8762]], grad_fn=<SubBackward0>)
[2022-11-06 06:17:18.966213] Process 3. Episode 30950, average_reward -0.072730
Episode 30950: Total Loss of tensor([[-30.6223]], grad_fn=<SubBackward0>)
[2022-11-06 06:17:51.491023] Process 2. Episode 31600, average_reward -0.075032
Episode 31600: Total Loss of tensor([[-20.0328]], grad_fn=<SubBackward0>)
[2022-11-06 06:17:58.817825] Process 0. Episode 30950, average_reward -0.070242
Episode 30950: Total Loss of tensor([[-105.3875]], grad_fn=<SubBackward0>)
[2022-11-06 06:18:29.645684] Process 4. Episode 33000, average_reward -0.071636
Episode 33000: Total Loss of tensor([[10.7009]], grad_fn=<SubBackward0>)
[2022-11-06 06:18:32.482933] Process 1. Episode 30350, average_reward -0.069061
Episode 30350: Total Loss of tensor([[1.5948]], grad_fn=<SubBackward0>)
[2022-11-06 06:18:59.002653] Process 5. Episode 31300, average_reward -0.069712
Episode 31300: Total Loss of tensor([[9.2071]], grad_fn=<SubBackward0>)
[2022-11-06 06:19:46.132717] Process 3. Episode 31000, average_reward -0.072710
Episode 31000: Total Loss of tensor([[-120.1113]], grad_fn=<SubBackward0>)
[2022-11-06 06:20:20.206006] Process 2. Episode 31650, average_reward -0.075103
Episode 31650: Total Loss of tensor([[3.0572]], grad_fn=<SubBackward0>)
[2022-11-06 06:20:26.629070] Process 0. Episode 31000, average_reward -0.070226
Episode 31000: Total Loss of tensor([[11.8225]], grad_fn=<SubBackward0>)
[2022-11-06 06:20:50.119017] Process 4. Episode 33050, average_reward -0.071800
Episode 33050: Total Loss of tensor([[7.4872]], grad_fn=<SubBackward0>)
[2022-11-06 06:21:08.333782] Process 1. Episode 30400, average_reward -0.068980
Episode 30400: Total Loss of tensor([[3.6849]], grad_fn=<SubBackward0>)
[2022-11-06 06:21:36.011808] Process 5. Episode 31350, average_reward -0.069665
Episode 31350: Total Loss of tensor([[-2.6313]], grad_fn=<SubBackward0>)
[2022-11-06 06:22:11.042925] Process 3. Episode 31050, average_reward -0.072689
Episode 31050: Total Loss of tensor([[3.3854]], grad_fn=<SubBackward0>)
[2022-11-06 06:22:44.853613] Process 2. Episode 31700, average_reward -0.075079
Episode 31700: Total Loss of tensor([[10.2683]], grad_fn=<SubBackward0>)
[2022-11-06 06:22:56.556786] Process 0. Episode 31050, average_reward -0.070177
Episode 31050: Total Loss of tensor([[1.1330]], grad_fn=<SubBackward0>)
[2022-11-06 06:23:12.052344] Process 4. Episode 33100, average_reward -0.071813
Episode 33100: Total Loss of tensor([[25.7062]], grad_fn=<SubBackward0>)
[2022-11-06 06:23:49.377997] Process 1. Episode 30450, average_reward -0.068966
Episode 30450: Total Loss of tensor([[0.4980]], grad_fn=<SubBackward0>)
[2022-11-06 06:24:13.196172] Process 5. Episode 31400, average_reward -0.069650
Episode 31400: Total Loss of tensor([[-0.5151]], grad_fn=<SubBackward0>)
[2022-11-06 06:24:42.415312] Process 3. Episode 31100, average_reward -0.072605
Episode 31100: Total Loss of tensor([[6.0540]], grad_fn=<SubBackward0>)
[2022-11-06 06:25:15.658924] Process 2. Episode 31750, average_reward -0.075118
Episode 31750: Total Loss of tensor([[5.1591]], grad_fn=<SubBackward0>)
[2022-11-06 06:25:30.442496] Process 4. Episode 33150, average_reward -0.071795
Episode 33150: Total Loss of tensor([[3.1217]], grad_fn=<SubBackward0>)
[2022-11-06 06:25:38.742341] Process 0. Episode 31100, average_reward -0.070064
Episode 31100: Total Loss of tensor([[-2.5897]], grad_fn=<SubBackward0>)
[2022-11-06 06:26:19.167806] Process 1. Episode 30500, average_reward -0.068951
Episode 30500: Total Loss of tensor([[8.8473]], grad_fn=<SubBackward0>)
[2022-11-06 06:26:40.498408] Process 5. Episode 31450, average_reward -0.069634
Episode 31450: Total Loss of tensor([[9.8032]], grad_fn=<SubBackward0>)
[2022-11-06 06:27:07.683230] Process 3. Episode 31150, average_reward -0.072616
Episode 31150: Total Loss of tensor([[15.2428]], grad_fn=<SubBackward0>)
[2022-11-06 06:27:52.514674] Process 4. Episode 33200, average_reward -0.071717
Episode 33200: Total Loss of tensor([[5.6793]], grad_fn=<SubBackward0>)
[2022-11-06 06:27:58.282934] Process 2. Episode 31800, average_reward -0.075126
Episode 31800: Total Loss of tensor([[-94.1929]], grad_fn=<SubBackward0>)
[2022-11-06 06:28:19.035211] Process 0. Episode 31150, average_reward -0.070048
Episode 31150: Total Loss of tensor([[7.9793]], grad_fn=<SubBackward0>)
[2022-11-06 06:28:56.524009] Process 1. Episode 30550, average_reward -0.068936
Episode 30550: Total Loss of tensor([[8.3444]], grad_fn=<SubBackward0>)
[2022-11-06 06:29:06.880105] Process 5. Episode 31500, average_reward -0.069619
Episode 31500: Total Loss of tensor([[-2.8363]], grad_fn=<SubBackward0>)
[2022-11-06 06:29:39.207029] Process 3. Episode 31200, average_reward -0.072660
Episode 31200: Total Loss of tensor([[10.7415]], grad_fn=<SubBackward0>)
[2022-11-06 06:30:16.489961] Process 4. Episode 33250, average_reward -0.071669
Episode 33250: Total Loss of tensor([[2.7286]], grad_fn=<SubBackward0>)
[2022-11-06 06:30:22.931954] Process 2. Episode 31850, average_reward -0.075102
Episode 31850: Total Loss of tensor([[2.5698]], grad_fn=<SubBackward0>)
[2022-11-06 06:30:45.836022] Process 0. Episode 31200, average_reward -0.070032
Episode 31200: Total Loss of tensor([[11.4265]], grad_fn=<SubBackward0>)
[2022-11-06 06:31:27.411067] Process 1. Episode 30600, average_reward -0.068856
Episode 30600: Total Loss of tensor([[5.1481]], grad_fn=<SubBackward0>)
[2022-11-06 06:31:46.252709] Process 5. Episode 31550, average_reward -0.069635
Episode 31550: Total Loss of tensor([[1.7529]], grad_fn=<SubBackward0>)
[2022-11-06 06:32:05.099427] Process 3. Episode 31250, average_reward -0.072704
Episode 31250: Total Loss of tensor([[7.2978]], grad_fn=<SubBackward0>)
[2022-11-06 06:32:43.136139] Process 4. Episode 33300, average_reward -0.071652
Episode 33300: Total Loss of tensor([[13.2813]], grad_fn=<SubBackward0>)
[2022-11-06 06:32:49.462101] Process 2. Episode 31900, average_reward -0.075141
Episode 31900: Total Loss of tensor([[8.1749]], grad_fn=<SubBackward0>)
[2022-11-06 06:33:21.952569] Process 0. Episode 31250, average_reward -0.070016
Episode 31250: Total Loss of tensor([[11.8614]], grad_fn=<SubBackward0>)
[2022-11-06 06:33:55.230476] Process 1. Episode 30650, average_reward -0.068809
Episode 30650: Total Loss of tensor([[14.8914]], grad_fn=<SubBackward0>)
[2022-11-06 06:34:20.079622] Process 5. Episode 31600, average_reward -0.069684
Episode 31600: Total Loss of tensor([[1.9956]], grad_fn=<SubBackward0>)
[2022-11-06 06:34:38.950606] Process 3. Episode 31300, average_reward -0.072652
Episode 31300: Total Loss of tensor([[-0.9636]], grad_fn=<SubBackward0>)
[2022-11-06 06:35:04.185262] Process 4. Episode 33350, average_reward -0.071664
Episode 33350: Total Loss of tensor([[27.4932]], grad_fn=<SubBackward0>)
[2022-11-06 06:35:11.331460] Process 2. Episode 31950, average_reward -0.075086
Episode 31950: Total Loss of tensor([[9.7553]], grad_fn=<SubBackward0>)
[2022-11-06 06:35:52.122153] Process 0. Episode 31300, average_reward -0.070000
Episode 31300: Total Loss of tensor([[-34.1926]], grad_fn=<SubBackward0>)
[2022-11-06 06:36:26.363907] Process 1. Episode 30700, average_reward -0.068795
Episode 30700: Total Loss of tensor([[8.9790]], grad_fn=<SubBackward0>)
[2022-11-06 06:37:00.343386] Process 5. Episode 31650, average_reward -0.069700
Episode 31650: Total Loss of tensor([[10.3873]], grad_fn=<SubBackward0>)
[2022-11-06 06:37:19.421090] Process 3. Episode 31350, average_reward -0.072600
Episode 31350: Total Loss of tensor([[22.1605]], grad_fn=<SubBackward0>)
[2022-11-06 06:37:24.434970] Process 4. Episode 33400, average_reward -0.071707
Episode 33400: Total Loss of tensor([[5.6469]], grad_fn=<SubBackward0>)
[2022-11-06 06:37:35.083342] Process 2. Episode 32000, average_reward -0.075063
Episode 32000: Total Loss of tensor([[3.4235]], grad_fn=<SubBackward0>)
[2022-11-06 06:38:26.163662] Process 0. Episode 31350, average_reward -0.069920
Episode 31350: Total Loss of tensor([[3.7512]], grad_fn=<SubBackward0>)
[2022-11-06 06:39:04.726388] Process 1. Episode 30750, average_reward -0.068846
Episode 30750: Total Loss of tensor([[6.2269]], grad_fn=<SubBackward0>)
[2022-11-06 06:39:29.587274] Process 5. Episode 31700, average_reward -0.069716
Episode 31700: Total Loss of tensor([[4.5481]], grad_fn=<SubBackward0>)
[2022-11-06 06:39:49.129802] Process 4. Episode 33450, average_reward -0.071689
Episode 33450: Total Loss of tensor([[12.1787]], grad_fn=<SubBackward0>)
[2022-11-06 06:39:52.079068] Process 3. Episode 31400, average_reward -0.072580
Episode 31400: Total Loss of tensor([[3.6769]], grad_fn=<SubBackward0>)
[2022-11-06 06:40:02.067144] Process 2. Episode 32050, average_reward -0.074945
Episode 32050: Total Loss of tensor([[-1.9608]], grad_fn=<SubBackward0>)
[2022-11-06 06:40:53.569500] Process 0. Episode 31400, average_reward -0.070000
Episode 31400: Total Loss of tensor([[5.0407]], grad_fn=<SubBackward0>)
[2022-11-06 06:41:40.278194] Process 1. Episode 30800, average_reward -0.068831
Episode 30800: Total Loss of tensor([[8.4663]], grad_fn=<SubBackward0>)
[2022-11-06 06:42:04.077412] Process 5. Episode 31750, average_reward -0.069638
Episode 31750: Total Loss of tensor([[5.0308]], grad_fn=<SubBackward0>)
[2022-11-06 06:42:10.891434] Process 4. Episode 33500, average_reward -0.071701
Episode 33500: Total Loss of tensor([[2.5664]], grad_fn=<SubBackward0>)
[2022-11-06 06:42:16.931000] Process 3. Episode 31450, average_reward -0.072560
Episode 31450: Total Loss of tensor([[5.4376]], grad_fn=<SubBackward0>)
[2022-11-06 06:42:33.793766] Process 2. Episode 32100, average_reward -0.074860
Episode 32100: Total Loss of tensor([[1.4352]], grad_fn=<SubBackward0>)
[2022-11-06 06:43:32.425108] Process 0. Episode 31450, average_reward -0.069921
Episode 31450: Total Loss of tensor([[8.1158]], grad_fn=<SubBackward0>)
[2022-11-06 06:44:12.672383] Process 1. Episode 30850, average_reward -0.068817
Episode 30850: Total Loss of tensor([[10.7863]], grad_fn=<SubBackward0>)
[2022-11-06 06:44:23.993852] Process 5. Episode 31800, average_reward -0.069560
Episode 31800: Total Loss of tensor([[5.2133]], grad_fn=<SubBackward0>)
[2022-11-06 06:44:32.160217] Process 4. Episode 33550, average_reward -0.071714
Episode 33550: Total Loss of tensor([[-25.0233]], grad_fn=<SubBackward0>)
[2022-11-06 06:44:52.773315] Process 3. Episode 31500, average_reward -0.072444
Episode 31500: Total Loss of tensor([[0.2268]], grad_fn=<SubBackward0>)
[2022-11-06 06:44:57.564837] Process 2. Episode 32150, average_reward -0.074930
Episode 32150: Total Loss of tensor([[2.1431]], grad_fn=<SubBackward0>)
[2022-11-06 06:46:11.235306] Process 0. Episode 31500, average_reward -0.069905
Episode 31500: Total Loss of tensor([[-5.2489]], grad_fn=<SubBackward0>)
[2022-11-06 06:46:39.257545] Process 1. Episode 30900, average_reward -0.068770
Episode 30900: Total Loss of tensor([[6.7475]], grad_fn=<SubBackward0>)
[2022-11-06 06:46:46.350348] Process 5. Episode 31850, average_reward -0.069513
Episode 31850: Total Loss of tensor([[4.6234]], grad_fn=<SubBackward0>)
[2022-11-06 06:46:55.966009] Process 4. Episode 33600, average_reward -0.071667
Episode 33600: Total Loss of tensor([[4.5542]], grad_fn=<SubBackward0>)
[2022-11-06 06:47:28.198577] Process 3. Episode 31550, average_reward -0.072393
Episode 31550: Total Loss of tensor([[1.6203]], grad_fn=<SubBackward0>)
[2022-11-06 06:47:30.626863] Process 2. Episode 32200, average_reward -0.074876
Episode 32200: Total Loss of tensor([[11.0770]], grad_fn=<SubBackward0>)
[2022-11-06 06:48:38.869847] Process 0. Episode 31550, average_reward -0.069921
Episode 31550: Total Loss of tensor([[8.7367]], grad_fn=<SubBackward0>)
[2022-11-06 06:49:10.253066] Process 1. Episode 30950, average_reward -0.068691
Episode 30950: Total Loss of tensor([[12.8374]], grad_fn=<SubBackward0>)
[2022-11-06 06:49:19.260915] Process 4. Episode 33650, average_reward -0.071649
Episode 33650: Total Loss of tensor([[-3.5069]], grad_fn=<SubBackward0>)
[2022-11-06 06:49:21.218167] Process 5. Episode 31900, average_reward -0.069467
Episode 31900: Total Loss of tensor([[12.9637]], grad_fn=<SubBackward0>)
[2022-11-06 06:49:58.024433] Process 3. Episode 31600, average_reward -0.072468
Episode 31600: Total Loss of tensor([[-124.1640]], grad_fn=<SubBackward0>)
[2022-11-06 06:50:01.606881] Process 2. Episode 32250, average_reward -0.074853
Episode 32250: Total Loss of tensor([[13.6746]], grad_fn=<SubBackward0>)
[2022-11-06 06:51:08.481536] Process 0. Episode 31600, average_reward -0.069937
Episode 31600: Total Loss of tensor([[6.6211]], grad_fn=<SubBackward0>)
[2022-11-06 06:51:41.266293] Process 1. Episode 31000, average_reward -0.068613
Episode 31000: Total Loss of tensor([[18.9667]], grad_fn=<SubBackward0>)
[2022-11-06 06:51:42.682707] Process 4. Episode 33700, average_reward -0.071691
Episode 33700: Total Loss of tensor([[10.7737]], grad_fn=<SubBackward0>)
[2022-11-06 06:51:55.066669] Process 5. Episode 31950, average_reward -0.069421
Episode 31950: Total Loss of tensor([[6.1987]], grad_fn=<SubBackward0>)
[2022-11-06 06:52:38.503162] Process 2. Episode 32300, average_reward -0.074923
Episode 32300: Total Loss of tensor([[10.7269]], grad_fn=<SubBackward0>)
[2022-11-06 06:52:40.654390] Process 3. Episode 31650, average_reward -0.072449
Episode 31650: Total Loss of tensor([[7.3569]], grad_fn=<SubBackward0>)
[2022-11-06 06:53:38.188462] Process 0. Episode 31650, average_reward -0.069858
Episode 31650: Total Loss of tensor([[3.5798]], grad_fn=<SubBackward0>)
[2022-11-06 06:54:03.541004] Process 4. Episode 33750, average_reward -0.071733
Episode 33750: Total Loss of tensor([[-10.1811]], grad_fn=<SubBackward0>)
[2022-11-06 06:54:19.591990] Process 1. Episode 31050, average_reward -0.068631
Episode 31050: Total Loss of tensor([[3.9735]], grad_fn=<SubBackward0>)
[2022-11-06 06:54:26.852295] Process 5. Episode 32000, average_reward -0.069375
Episode 32000: Total Loss of tensor([[-11.7883]], grad_fn=<SubBackward0>)
[2022-11-06 06:55:03.576940] Process 2. Episode 32350, average_reward -0.074961
Episode 32350: Total Loss of tensor([[6.0160]], grad_fn=<SubBackward0>)
[2022-11-06 06:55:14.148153] Process 3. Episode 31700, average_reward -0.072429
Episode 31700: Total Loss of tensor([[19.4724]], grad_fn=<SubBackward0>)
[2022-11-06 06:56:06.882400] Process 0. Episode 31700, average_reward -0.069842
Episode 31700: Total Loss of tensor([[16.0530]], grad_fn=<SubBackward0>)
[2022-11-06 06:56:23.637588] Process 4. Episode 33800, average_reward -0.071775
Episode 33800: Total Loss of tensor([[-0.9373]], grad_fn=<SubBackward0>)
[2022-11-06 06:56:59.254439] Process 5. Episode 32050, average_reward -0.069485
Episode 32050: Total Loss of tensor([[-0.5844]], grad_fn=<SubBackward0>)
[2022-11-06 06:57:02.768216] Process 1. Episode 31100, average_reward -0.068553
Episode 31100: Total Loss of tensor([[1.2333]], grad_fn=<SubBackward0>)
[2022-11-06 06:57:25.035750] Process 2. Episode 32400, average_reward -0.075000
Episode 32400: Total Loss of tensor([[6.5234]], grad_fn=<SubBackward0>)
[2022-11-06 06:57:37.701460] Process 3. Episode 31750, average_reward -0.072283
Episode 31750: Total Loss of tensor([[3.1821]], grad_fn=<SubBackward0>)
[2022-11-06 06:58:38.891085] Process 0. Episode 31750, average_reward -0.069795
Episode 31750: Total Loss of tensor([[-13.4290]], grad_fn=<SubBackward0>)
[2022-11-06 06:58:48.146601] Process 4. Episode 33850, average_reward -0.071728
Episode 33850: Total Loss of tensor([[-21.1179]], grad_fn=<SubBackward0>)
[2022-11-06 06:59:27.780590] Process 5. Episode 32100, average_reward -0.069470
Episode 32100: Total Loss of tensor([[7.4701]], grad_fn=<SubBackward0>)
[2022-11-06 06:59:40.316139] Process 1. Episode 31150, average_reward -0.068539
Episode 31150: Total Loss of tensor([[-1.8029]], grad_fn=<SubBackward0>)
[2022-11-06 06:59:48.602777] Process 2. Episode 32450, average_reward -0.074915
Episode 32450: Total Loss of tensor([[15.4195]], grad_fn=<SubBackward0>)
[2022-11-06 07:00:07.026054] Process 3. Episode 31800, average_reward -0.072390
Episode 31800: Total Loss of tensor([[2.7594]], grad_fn=<SubBackward0>)
[2022-11-06 07:01:06.859984] Process 0. Episode 31800, average_reward -0.069748
Episode 31800: Total Loss of tensor([[10.8358]], grad_fn=<SubBackward0>)
[2022-11-06 07:01:11.273588] Process 4. Episode 33900, average_reward -0.071740
Episode 33900: Total Loss of tensor([[-8.1578]], grad_fn=<SubBackward0>)
[2022-11-06 07:01:56.739761] Process 5. Episode 32150, average_reward -0.069487
Episode 32150: Total Loss of tensor([[5.7631]], grad_fn=<SubBackward0>)
[2022-11-06 07:02:14.312724] Process 2. Episode 32500, average_reward -0.074892
Episode 32500: Total Loss of tensor([[-3.6135]], grad_fn=<SubBackward0>)
[2022-11-06 07:02:25.314441] Process 1. Episode 31200, average_reward -0.068654
Episode 31200: Total Loss of tensor([[10.0047]], grad_fn=<SubBackward0>)
[2022-11-06 07:02:33.937911] Process 3. Episode 31850, average_reward -0.072370
Episode 31850: Total Loss of tensor([[12.9468]], grad_fn=<SubBackward0>)
[2022-11-06 07:03:31.234883] Process 4. Episode 33950, average_reward -0.071694
Episode 33950: Total Loss of tensor([[-109.1522]], grad_fn=<SubBackward0>)
[2022-11-06 07:03:49.275870] Process 0. Episode 31850, average_reward -0.069702
Episode 31850: Total Loss of tensor([[10.5899]], grad_fn=<SubBackward0>)
[2022-11-06 07:04:20.127654] Process 5. Episode 32200, average_reward -0.069534
Episode 32200: Total Loss of tensor([[-13.6796]], grad_fn=<SubBackward0>)
[2022-11-06 07:04:40.367444] Process 2. Episode 32550, average_reward -0.074900
Episode 32550: Total Loss of tensor([[4.0514]], grad_fn=<SubBackward0>)
[2022-11-06 07:05:02.974084] Process 1. Episode 31250, average_reward -0.068640
Episode 31250: Total Loss of tensor([[13.1110]], grad_fn=<SubBackward0>)
[2022-11-06 07:05:12.032032] Process 3. Episode 31900, average_reward -0.072320
Episode 31900: Total Loss of tensor([[-3.2436]], grad_fn=<SubBackward0>)
[2022-11-06 07:05:52.436193] Process 4. Episode 34000, average_reward -0.071676
Episode 34000: Total Loss of tensor([[9.2811]], grad_fn=<SubBackward0>)
[2022-11-06 07:06:23.183346] Process 0. Episode 31900, average_reward -0.069655
Episode 31900: Total Loss of tensor([[7.1122]], grad_fn=<SubBackward0>)
[2022-11-06 07:06:44.744431] Process 5. Episode 32250, average_reward -0.069488
Episode 32250: Total Loss of tensor([[-27.0284]], grad_fn=<SubBackward0>)
[2022-11-06 07:07:04.274432] Process 2. Episode 32600, average_reward -0.074847
Episode 32600: Total Loss of tensor([[9.9176]], grad_fn=<SubBackward0>)
[2022-11-06 07:07:41.531312] Process 3. Episode 31950, average_reward -0.072238
Episode 31950: Total Loss of tensor([[15.8780]], grad_fn=<SubBackward0>)
[2022-11-06 07:07:48.893727] Process 1. Episode 31300, average_reward -0.068690
Episode 31300: Total Loss of tensor([[-0.5058]], grad_fn=<SubBackward0>)
[2022-11-06 07:08:14.831576] Process 4. Episode 34050, average_reward -0.071630
Episode 34050: Total Loss of tensor([[2.9557]], grad_fn=<SubBackward0>)
[2022-11-06 07:08:59.209070] Process 0. Episode 31950, average_reward -0.069640
Episode 31950: Total Loss of tensor([[7.9935]], grad_fn=<SubBackward0>)
[2022-11-06 07:09:16.454843] Process 5. Episode 32300, average_reward -0.069598
Episode 32300: Total Loss of tensor([[6.5297]], grad_fn=<SubBackward0>)
[2022-11-06 07:09:27.156233] Process 2. Episode 32650, average_reward -0.074855
Episode 32650: Total Loss of tensor([[10.5942]], grad_fn=<SubBackward0>)
[2022-11-06 07:10:05.070212] Process 3. Episode 32000, average_reward -0.072125
Episode 32000: Total Loss of tensor([[0.7256]], grad_fn=<SubBackward0>)
[2022-11-06 07:10:32.551717] Process 1. Episode 31350, average_reward -0.068676
Episode 31350: Total Loss of tensor([[11.6602]], grad_fn=<SubBackward0>)
[2022-11-06 07:10:42.578899] Process 4. Episode 34100, average_reward -0.071730
Episode 34100: Total Loss of tensor([[6.0396]], grad_fn=<SubBackward0>)
[2022-11-06 07:11:41.777945] Process 0. Episode 32000, average_reward -0.069750
Episode 32000: Total Loss of tensor([[3.3589]], grad_fn=<SubBackward0>)
[2022-11-06 07:11:44.584428] Process 2. Episode 32700, average_reward -0.074893
Episode 32700: Total Loss of tensor([[-40.6257]], grad_fn=<SubBackward0>)
[2022-11-06 07:11:51.756118] Process 5. Episode 32350, average_reward -0.069614
Episode 32350: Total Loss of tensor([[10.6147]], grad_fn=<SubBackward0>)
[2022-11-06 07:12:28.842453] Process 3. Episode 32050, average_reward -0.072137
Episode 32050: Total Loss of tensor([[12.6815]], grad_fn=<SubBackward0>)
[2022-11-06 07:13:05.253082] Process 1. Episode 31400, average_reward -0.068662
Episode 31400: Total Loss of tensor([[-7.6774]], grad_fn=<SubBackward0>)
[2022-11-06 07:13:21.680735] Process 4. Episode 34150, average_reward -0.071859
Episode 34150: Total Loss of tensor([[14.2910]], grad_fn=<SubBackward0>)
[2022-11-06 07:14:14.591692] Process 2. Episode 32750, average_reward -0.074870
Episode 32750: Total Loss of tensor([[6.6960]], grad_fn=<SubBackward0>)
[2022-11-06 07:14:15.541637] Process 5. Episode 32400, average_reward -0.069630
Episode 32400: Total Loss of tensor([[12.2233]], grad_fn=<SubBackward0>)
[2022-11-06 07:14:16.543437] Process 0. Episode 32050, average_reward -0.069797
Episode 32050: Total Loss of tensor([[18.2832]], grad_fn=<SubBackward0>)
[2022-11-06 07:14:56.770470] Process 3. Episode 32100, average_reward -0.072243
Episode 32100: Total Loss of tensor([[8.8529]], grad_fn=<SubBackward0>)
[2022-11-06 07:15:33.822821] Process 1. Episode 31450, average_reward -0.068712
Episode 31450: Total Loss of tensor([[7.0202]], grad_fn=<SubBackward0>)
[2022-11-06 07:15:46.851719] Process 4. Episode 34200, average_reward -0.071813
Episode 34200: Total Loss of tensor([[2.3986]], grad_fn=<SubBackward0>)
[2022-11-06 07:16:34.302404] Process 2. Episode 32800, average_reward -0.074817
Episode 32800: Total Loss of tensor([[1.2878]], grad_fn=<SubBackward0>)
[2022-11-06 07:16:46.230719] Process 5. Episode 32450, average_reward -0.069676
Episode 32450: Total Loss of tensor([[19.3163]], grad_fn=<SubBackward0>)
[2022-11-06 07:16:55.502914] Process 0. Episode 32100, average_reward -0.069782
Episode 32100: Total Loss of tensor([[-3.1918]], grad_fn=<SubBackward0>)
[2022-11-06 07:17:27.431098] Process 3. Episode 32150, average_reward -0.072162
Episode 32150: Total Loss of tensor([[16.0886]], grad_fn=<SubBackward0>)
[2022-11-06 07:18:07.885714] Process 4. Episode 34250, average_reward -0.071854
Episode 34250: Total Loss of tensor([[-98.2836]], grad_fn=<SubBackward0>)
[2022-11-06 07:18:13.464727] Process 1. Episode 31500, average_reward -0.068698
Episode 31500: Total Loss of tensor([[-2.2476]], grad_fn=<SubBackward0>)
[2022-11-06 07:19:08.447491] Process 2. Episode 32850, average_reward -0.074886
Episode 32850: Total Loss of tensor([[20.4885]], grad_fn=<SubBackward0>)
[2022-11-06 07:19:12.180821] Process 5. Episode 32500, average_reward -0.069631
Episode 32500: Total Loss of tensor([[11.4281]], grad_fn=<SubBackward0>)
[2022-11-06 07:19:32.471017] Process 0. Episode 32150, average_reward -0.069767
Episode 32150: Total Loss of tensor([[12.6146]], grad_fn=<SubBackward0>)
[2022-11-06 07:19:58.008034] Process 3. Episode 32200, average_reward -0.072298
Episode 32200: Total Loss of tensor([[-81.2094]], grad_fn=<SubBackward0>)
[2022-11-06 07:20:29.659130] Process 4. Episode 34300, average_reward -0.071808
Episode 34300: Total Loss of tensor([[3.4856]], grad_fn=<SubBackward0>)
[2022-11-06 07:20:47.309187] Process 1. Episode 31550, average_reward -0.068621
Episode 31550: Total Loss of tensor([[-13.6006]], grad_fn=<SubBackward0>)
[2022-11-06 07:21:33.044436] Process 2. Episode 32900, average_reward -0.074833
Episode 32900: Total Loss of tensor([[-1.4049]], grad_fn=<SubBackward0>)
[2022-11-06 07:21:34.434140] Process 5. Episode 32550, average_reward -0.069524
Episode 32550: Total Loss of tensor([[0.4167]], grad_fn=<SubBackward0>)
[2022-11-06 07:22:06.392397] Process 0. Episode 32200, average_reward -0.069783
Episode 32200: Total Loss of tensor([[-32.2763]], grad_fn=<SubBackward0>)
[2022-11-06 07:22:26.296238] Process 3. Episode 32250, average_reward -0.072279
Episode 32250: Total Loss of tensor([[5.4939]], grad_fn=<SubBackward0>)
[2022-11-06 07:22:51.528619] Process 4. Episode 34350, average_reward -0.071703
Episode 34350: Total Loss of tensor([[0.3392]], grad_fn=<SubBackward0>)
[2022-11-06 07:23:27.897831] Process 1. Episode 31600, average_reward -0.068608
Episode 31600: Total Loss of tensor([[1.2570]], grad_fn=<SubBackward0>)
[2022-11-06 07:24:00.184181] Process 5. Episode 32600, average_reward -0.069571
Episode 32600: Total Loss of tensor([[-0.9387]], grad_fn=<SubBackward0>)
[2022-11-06 07:24:04.379621] Process 2. Episode 32950, average_reward -0.074841
Episode 32950: Total Loss of tensor([[-3.8803]], grad_fn=<SubBackward0>)
[2022-11-06 07:24:36.986905] Process 0. Episode 32250, average_reward -0.069829
Episode 32250: Total Loss of tensor([[-96.1222]], grad_fn=<SubBackward0>)
[2022-11-06 07:24:52.852122] Process 3. Episode 32300, average_reward -0.072322
Episode 32300: Total Loss of tensor([[2.6392]], grad_fn=<SubBackward0>)
[2022-11-06 07:25:13.955471] Process 4. Episode 34400, average_reward -0.071657
Episode 34400: Total Loss of tensor([[6.3883]], grad_fn=<SubBackward0>)
[2022-11-06 07:25:58.584736] Process 1. Episode 31650, average_reward -0.068657
Episode 31650: Total Loss of tensor([[6.6838]], grad_fn=<SubBackward0>)
[2022-11-06 07:26:29.678522] Process 2. Episode 33000, average_reward -0.074818
Episode 33000: Total Loss of tensor([[15.9494]], grad_fn=<SubBackward0>)
[2022-11-06 07:26:35.281481] Process 5. Episode 32650, average_reward -0.069587
Episode 32650: Total Loss of tensor([[3.1276]], grad_fn=<SubBackward0>)
[2022-11-06 07:27:03.640324] Process 0. Episode 32300, average_reward -0.069814
Episode 32300: Total Loss of tensor([[1.3622]], grad_fn=<SubBackward0>)
[2022-11-06 07:27:25.351611] Process 3. Episode 32350, average_reward -0.072334
Episode 32350: Total Loss of tensor([[10.6349]], grad_fn=<SubBackward0>)
[2022-11-06 07:27:36.217287] Process 4. Episode 34450, average_reward -0.071669
Episode 34450: Total Loss of tensor([[-2.2168]], grad_fn=<SubBackward0>)
[2022-11-06 07:28:37.672677] Process 1. Episode 31700, average_reward -0.068612
Episode 31700: Total Loss of tensor([[1.8382]], grad_fn=<SubBackward0>)
[2022-11-06 07:28:54.686542] Process 2. Episode 33050, average_reward -0.074796
Episode 33050: Total Loss of tensor([[-0.2695]], grad_fn=<SubBackward0>)
[2022-11-06 07:29:00.929507] Process 5. Episode 32700, average_reward -0.069602
Episode 32700: Total Loss of tensor([[10.4360]], grad_fn=<SubBackward0>)
[2022-11-06 07:29:29.379778] Process 0. Episode 32350, average_reward -0.069861
Episode 32350: Total Loss of tensor([[11.0422]], grad_fn=<SubBackward0>)
[2022-11-06 07:29:49.647564] Process 3. Episode 32400, average_reward -0.072315
Episode 32400: Total Loss of tensor([[7.6757]], grad_fn=<SubBackward0>)
[2022-11-06 07:30:00.167952] Process 4. Episode 34500, average_reward -0.071681
Episode 34500: Total Loss of tensor([[1.2493]], grad_fn=<SubBackward0>)
[2022-11-06 07:31:24.341830] Process 5. Episode 32750, average_reward -0.069557
Episode 32750: Total Loss of tensor([[12.1435]], grad_fn=<SubBackward0>)
[2022-11-06 07:31:25.192872] Process 1. Episode 31750, average_reward -0.068598
Episode 31750: Total Loss of tensor([[4.9648]], grad_fn=<SubBackward0>)
[2022-11-06 07:31:25.912814] Process 2. Episode 33100, average_reward -0.074804
Episode 33100: Total Loss of tensor([[1.6887]], grad_fn=<SubBackward0>)
[2022-11-06 07:31:55.707795] Process 0. Episode 32400, average_reward -0.069877
Episode 32400: Total Loss of tensor([[2.6942]], grad_fn=<SubBackward0>)
[2022-11-06 07:32:20.877447] Process 3. Episode 32450, average_reward -0.072296
Episode 32450: Total Loss of tensor([[-0.6909]], grad_fn=<SubBackward0>)
[2022-11-06 07:32:22.950308] Process 4. Episode 34550, average_reward -0.071751
Episode 34550: Total Loss of tensor([[4.0934]], grad_fn=<SubBackward0>)
[2022-11-06 07:33:49.177777] Process 2. Episode 33150, average_reward -0.074781
Episode 33150: Total Loss of tensor([[11.0545]], grad_fn=<SubBackward0>)
[2022-11-06 07:33:52.270618] Process 5. Episode 32800, average_reward -0.069451
Episode 32800: Total Loss of tensor([[10.2870]], grad_fn=<SubBackward0>)
[2022-11-06 07:33:58.014312] Process 1. Episode 31800, average_reward -0.068553
Episode 31800: Total Loss of tensor([[11.4729]], grad_fn=<SubBackward0>)
[2022-11-06 07:34:29.546677] Process 0. Episode 32450, average_reward -0.069985
Episode 32450: Total Loss of tensor([[5.8022]], grad_fn=<SubBackward0>)
[2022-11-06 07:34:42.359566] Process 4. Episode 34600, average_reward -0.071676
Episode 34600: Total Loss of tensor([[4.6942]], grad_fn=<SubBackward0>)
[2022-11-06 07:34:59.788806] Process 3. Episode 32500, average_reward -0.072277
Episode 32500: Total Loss of tensor([[-5.4880]], grad_fn=<SubBackward0>)
[2022-11-06 07:36:11.028432] Process 2. Episode 33200, average_reward -0.074729
Episode 33200: Total Loss of tensor([[16.5022]], grad_fn=<SubBackward0>)
[2022-11-06 07:36:18.444915] Process 5. Episode 32850, average_reward -0.069498
Episode 32850: Total Loss of tensor([[-44.8562]], grad_fn=<SubBackward0>)
[2022-11-06 07:36:40.574548] Process 1. Episode 31850, average_reward -0.068571
Episode 31850: Total Loss of tensor([[12.9361]], grad_fn=<SubBackward0>)
[2022-11-06 07:37:04.699343] Process 0. Episode 32500, average_reward -0.070000
Episode 32500: Total Loss of tensor([[-0.5716]], grad_fn=<SubBackward0>)
[2022-11-06 07:37:04.883600] Process 4. Episode 34650, average_reward -0.071659
Episode 34650: Total Loss of tensor([[8.3255]], grad_fn=<SubBackward0>)
[2022-11-06 07:37:28.458162] Process 3. Episode 32550, average_reward -0.072289
Episode 32550: Total Loss of tensor([[11.9816]], grad_fn=<SubBackward0>)
[2022-11-06 07:38:38.895169] Process 2. Episode 33250, average_reward -0.074737
Episode 33250: Total Loss of tensor([[12.8569]], grad_fn=<SubBackward0>)
[2022-11-06 07:38:42.704578] Process 5. Episode 32900, average_reward -0.069514
Episode 32900: Total Loss of tensor([[-12.2504]], grad_fn=<SubBackward0>)
[2022-11-06 07:39:22.356455] Process 1. Episode 31900, average_reward -0.068527
Episode 31900: Total Loss of tensor([[7.6385]], grad_fn=<SubBackward0>)
[2022-11-06 07:39:26.970942] Process 4. Episode 34700, average_reward -0.071671
Episode 34700: Total Loss of tensor([[-2.8795]], grad_fn=<SubBackward0>)
[2022-11-06 07:39:30.845394] Process 0. Episode 32550, average_reward -0.069954
Episode 32550: Total Loss of tensor([[9.4317]], grad_fn=<SubBackward0>)
[2022-11-06 07:39:53.237242] Process 3. Episode 32600, average_reward -0.072331
Episode 32600: Total Loss of tensor([[6.4955]], grad_fn=<SubBackward0>)
[2022-11-06 07:41:12.164581] Process 5. Episode 32950, average_reward -0.069560
Episode 32950: Total Loss of tensor([[-43.4572]], grad_fn=<SubBackward0>)
[2022-11-06 07:41:18.508052] Process 2. Episode 33300, average_reward -0.074715
Episode 33300: Total Loss of tensor([[2.7058]], grad_fn=<SubBackward0>)
[2022-11-06 07:41:50.261493] Process 4. Episode 34750, average_reward -0.071712
Episode 34750: Total Loss of tensor([[-23.9984]], grad_fn=<SubBackward0>)
[2022-11-06 07:41:53.607878] Process 1. Episode 31950, average_reward -0.068419
Episode 31950: Total Loss of tensor([[12.2470]], grad_fn=<SubBackward0>)
[2022-11-06 07:42:09.674850] Process 0. Episode 32600, average_reward -0.069969
Episode 32600: Total Loss of tensor([[10.4435]], grad_fn=<SubBackward0>)
[2022-11-06 07:42:18.248632] Process 3. Episode 32650, average_reward -0.072312
Episode 32650: Total Loss of tensor([[2.5219]], grad_fn=<SubBackward0>)
[2022-11-06 07:43:33.958018] Process 5. Episode 33000, average_reward -0.069667
Episode 33000: Total Loss of tensor([[2.6627]], grad_fn=<SubBackward0>)
[2022-11-06 07:43:40.389622] Process 2. Episode 33350, average_reward -0.074693
Episode 33350: Total Loss of tensor([[7.0804]], grad_fn=<SubBackward0>)
[2022-11-06 07:44:10.613066] Process 4. Episode 34800, average_reward -0.071724
Episode 34800: Total Loss of tensor([[10.0746]], grad_fn=<SubBackward0>)
[2022-11-06 07:44:26.501009] Process 1. Episode 32000, average_reward -0.068469
Episode 32000: Total Loss of tensor([[-31.8212]], grad_fn=<SubBackward0>)
[2022-11-06 07:44:42.293631] Process 3. Episode 32700, average_reward -0.072202
Episode 32700: Total Loss of tensor([[18.1147]], grad_fn=<SubBackward0>)
[2022-11-06 07:44:53.182380] Process 0. Episode 32650, average_reward -0.069923
Episode 32650: Total Loss of tensor([[3.1449]], grad_fn=<SubBackward0>)
[2022-11-06 07:45:58.370137] Process 5. Episode 33050, average_reward -0.069713
Episode 33050: Total Loss of tensor([[-20.0179]], grad_fn=<SubBackward0>)
[2022-11-06 07:46:04.641079] Process 2. Episode 33400, average_reward -0.074641
Episode 33400: Total Loss of tensor([[9.2395]], grad_fn=<SubBackward0>)
[2022-11-06 07:46:33.709692] Process 4. Episode 34850, average_reward -0.071736
Episode 34850: Total Loss of tensor([[-37.5155]], grad_fn=<SubBackward0>)
[2022-11-06 07:46:56.492589] Process 1. Episode 32050, average_reward -0.068518
Episode 32050: Total Loss of tensor([[19.3782]], grad_fn=<SubBackward0>)
[2022-11-06 07:47:06.555351] Process 3. Episode 32750, average_reward -0.072183
Episode 32750: Total Loss of tensor([[-4.2905]], grad_fn=<SubBackward0>)
[2022-11-06 07:47:34.941557] Process 0. Episode 32700, average_reward -0.069817
Episode 32700: Total Loss of tensor([[10.9037]], grad_fn=<SubBackward0>)
[2022-11-06 07:48:22.178002] Process 5. Episode 33100, average_reward -0.069668
Episode 33100: Total Loss of tensor([[8.8931]], grad_fn=<SubBackward0>)
[2022-11-06 07:48:26.654881] Process 2. Episode 33450, average_reward -0.074738
Episode 33450: Total Loss of tensor([[5.6748]], grad_fn=<SubBackward0>)
[2022-11-06 07:48:55.540997] Process 4. Episode 34900, average_reward -0.071748
Episode 34900: Total Loss of tensor([[1.0630]], grad_fn=<SubBackward0>)
[2022-11-06 07:49:31.931006] Process 1. Episode 32100, average_reward -0.068567
Episode 32100: Total Loss of tensor([[2.2345]], grad_fn=<SubBackward0>)
[2022-11-06 07:49:33.026051] Process 3. Episode 32800, average_reward -0.072165
Episode 32800: Total Loss of tensor([[5.5316]], grad_fn=<SubBackward0>)
[2022-11-06 07:50:11.412496] Process 0. Episode 32750, average_reward -0.069802
Episode 32750: Total Loss of tensor([[-6.2831]], grad_fn=<SubBackward0>)
[2022-11-06 07:50:46.419916] Process 2. Episode 33500, average_reward -0.074716
Episode 33500: Total Loss of tensor([[23.2949]], grad_fn=<SubBackward0>)
[2022-11-06 07:50:58.187568] Process 5. Episode 33150, average_reward -0.069744
Episode 33150: Total Loss of tensor([[-18.3245]], grad_fn=<SubBackward0>)
[2022-11-06 07:51:18.410151] Process 4. Episode 34950, average_reward -0.071760
Episode 34950: Total Loss of tensor([[7.8345]], grad_fn=<SubBackward0>)
[2022-11-06 07:52:01.813221] Process 3. Episode 32850, average_reward -0.072116
Episode 32850: Total Loss of tensor([[-18.4709]], grad_fn=<SubBackward0>)
[2022-11-06 07:52:04.751641] Process 1. Episode 32150, average_reward -0.068554
Episode 32150: Total Loss of tensor([[-33.8166]], grad_fn=<SubBackward0>)
[2022-11-06 07:52:50.670320] Process 0. Episode 32800, average_reward -0.069909
Episode 32800: Total Loss of tensor([[4.6274]], grad_fn=<SubBackward0>)
[2022-11-06 07:53:13.437580] Process 2. Episode 33550, average_reward -0.074754
Episode 33550: Total Loss of tensor([[18.8016]], grad_fn=<SubBackward0>)
[2022-11-06 07:53:28.184564] Process 5. Episode 33200, average_reward -0.069759
Episode 33200: Total Loss of tensor([[-5.3479]], grad_fn=<SubBackward0>)
[2022-11-06 07:53:43.359835] Process 4. Episode 35000, average_reward -0.071800
Episode 35000: Total Loss of tensor([[-8.2838]], grad_fn=<SubBackward0>)
[2022-11-06 07:54:26.974483] Process 3. Episode 32900, average_reward -0.072097
Episode 32900: Total Loss of tensor([[0.6791]], grad_fn=<SubBackward0>)
[2022-11-06 07:54:43.279092] Process 1. Episode 32200, average_reward -0.068478
Episode 32200: Total Loss of tensor([[0.1723]], grad_fn=<SubBackward0>)
[2022-11-06 07:55:34.232168] Process 0. Episode 32850, average_reward -0.069924
Episode 32850: Total Loss of tensor([[12.8120]], grad_fn=<SubBackward0>)
[2022-11-06 07:55:50.196766] Process 2. Episode 33600, average_reward -0.074702
Episode 33600: Total Loss of tensor([[-16.6958]], grad_fn=<SubBackward0>)
[2022-11-06 07:55:53.309223] Process 5. Episode 33250, average_reward -0.069744
Episode 33250: Total Loss of tensor([[0.5865]], grad_fn=<SubBackward0>)
[2022-11-06 07:56:01.777393] Process 4. Episode 35050, average_reward -0.071783
Episode 35050: Total Loss of tensor([[1.2672]], grad_fn=<SubBackward0>)
[2022-11-06 07:56:50.039680] Process 3. Episode 32950, average_reward -0.072140
Episode 32950: Total Loss of tensor([[5.1980]], grad_fn=<SubBackward0>)
[2022-11-06 07:57:14.339330] Process 1. Episode 32250, average_reward -0.068465
Episode 32250: Total Loss of tensor([[-9.3650]], grad_fn=<SubBackward0>)
[2022-11-06 07:58:15.992625] Process 0. Episode 32900, average_reward -0.070000
Episode 32900: Total Loss of tensor([[-6.7594]], grad_fn=<SubBackward0>)
[2022-11-06 07:58:17.614944] Process 5. Episode 33300, average_reward -0.069730
Episode 33300: Total Loss of tensor([[15.9620]], grad_fn=<SubBackward0>)
[2022-11-06 07:58:21.605129] Process 4. Episode 35100, average_reward -0.071795
Episode 35100: Total Loss of tensor([[-5.4280]], grad_fn=<SubBackward0>)
[2022-11-06 07:58:32.808599] Process 2. Episode 33650, average_reward -0.074681
Episode 33650: Total Loss of tensor([[-0.3519]], grad_fn=<SubBackward0>)
[2022-11-06 07:59:23.261703] Process 3. Episode 33000, average_reward -0.072061
Episode 33000: Total Loss of tensor([[0.2454]], grad_fn=<SubBackward0>)
[2022-11-06 07:59:37.071510] Process 1. Episode 32300, average_reward -0.068421
Episode 32300: Total Loss of tensor([[-1.7821]], grad_fn=<SubBackward0>)
[2022-11-06 08:00:39.158954] Process 4. Episode 35150, average_reward -0.071778
Episode 35150: Total Loss of tensor([[12.6013]], grad_fn=<SubBackward0>)
[2022-11-06 08:00:48.844201] Process 5. Episode 33350, average_reward -0.069715
Episode 33350: Total Loss of tensor([[10.6273]], grad_fn=<SubBackward0>)
[2022-11-06 08:00:49.696775] Process 0. Episode 32950, average_reward -0.069954
Episode 32950: Total Loss of tensor([[10.7916]], grad_fn=<SubBackward0>)
[2022-11-06 08:01:09.641047] Process 2. Episode 33700, average_reward -0.074629
Episode 33700: Total Loss of tensor([[-25.5113]], grad_fn=<SubBackward0>)
[2022-11-06 08:01:50.959772] Process 3. Episode 33050, average_reward -0.072012
Episode 33050: Total Loss of tensor([[12.7210]], grad_fn=<SubBackward0>)
[2022-11-06 08:02:00.377244] Process 1. Episode 32350, average_reward -0.068501
Episode 32350: Total Loss of tensor([[9.1589]], grad_fn=<SubBackward0>)
[2022-11-06 08:02:59.050429] Process 4. Episode 35200, average_reward -0.071733
Episode 35200: Total Loss of tensor([[11.2409]], grad_fn=<SubBackward0>)
[2022-11-06 08:03:16.772717] Process 0. Episode 33000, average_reward -0.070030
Episode 33000: Total Loss of tensor([[5.9847]], grad_fn=<SubBackward0>)
[2022-11-06 08:03:20.604683] Process 5. Episode 33400, average_reward -0.069701
Episode 33400: Total Loss of tensor([[3.0715]], grad_fn=<SubBackward0>)
[2022-11-06 08:03:45.191520] Process 2. Episode 33750, average_reward -0.074637
Episode 33750: Total Loss of tensor([[18.0623]], grad_fn=<SubBackward0>)
[2022-11-06 08:04:18.555903] Process 3. Episode 33100, average_reward -0.072085
Episode 33100: Total Loss of tensor([[2.5581]], grad_fn=<SubBackward0>)
[2022-11-06 08:04:27.402443] Process 1. Episode 32400, average_reward -0.068488
Episode 32400: Total Loss of tensor([[15.2805]], grad_fn=<SubBackward0>)
[2022-11-06 08:05:21.553012] Process 4. Episode 35250, average_reward -0.071688
Episode 35250: Total Loss of tensor([[-17.5434]], grad_fn=<SubBackward0>)
[2022-11-06 08:05:49.777541] Process 5. Episode 33450, average_reward -0.069716
Episode 33450: Total Loss of tensor([[13.9008]], grad_fn=<SubBackward0>)
[2022-11-06 08:05:54.559368] Process 0. Episode 33050, average_reward -0.070045
Episode 33050: Total Loss of tensor([[0.7019]], grad_fn=<SubBackward0>)
[2022-11-06 08:06:08.507123] Process 2. Episode 33800, average_reward -0.074586
Episode 33800: Total Loss of tensor([[10.6701]], grad_fn=<SubBackward0>)
[2022-11-06 08:06:46.869868] Process 3. Episode 33150, average_reward -0.072036
Episode 33150: Total Loss of tensor([[8.5295]], grad_fn=<SubBackward0>)
[2022-11-06 08:06:59.318810] Process 1. Episode 32450, average_reward -0.068505
Episode 32450: Total Loss of tensor([[1.5484]], grad_fn=<SubBackward0>)
[2022-11-06 08:07:45.341118] Process 4. Episode 35300, average_reward -0.071700
Episode 35300: Total Loss of tensor([[-21.0422]], grad_fn=<SubBackward0>)
[2022-11-06 08:08:21.470682] Process 5. Episode 33500, average_reward -0.069672
Episode 33500: Total Loss of tensor([[7.9711]], grad_fn=<SubBackward0>)
[2022-11-06 08:08:26.735164] Process 0. Episode 33100, average_reward -0.070030
Episode 33100: Total Loss of tensor([[10.3793]], grad_fn=<SubBackward0>)
[2022-11-06 08:08:28.178045] Process 2. Episode 33850, average_reward -0.074564
Episode 33850: Total Loss of tensor([[9.3178]], grad_fn=<SubBackward0>)
[2022-11-06 08:09:16.238144] Process 3. Episode 33200, average_reward -0.072048
Episode 33200: Total Loss of tensor([[10.2227]], grad_fn=<SubBackward0>)
[2022-11-06 08:09:30.970069] Process 1. Episode 32500, average_reward -0.068400
Episode 32500: Total Loss of tensor([[11.0061]], grad_fn=<SubBackward0>)
[2022-11-06 08:10:09.261981] Process 4. Episode 35350, average_reward -0.071627
Episode 35350: Total Loss of tensor([[-1.7003]], grad_fn=<SubBackward0>)
[2022-11-06 08:10:49.151759] Process 5. Episode 33550, average_reward -0.069657
Episode 33550: Total Loss of tensor([[-4.4559]], grad_fn=<SubBackward0>)
[2022-11-06 08:10:49.596789] Process 2. Episode 33900, average_reward -0.074543
Episode 33900: Total Loss of tensor([[6.1285]], grad_fn=<SubBackward0>)
[2022-11-06 08:11:10.960125] Process 0. Episode 33150, average_reward -0.070106
Episode 33150: Total Loss of tensor([[6.6750]], grad_fn=<SubBackward0>)
[2022-11-06 08:11:39.771509] Process 3. Episode 33250, average_reward -0.072030
Episode 33250: Total Loss of tensor([[2.2044]], grad_fn=<SubBackward0>)
[2022-11-06 08:12:14.859336] Process 1. Episode 32550, average_reward -0.068418
Episode 32550: Total Loss of tensor([[-14.1876]], grad_fn=<SubBackward0>)
[2022-11-06 08:12:29.969157] Process 4. Episode 35400, average_reward -0.071638
Episode 35400: Total Loss of tensor([[3.7724]], grad_fn=<SubBackward0>)
[2022-11-06 08:13:10.309696] Process 2. Episode 33950, average_reward -0.074610
Episode 33950: Total Loss of tensor([[1.2565]], grad_fn=<SubBackward0>)
[2022-11-06 08:13:28.030200] Process 5. Episode 33600, average_reward -0.069613
Episode 33600: Total Loss of tensor([[13.4364]], grad_fn=<SubBackward0>)
[2022-11-06 08:13:49.295102] Process 0. Episode 33200, average_reward -0.070151
Episode 33200: Total Loss of tensor([[18.1511]], grad_fn=<SubBackward0>)
[2022-11-06 08:14:01.628064] Process 3. Episode 33300, average_reward -0.072072
Episode 33300: Total Loss of tensor([[5.5551]], grad_fn=<SubBackward0>)
[2022-11-06 08:14:49.282572] Process 4. Episode 35450, average_reward -0.071650
Episode 35450: Total Loss of tensor([[7.3610]], grad_fn=<SubBackward0>)
[2022-11-06 08:14:56.196748] Process 1. Episode 32600, average_reward -0.068466
Episode 32600: Total Loss of tensor([[-127.7924]], grad_fn=<SubBackward0>)
[2022-11-06 08:15:30.945282] Process 2. Episode 34000, average_reward -0.074647
Episode 34000: Total Loss of tensor([[-6.1975]], grad_fn=<SubBackward0>)
[2022-11-06 08:15:52.892351] Process 5. Episode 33650, average_reward -0.069539
Episode 33650: Total Loss of tensor([[17.8782]], grad_fn=<SubBackward0>)
[2022-11-06 08:16:27.362766] Process 3. Episode 33350, average_reward -0.072054
Episode 33350: Total Loss of tensor([[16.9926]], grad_fn=<SubBackward0>)
[2022-11-06 08:16:31.198352] Process 0. Episode 33250, average_reward -0.070165
Episode 33250: Total Loss of tensor([[9.4134]], grad_fn=<SubBackward0>)
[2022-11-06 08:17:10.064873] Process 4. Episode 35500, average_reward -0.071718
Episode 35500: Total Loss of tensor([[-6.7161]], grad_fn=<SubBackward0>)
[2022-11-06 08:17:24.160154] Process 1. Episode 32650, average_reward -0.068423
Episode 32650: Total Loss of tensor([[10.8014]], grad_fn=<SubBackward0>)
[2022-11-06 08:17:57.999827] Process 2. Episode 34050, average_reward -0.074626
Episode 34050: Total Loss of tensor([[-2.6818]], grad_fn=<SubBackward0>)
[2022-11-06 08:18:35.961454] Process 5. Episode 33700, average_reward -0.069555
Episode 33700: Total Loss of tensor([[11.0097]], grad_fn=<SubBackward0>)
[2022-11-06 08:18:55.052358] Process 3. Episode 33400, average_reward -0.071976
Episode 33400: Total Loss of tensor([[-4.3459]], grad_fn=<SubBackward0>)
[2022-11-06 08:18:59.018527] Process 0. Episode 33300, average_reward -0.070090
Episode 33300: Total Loss of tensor([[4.7491]], grad_fn=<SubBackward0>)
[2022-11-06 08:19:32.077399] Process 4. Episode 35550, average_reward -0.071786
Episode 35550: Total Loss of tensor([[8.3197]], grad_fn=<SubBackward0>)
[2022-11-06 08:19:59.954633] Process 1. Episode 32700, average_reward -0.068379
Episode 32700: Total Loss of tensor([[-6.7645]], grad_fn=<SubBackward0>)
[2022-11-06 08:20:25.783333] Process 2. Episode 34100, average_reward -0.074604
Episode 34100: Total Loss of tensor([[15.1445]], grad_fn=<SubBackward0>)
[2022-11-06 08:21:08.054366] Process 5. Episode 33750, average_reward -0.069541
Episode 33750: Total Loss of tensor([[6.8488]], grad_fn=<SubBackward0>)
[2022-11-06 08:21:23.260653] Process 0. Episode 33350, average_reward -0.070045
Episode 33350: Total Loss of tensor([[-1.1954]], grad_fn=<SubBackward0>)
[2022-11-06 08:21:31.219665] Process 3. Episode 33450, average_reward -0.072018
Episode 33450: Total Loss of tensor([[-120.4837]], grad_fn=<SubBackward0>)
[2022-11-06 08:21:50.338427] Process 4. Episode 35600, average_reward -0.071685
Episode 35600: Total Loss of tensor([[-1.0412]], grad_fn=<SubBackward0>)
[2022-11-06 08:22:29.472486] Process 1. Episode 32750, average_reward -0.068427
Episode 32750: Total Loss of tensor([[11.0621]], grad_fn=<SubBackward0>)
[2022-11-06 08:22:59.690333] Process 2. Episode 34150, average_reward -0.074524
Episode 34150: Total Loss of tensor([[13.8004]], grad_fn=<SubBackward0>)
[2022-11-06 08:23:32.496238] Process 5. Episode 33800, average_reward -0.069556
Episode 33800: Total Loss of tensor([[11.3524]], grad_fn=<SubBackward0>)
[2022-11-06 08:23:56.896439] Process 3. Episode 33500, average_reward -0.072060
Episode 33500: Total Loss of tensor([[7.0088]], grad_fn=<SubBackward0>)
[2022-11-06 08:24:00.216365] Process 0. Episode 33400, average_reward -0.070060
Episode 33400: Total Loss of tensor([[6.8322]], grad_fn=<SubBackward0>)
[2022-11-06 08:24:10.581729] Process 4. Episode 35650, average_reward -0.071725
Episode 35650: Total Loss of tensor([[3.6966]], grad_fn=<SubBackward0>)
[2022-11-06 08:25:13.617745] Process 1. Episode 32800, average_reward -0.068384
Episode 32800: Total Loss of tensor([[11.4820]], grad_fn=<SubBackward0>)
[2022-11-06 08:25:18.945853] Process 2. Episode 34200, average_reward -0.074474
Episode 34200: Total Loss of tensor([[6.8577]], grad_fn=<SubBackward0>)
[2022-11-06 08:25:59.373115] Process 5. Episode 33850, average_reward -0.069542
Episode 33850: Total Loss of tensor([[3.5709]], grad_fn=<SubBackward0>)
[2022-11-06 08:26:24.504240] Process 3. Episode 33550, average_reward -0.072072
Episode 33550: Total Loss of tensor([[7.5816]], grad_fn=<SubBackward0>)
[2022-11-06 08:26:30.340733] Process 0. Episode 33450, average_reward -0.069985
Episode 33450: Total Loss of tensor([[8.6474]], grad_fn=<SubBackward0>)
[2022-11-06 08:26:30.799918] Process 4. Episode 35700, average_reward -0.071765
Episode 35700: Total Loss of tensor([[-12.6706]], grad_fn=<SubBackward0>)
[2022-11-06 08:27:45.785388] Process 2. Episode 34250, average_reward -0.074482
Episode 34250: Total Loss of tensor([[4.2137]], grad_fn=<SubBackward0>)
[2022-11-06 08:27:46.201386] Process 1. Episode 32850, average_reward -0.068371
Episode 32850: Total Loss of tensor([[6.8726]], grad_fn=<SubBackward0>)
[2022-11-06 08:28:32.753242] Process 5. Episode 33900, average_reward -0.069499
Episode 33900: Total Loss of tensor([[11.5534]], grad_fn=<SubBackward0>)
[2022-11-06 08:28:52.287510] Process 4. Episode 35750, average_reward -0.071748
Episode 35750: Total Loss of tensor([[-0.8772]], grad_fn=<SubBackward0>)
[2022-11-06 08:28:57.585281] Process 3. Episode 33600, average_reward -0.072054
Episode 33600: Total Loss of tensor([[5.6508]], grad_fn=<SubBackward0>)
[2022-11-06 08:29:01.588616] Process 0. Episode 33500, average_reward -0.070030
Episode 33500: Total Loss of tensor([[6.6482]], grad_fn=<SubBackward0>)
[2022-11-06 08:30:09.859247] Process 2. Episode 34300, average_reward -0.074548
Episode 34300: Total Loss of tensor([[-23.3594]], grad_fn=<SubBackward0>)
[2022-11-06 08:30:20.839562] Process 1. Episode 32900, average_reward -0.068267
Episode 32900: Total Loss of tensor([[0.8160]], grad_fn=<SubBackward0>)
[2022-11-06 08:31:02.071249] Process 5. Episode 33950, average_reward -0.069426
Episode 33950: Total Loss of tensor([[-15.5976]], grad_fn=<SubBackward0>)
[2022-11-06 08:31:14.361289] Process 4. Episode 35800, average_reward -0.071704
Episode 35800: Total Loss of tensor([[9.3783]], grad_fn=<SubBackward0>)
[2022-11-06 08:31:23.311026] Process 3. Episode 33650, average_reward -0.072036
Episode 33650: Total Loss of tensor([[-120.4176]], grad_fn=<SubBackward0>)
[2022-11-06 08:31:37.059910] Process 0. Episode 33550, average_reward -0.070015
Episode 33550: Total Loss of tensor([[-121.7623]], grad_fn=<SubBackward0>)
[2022-11-06 08:32:38.019044] Process 2. Episode 34350, average_reward -0.074498
Episode 34350: Total Loss of tensor([[-0.8706]], grad_fn=<SubBackward0>)
[2022-11-06 08:32:59.503832] Process 1. Episode 32950, average_reward -0.068255
Episode 32950: Total Loss of tensor([[-62.1385]], grad_fn=<SubBackward0>)
[2022-11-06 08:33:27.217129] Process 5. Episode 34000, average_reward -0.069471
Episode 34000: Total Loss of tensor([[-11.4757]], grad_fn=<SubBackward0>)
[2022-11-06 08:33:37.282004] Process 4. Episode 35850, average_reward -0.071715
Episode 35850: Total Loss of tensor([[-1.7725]], grad_fn=<SubBackward0>)
[2022-11-06 08:33:46.972365] Process 3. Episode 33700, average_reward -0.071929
Episode 33700: Total Loss of tensor([[5.0039]], grad_fn=<SubBackward0>)
[2022-11-06 08:34:14.291721] Process 0. Episode 33600, average_reward -0.069970
Episode 33600: Total Loss of tensor([[13.1796]], grad_fn=<SubBackward0>)
[2022-11-06 08:35:08.654500] Process 2. Episode 34400, average_reward -0.074506
Episode 34400: Total Loss of tensor([[11.3668]], grad_fn=<SubBackward0>)
[2022-11-06 08:35:41.509116] Process 1. Episode 33000, average_reward -0.068273
Episode 33000: Total Loss of tensor([[8.5670]], grad_fn=<SubBackward0>)
[2022-11-06 08:35:51.161168] Process 5. Episode 34050, average_reward -0.069545
Episode 34050: Total Loss of tensor([[13.8596]], grad_fn=<SubBackward0>)
[2022-11-06 08:35:58.994328] Process 4. Episode 35900, average_reward -0.071699
Episode 35900: Total Loss of tensor([[12.8460]], grad_fn=<SubBackward0>)
[2022-11-06 08:36:15.870498] Process 3. Episode 33750, average_reward -0.071881
Episode 33750: Total Loss of tensor([[5.5314]], grad_fn=<SubBackward0>)
[2022-11-06 08:36:43.371538] Process 0. Episode 33650, average_reward -0.070015
Episode 33650: Total Loss of tensor([[9.0054]], grad_fn=<SubBackward0>)
[2022-11-06 08:37:31.409200] Process 2. Episode 34450, average_reward -0.074543
Episode 34450: Total Loss of tensor([[2.9469]], grad_fn=<SubBackward0>)
[2022-11-06 08:38:14.613159] Process 5. Episode 34100, average_reward -0.069501
Episode 34100: Total Loss of tensor([[6.1485]], grad_fn=<SubBackward0>)
[2022-11-06 08:38:15.965498] Process 1. Episode 33050, average_reward -0.068260
Episode 33050: Total Loss of tensor([[1.0680]], grad_fn=<SubBackward0>)
[2022-11-06 08:38:20.191318] Process 4. Episode 35950, average_reward -0.071711
Episode 35950: Total Loss of tensor([[7.9134]], grad_fn=<SubBackward0>)
[2022-11-06 08:38:51.272393] Process 3. Episode 33800, average_reward -0.071893
Episode 33800: Total Loss of tensor([[-23.2500]], grad_fn=<SubBackward0>)
[2022-11-06 08:39:11.307529] Process 0. Episode 33700, average_reward -0.069970
Episode 33700: Total Loss of tensor([[14.6263]], grad_fn=<SubBackward0>)
[2022-11-06 08:39:53.170562] Process 2. Episode 34500, average_reward -0.074551
Episode 34500: Total Loss of tensor([[-10.9877]], grad_fn=<SubBackward0>)
[2022-11-06 08:40:41.997015] Process 4. Episode 36000, average_reward -0.071722
Episode 36000: Total Loss of tensor([[10.2477]], grad_fn=<SubBackward0>)
[2022-11-06 08:40:45.657751] Process 1. Episode 33100, average_reward -0.068308
Episode 33100: Total Loss of tensor([[0.5341]], grad_fn=<SubBackward0>)
[2022-11-06 08:40:55.499974] Process 5. Episode 34150, average_reward -0.069429
Episode 34150: Total Loss of tensor([[4.8710]], grad_fn=<SubBackward0>)
[2022-11-06 08:41:18.980247] Process 3. Episode 33850, average_reward -0.071876
Episode 33850: Total Loss of tensor([[-6.3679]], grad_fn=<SubBackward0>)
[2022-11-06 08:41:43.658964] Process 0. Episode 33750, average_reward -0.069956
Episode 33750: Total Loss of tensor([[7.0466]], grad_fn=<SubBackward0>)
[2022-11-06 08:42:19.256787] Process 2. Episode 34550, average_reward -0.074443
Episode 34550: Total Loss of tensor([[4.3858]], grad_fn=<SubBackward0>)
[2022-11-06 08:43:01.860177] Process 4. Episode 36050, average_reward -0.071761
Episode 36050: Total Loss of tensor([[-5.2969]], grad_fn=<SubBackward0>)
[2022-11-06 08:43:09.962658] Process 1. Episode 33150, average_reward -0.068326
Episode 33150: Total Loss of tensor([[-13.9899]], grad_fn=<SubBackward0>)
[2022-11-06 08:43:22.275399] Process 5. Episode 34200, average_reward -0.069386
Episode 34200: Total Loss of tensor([[4.0762]], grad_fn=<SubBackward0>)
[2022-11-06 08:43:47.750853] Process 3. Episode 33900, average_reward -0.071888
Episode 33900: Total Loss of tensor([[-115.9941]], grad_fn=<SubBackward0>)
[2022-11-06 08:44:26.459016] Process 0. Episode 33800, average_reward -0.069941
Episode 33800: Total Loss of tensor([[-24.0591]], grad_fn=<SubBackward0>)
[2022-11-06 08:44:50.333436] Process 2. Episode 34600, average_reward -0.074451
Episode 34600: Total Loss of tensor([[6.6826]], grad_fn=<SubBackward0>)
[2022-11-06 08:45:20.789903] Process 4. Episode 36100, average_reward -0.071745
Episode 36100: Total Loss of tensor([[-1.2930]], grad_fn=<SubBackward0>)
[2022-11-06 08:45:47.427509] Process 1. Episode 33200, average_reward -0.068404
Episode 33200: Total Loss of tensor([[12.2962]], grad_fn=<SubBackward0>)
[2022-11-06 08:46:00.258519] Process 5. Episode 34250, average_reward -0.069431
Episode 34250: Total Loss of tensor([[3.5703]], grad_fn=<SubBackward0>)
[2022-11-06 08:46:10.814584] Process 3. Episode 33950, average_reward -0.071811
Episode 33950: Total Loss of tensor([[19.1705]], grad_fn=<SubBackward0>)
[2022-11-06 08:46:55.280077] Process 0. Episode 33850, average_reward -0.069867
Episode 33850: Total Loss of tensor([[10.2777]], grad_fn=<SubBackward0>)
[2022-11-06 08:47:23.008581] Process 2. Episode 34650, average_reward -0.074488
Episode 34650: Total Loss of tensor([[-106.3913]], grad_fn=<SubBackward0>)
[2022-11-06 08:47:40.034668] Process 4. Episode 36150, average_reward -0.071812
Episode 36150: Total Loss of tensor([[5.2022]], grad_fn=<SubBackward0>)
[2022-11-06 08:48:18.581871] Process 1. Episode 33250, average_reward -0.068451
Episode 33250: Total Loss of tensor([[5.1257]], grad_fn=<SubBackward0>)
[2022-11-06 08:48:36.640276] Process 3. Episode 34000, average_reward -0.071735
Episode 34000: Total Loss of tensor([[15.3630]], grad_fn=<SubBackward0>)
[2022-11-06 08:48:42.355716] Process 5. Episode 34300, average_reward -0.069417
Episode 34300: Total Loss of tensor([[13.2136]], grad_fn=<SubBackward0>)
[2022-11-06 08:49:39.789321] Process 0. Episode 33900, average_reward -0.069941
Episode 33900: Total Loss of tensor([[-51.2380]], grad_fn=<SubBackward0>)
[2022-11-06 08:49:55.537153] Process 4. Episode 36200, average_reward -0.071823
Episode 36200: Total Loss of tensor([[-8.1044]], grad_fn=<SubBackward0>)
[2022-11-06 08:49:56.612667] Process 2. Episode 34700, average_reward -0.074438
Episode 34700: Total Loss of tensor([[3.5749]], grad_fn=<SubBackward0>)
[2022-11-06 08:50:50.744896] Process 1. Episode 33300, average_reward -0.068498
Episode 33300: Total Loss of tensor([[6.6292]], grad_fn=<SubBackward0>)
[2022-11-06 08:51:06.609741] Process 3. Episode 34050, average_reward -0.071689
Episode 34050: Total Loss of tensor([[-3.6529]], grad_fn=<SubBackward0>)
[2022-11-06 08:51:10.796184] Process 5. Episode 34350, average_reward -0.069374
Episode 34350: Total Loss of tensor([[10.3921]], grad_fn=<SubBackward0>)
[2022-11-06 08:52:17.065950] Process 4. Episode 36250, average_reward -0.071752
Episode 36250: Total Loss of tensor([[4.9425]], grad_fn=<SubBackward0>)
[2022-11-06 08:52:23.792367] Process 2. Episode 34750, average_reward -0.074417
Episode 34750: Total Loss of tensor([[11.2607]], grad_fn=<SubBackward0>)
[2022-11-06 08:52:24.190771] Process 0. Episode 33950, average_reward -0.070015
Episode 33950: Total Loss of tensor([[-41.6070]], grad_fn=<SubBackward0>)
[2022-11-06 08:53:20.657641] Process 1. Episode 33350, average_reward -0.068426
Episode 33350: Total Loss of tensor([[1.7977]], grad_fn=<SubBackward0>)
[2022-11-06 08:53:34.550779] Process 5. Episode 34400, average_reward -0.069331
Episode 34400: Total Loss of tensor([[15.3218]], grad_fn=<SubBackward0>)
[2022-11-06 08:53:37.557843] Process 3. Episode 34100, average_reward -0.071789
Episode 34100: Total Loss of tensor([[10.8293]], grad_fn=<SubBackward0>)
[2022-11-06 08:54:40.058007] Process 4. Episode 36300, average_reward -0.071708
Episode 36300: Total Loss of tensor([[-0.3605]], grad_fn=<SubBackward0>)
[2022-11-06 08:54:53.788973] Process 2. Episode 34800, average_reward -0.074454
Episode 34800: Total Loss of tensor([[10.4254]], grad_fn=<SubBackward0>)
[2022-11-06 08:55:14.997033] Process 0. Episode 34000, average_reward -0.070000
Episode 34000: Total Loss of tensor([[10.4160]], grad_fn=<SubBackward0>)
[2022-11-06 08:55:54.812091] Process 1. Episode 33400, average_reward -0.068443
Episode 33400: Total Loss of tensor([[11.3610]], grad_fn=<SubBackward0>)
[2022-11-06 08:55:59.517120] Process 5. Episode 34450, average_reward -0.069376
Episode 34450: Total Loss of tensor([[0.4985]], grad_fn=<SubBackward0>)
[2022-11-06 08:56:08.430827] Process 3. Episode 34150, average_reward -0.071742
Episode 34150: Total Loss of tensor([[0.2184]], grad_fn=<SubBackward0>)
[2022-11-06 08:57:00.986445] Process 4. Episode 36350, average_reward -0.071637
Episode 36350: Total Loss of tensor([[5.7634]], grad_fn=<SubBackward0>)
[2022-11-06 08:57:16.843497] Process 2. Episode 34850, average_reward -0.074462
Episode 34850: Total Loss of tensor([[-22.1972]], grad_fn=<SubBackward0>)
[2022-11-06 08:58:02.050811] Process 0. Episode 34050, average_reward -0.070073
Episode 34050: Total Loss of tensor([[-0.3338]], grad_fn=<SubBackward0>)
[2022-11-06 08:58:29.684046] Process 1. Episode 33450, average_reward -0.068460
Episode 33450: Total Loss of tensor([[26.6243]], grad_fn=<SubBackward0>)
[2022-11-06 08:58:29.729997] Process 5. Episode 34500, average_reward -0.069449
Episode 34500: Total Loss of tensor([[-2.6797]], grad_fn=<SubBackward0>)
[2022-11-06 08:58:30.331910] Process 3. Episode 34200, average_reward -0.071725
Episode 34200: Total Loss of tensor([[10.9213]], grad_fn=<SubBackward0>)
[2022-11-06 08:59:18.174882] Process 4. Episode 36400, average_reward -0.071538
Episode 36400: Total Loss of tensor([[5.1487]], grad_fn=<SubBackward0>)
[2022-11-06 08:59:45.379249] Process 2. Episode 34900, average_reward -0.074556
Episode 34900: Total Loss of tensor([[9.4542]], grad_fn=<SubBackward0>)
[2022-11-06 09:00:36.002701] Process 0. Episode 34100, average_reward -0.070059
Episode 34100: Total Loss of tensor([[14.5224]], grad_fn=<SubBackward0>)
[2022-11-06 09:01:04.928782] Process 1. Episode 33500, average_reward -0.068537
Episode 33500: Total Loss of tensor([[14.5330]], grad_fn=<SubBackward0>)
[2022-11-06 09:01:08.339159] Process 3. Episode 34250, average_reward -0.071796
Episode 34250: Total Loss of tensor([[16.6653]], grad_fn=<SubBackward0>)
[2022-11-06 09:01:09.816612] Process 5. Episode 34550, average_reward -0.069407
Episode 34550: Total Loss of tensor([[5.7426]], grad_fn=<SubBackward0>)
[2022-11-06 09:01:39.640849] Process 4. Episode 36450, average_reward -0.071578
Episode 36450: Total Loss of tensor([[9.6823]], grad_fn=<SubBackward0>)
[2022-11-06 09:02:05.398349] Process 2. Episode 34950, average_reward -0.074564
Episode 34950: Total Loss of tensor([[10.7268]], grad_fn=<SubBackward0>)
[2022-11-06 09:03:12.646513] Process 0. Episode 34150, average_reward -0.070132
Episode 34150: Total Loss of tensor([[11.2022]], grad_fn=<SubBackward0>)
[2022-11-06 09:03:33.245269] Process 3. Episode 34300, average_reward -0.071895
Episode 34300: Total Loss of tensor([[2.5859]], grad_fn=<SubBackward0>)
[2022-11-06 09:03:41.866010] Process 5. Episode 34600, average_reward -0.069393
Episode 34600: Total Loss of tensor([[3.4992]], grad_fn=<SubBackward0>)
[2022-11-06 09:03:46.624267] Process 1. Episode 33550, average_reward -0.068495
Episode 33550: Total Loss of tensor([[1.9122]], grad_fn=<SubBackward0>)
[2022-11-06 09:04:02.044076] Process 4. Episode 36500, average_reward -0.071534
Episode 36500: Total Loss of tensor([[7.1966]], grad_fn=<SubBackward0>)
[2022-11-06 09:04:35.911673] Process 2. Episode 35000, average_reward -0.074514
Episode 35000: Total Loss of tensor([[-2.5553]], grad_fn=<SubBackward0>)
[2022-11-06 09:05:44.788358] Process 0. Episode 34200, average_reward -0.070088
Episode 34200: Total Loss of tensor([[12.7047]], grad_fn=<SubBackward0>)
[2022-11-06 09:06:09.331441] Process 3. Episode 34350, average_reward -0.071878
Episode 34350: Total Loss of tensor([[-9.2023]], grad_fn=<SubBackward0>)
[2022-11-06 09:06:11.868945] Process 5. Episode 34650, average_reward -0.069437
Episode 34650: Total Loss of tensor([[14.5699]], grad_fn=<SubBackward0>)
[2022-11-06 09:06:28.269342] Process 1. Episode 33600, average_reward -0.068542
Episode 33600: Total Loss of tensor([[9.7828]], grad_fn=<SubBackward0>)
[2022-11-06 09:06:29.210494] Process 4. Episode 36550, average_reward -0.071655
Episode 36550: Total Loss of tensor([[4.8459]], grad_fn=<SubBackward0>)
[2022-11-06 09:07:02.989011] Process 2. Episode 35050, average_reward -0.074465
Episode 35050: Total Loss of tensor([[12.0991]], grad_fn=<SubBackward0>)
[2022-11-06 09:08:13.888199] Process 0. Episode 34250, average_reward -0.070161
Episode 34250: Total Loss of tensor([[10.7291]], grad_fn=<SubBackward0>)
[2022-11-06 09:08:35.548367] Process 5. Episode 34700, average_reward -0.069481
Episode 34700: Total Loss of tensor([[23.8064]], grad_fn=<SubBackward0>)
[2022-11-06 09:08:46.316267] Process 3. Episode 34400, average_reward -0.071860
Episode 34400: Total Loss of tensor([[-52.0336]], grad_fn=<SubBackward0>)
[2022-11-06 09:08:51.312700] Process 4. Episode 36600, average_reward -0.071721
Episode 36600: Total Loss of tensor([[-84.7919]], grad_fn=<SubBackward0>)
[2022-11-06 09:09:12.463295] Process 1. Episode 33650, average_reward -0.068588
Episode 33650: Total Loss of tensor([[2.5326]], grad_fn=<SubBackward0>)
[2022-11-06 09:09:45.001407] Process 2. Episode 35100, average_reward -0.074473
Episode 35100: Total Loss of tensor([[12.8253]], grad_fn=<SubBackward0>)
[2022-11-06 09:10:43.033395] Process 0. Episode 34300, average_reward -0.070146
Episode 34300: Total Loss of tensor([[8.7551]], grad_fn=<SubBackward0>)
[2022-11-06 09:10:58.416365] Process 5. Episode 34750, average_reward -0.069496
Episode 34750: Total Loss of tensor([[4.5133]], grad_fn=<SubBackward0>)
[2022-11-06 09:11:13.762883] Process 4. Episode 36650, average_reward -0.071705
Episode 36650: Total Loss of tensor([[4.9891]], grad_fn=<SubBackward0>)
[2022-11-06 09:11:17.923263] Process 3. Episode 34450, average_reward -0.071930
Episode 34450: Total Loss of tensor([[-1.4762]], grad_fn=<SubBackward0>)
[2022-11-06 09:11:55.403057] Process 1. Episode 33700, average_reward -0.068635
Episode 33700: Total Loss of tensor([[3.0157]], grad_fn=<SubBackward0>)
[2022-11-06 09:12:15.539031] Process 2. Episode 35150, average_reward -0.074481
Episode 35150: Total Loss of tensor([[14.4089]], grad_fn=<SubBackward0>)
[2022-11-06 09:13:18.990424] Process 0. Episode 34350, average_reward -0.070044
Episode 34350: Total Loss of tensor([[11.5988]], grad_fn=<SubBackward0>)
[2022-11-06 09:13:19.491921] Process 5. Episode 34800, average_reward -0.069483
Episode 34800: Total Loss of tensor([[13.0612]], grad_fn=<SubBackward0>)
[2022-11-06 09:13:32.879088] Process 4. Episode 36700, average_reward -0.071689
Episode 36700: Total Loss of tensor([[12.2367]], grad_fn=<SubBackward0>)
[2022-11-06 09:13:44.589783] Process 3. Episode 34500, average_reward -0.071913
Episode 34500: Total Loss of tensor([[16.1303]], grad_fn=<SubBackward0>)
[2022-11-06 09:14:39.069268] Process 1. Episode 33750, average_reward -0.068622
Episode 33750: Total Loss of tensor([[-4.3795]], grad_fn=<SubBackward0>)
[2022-11-06 09:14:39.361286] Process 2. Episode 35200, average_reward -0.074631
Episode 35200: Total Loss of tensor([[12.5276]], grad_fn=<SubBackward0>)
[2022-11-06 09:15:40.628065] Process 5. Episode 34850, average_reward -0.069469
Episode 34850: Total Loss of tensor([[-57.4059]], grad_fn=<SubBackward0>)
[2022-11-06 09:15:57.548525] Process 4. Episode 36750, average_reward -0.071701
Episode 36750: Total Loss of tensor([[1.3010]], grad_fn=<SubBackward0>)
[2022-11-06 09:15:59.026160] Process 0. Episode 34400, average_reward -0.070029
Episode 34400: Total Loss of tensor([[3.4429]], grad_fn=<SubBackward0>)
[2022-11-06 09:16:22.980540] Process 3. Episode 34550, average_reward -0.071896
Episode 34550: Total Loss of tensor([[8.6477]], grad_fn=<SubBackward0>)
[2022-11-06 09:17:06.307316] Process 2. Episode 35250, average_reward -0.074582
Episode 35250: Total Loss of tensor([[3.7694]], grad_fn=<SubBackward0>)
[2022-11-06 09:17:13.795438] Process 1. Episode 33800, average_reward -0.068550
Episode 33800: Total Loss of tensor([[6.2056]], grad_fn=<SubBackward0>)
[2022-11-06 09:18:04.423825] Process 5. Episode 34900, average_reward -0.069398
Episode 34900: Total Loss of tensor([[14.8086]], grad_fn=<SubBackward0>)
[2022-11-06 09:18:24.837519] Process 0. Episode 34450, average_reward -0.070015
Episode 34450: Total Loss of tensor([[17.4528]], grad_fn=<SubBackward0>)
[2022-11-06 09:18:25.567774] Process 4. Episode 36800, average_reward -0.071712
Episode 36800: Total Loss of tensor([[7.7026]], grad_fn=<SubBackward0>)
[2022-11-06 09:18:58.783004] Process 3. Episode 34600, average_reward -0.071879
Episode 34600: Total Loss of tensor([[12.9313]], grad_fn=<SubBackward0>)
[2022-11-06 09:19:44.485330] Process 2. Episode 35300, average_reward -0.074561
Episode 35300: Total Loss of tensor([[9.6228]], grad_fn=<SubBackward0>)
[2022-11-06 09:19:52.464817] Process 1. Episode 33850, average_reward -0.068597
Episode 33850: Total Loss of tensor([[13.3940]], grad_fn=<SubBackward0>)
[2022-11-06 09:20:25.785694] Process 5. Episode 34950, average_reward -0.069528
Episode 34950: Total Loss of tensor([[1.3786]], grad_fn=<SubBackward0>)
[2022-11-06 09:20:47.975533] Process 4. Episode 36850, average_reward -0.071750
Episode 36850: Total Loss of tensor([[3.9133]], grad_fn=<SubBackward0>)
[2022-11-06 09:20:59.560055] Process 0. Episode 34500, average_reward -0.070029
Episode 34500: Total Loss of tensor([[4.9429]], grad_fn=<SubBackward0>)
[2022-11-06 09:21:29.607796] Process 3. Episode 34650, average_reward -0.071861
Episode 34650: Total Loss of tensor([[23.9119]], grad_fn=<SubBackward0>)
[2022-11-06 09:22:17.956778] Process 2. Episode 35350, average_reward -0.074625
Episode 35350: Total Loss of tensor([[-112.8487]], grad_fn=<SubBackward0>)
[2022-11-06 09:22:33.353913] Process 1. Episode 33900, average_reward -0.068555
Episode 33900: Total Loss of tensor([[7.3598]], grad_fn=<SubBackward0>)
[2022-11-06 09:22:49.232154] Process 5. Episode 35000, average_reward -0.069543
Episode 35000: Total Loss of tensor([[-16.9817]], grad_fn=<SubBackward0>)
[2022-11-06 09:23:12.470699] Process 4. Episode 36900, average_reward -0.071762
Episode 36900: Total Loss of tensor([[-109.7303]], grad_fn=<SubBackward0>)
[2022-11-06 09:23:28.468184] Process 0. Episode 34550, average_reward -0.070043
Episode 34550: Total Loss of tensor([[-91.3498]], grad_fn=<SubBackward0>)
[2022-11-06 09:24:17.393872] Process 3. Episode 34700, average_reward -0.071816
Episode 34700: Total Loss of tensor([[0.1300]], grad_fn=<SubBackward0>)
[2022-11-06 09:24:42.890996] Process 2. Episode 35400, average_reward -0.074576
Episode 35400: Total Loss of tensor([[5.7111]], grad_fn=<SubBackward0>)
[2022-11-06 09:25:10.691247] Process 1. Episode 33950, average_reward -0.068571
Episode 33950: Total Loss of tensor([[-124.4397]], grad_fn=<SubBackward0>)
[2022-11-06 09:25:11.259935] Process 5. Episode 35050, average_reward -0.069529
Episode 35050: Total Loss of tensor([[-0.5463]], grad_fn=<SubBackward0>)
[2022-11-06 09:25:33.162862] Process 4. Episode 36950, average_reward -0.071854
Episode 36950: Total Loss of tensor([[-60.4558]], grad_fn=<SubBackward0>)
[2022-11-06 09:25:53.400628] Process 0. Episode 34600, average_reward -0.070000
Episode 34600: Total Loss of tensor([[10.8832]], grad_fn=<SubBackward0>)
[2022-11-06 09:26:56.010510] Process 3. Episode 34750, average_reward -0.071799
Episode 34750: Total Loss of tensor([[9.0865]], grad_fn=<SubBackward0>)
[2022-11-06 09:27:24.123737] Process 2. Episode 35450, average_reward -0.074471
Episode 35450: Total Loss of tensor([[9.8757]], grad_fn=<SubBackward0>)
[2022-11-06 09:27:39.107380] Process 1. Episode 34000, average_reward -0.068618
Episode 34000: Total Loss of tensor([[9.0003]], grad_fn=<SubBackward0>)
[2022-11-06 09:27:39.836486] Process 5. Episode 35100, average_reward -0.069573
Episode 35100: Total Loss of tensor([[28.5068]], grad_fn=<SubBackward0>)
[2022-11-06 09:27:57.581672] Process 4. Episode 37000, average_reward -0.071784
Episode 37000: Total Loss of tensor([[-1.8150]], grad_fn=<SubBackward0>)
[2022-11-06 09:28:22.133154] Process 0. Episode 34650, average_reward -0.070014
Episode 34650: Total Loss of tensor([[7.5466]], grad_fn=<SubBackward0>)
[2022-11-06 09:29:33.944861] Process 3. Episode 34800, average_reward -0.071897
Episode 34800: Total Loss of tensor([[6.1777]], grad_fn=<SubBackward0>)
[2022-11-06 09:29:54.867174] Process 2. Episode 35500, average_reward -0.074451
Episode 35500: Total Loss of tensor([[9.8061]], grad_fn=<SubBackward0>)
[2022-11-06 09:30:02.200078] Process 5. Episode 35150, average_reward -0.069531
Episode 35150: Total Loss of tensor([[-1.5481]], grad_fn=<SubBackward0>)
[2022-11-06 09:30:04.116197] Process 1. Episode 34050, average_reward -0.068546
Episode 34050: Total Loss of tensor([[14.1734]], grad_fn=<SubBackward0>)
[2022-11-06 09:30:16.944357] Process 4. Episode 37050, average_reward -0.071714
Episode 37050: Total Loss of tensor([[12.4324]], grad_fn=<SubBackward0>)
[2022-11-06 09:30:55.457225] Process 0. Episode 34700, average_reward -0.070058
Episode 34700: Total Loss of tensor([[15.1045]], grad_fn=<SubBackward0>)
[2022-11-06 09:32:14.684843] Process 3. Episode 34850, average_reward -0.071879
Episode 34850: Total Loss of tensor([[-76.4104]], grad_fn=<SubBackward0>)
[2022-11-06 09:32:20.864536] Process 2. Episode 35550, average_reward -0.074374
Episode 35550: Total Loss of tensor([[3.1485]], grad_fn=<SubBackward0>)
[2022-11-06 09:32:27.029956] Process 5. Episode 35200, average_reward -0.069489
Episode 35200: Total Loss of tensor([[10.5107]], grad_fn=<SubBackward0>)
[2022-11-06 09:32:37.458368] Process 4. Episode 37100, average_reward -0.071698
Episode 37100: Total Loss of tensor([[-8.8448]], grad_fn=<SubBackward0>)
[2022-11-06 09:32:42.945673] Process 1. Episode 34100, average_reward -0.068504
Episode 34100: Total Loss of tensor([[-5.1084]], grad_fn=<SubBackward0>)
[2022-11-06 09:33:28.158498] Process 0. Episode 34750, average_reward -0.070129
Episode 34750: Total Loss of tensor([[7.6089]], grad_fn=<SubBackward0>)
[2022-11-06 09:34:46.086245] Process 3. Episode 34900, average_reward -0.071862
Episode 34900: Total Loss of tensor([[16.9325]], grad_fn=<SubBackward0>)
[2022-11-06 09:34:55.702035] Process 5. Episode 35250, average_reward -0.069560
Episode 35250: Total Loss of tensor([[5.2163]], grad_fn=<SubBackward0>)
[2022-11-06 09:35:00.790609] Process 4. Episode 37150, average_reward -0.071629
Episode 37150: Total Loss of tensor([[7.1242]], grad_fn=<SubBackward0>)
[2022-11-06 09:35:02.263311] Process 2. Episode 35600, average_reward -0.074354
Episode 35600: Total Loss of tensor([[-5.1777]], grad_fn=<SubBackward0>)
[2022-11-06 09:35:10.061049] Process 1. Episode 34150, average_reward -0.068492
Episode 34150: Total Loss of tensor([[-53.8581]], grad_fn=<SubBackward0>)
[2022-11-06 09:36:08.038921] Process 0. Episode 34800, average_reward -0.070057
Episode 34800: Total Loss of tensor([[7.9725]], grad_fn=<SubBackward0>)
[2022-11-06 09:37:20.794595] Process 3. Episode 34950, average_reward -0.071788
Episode 34950: Total Loss of tensor([[12.9229]], grad_fn=<SubBackward0>)
[2022-11-06 09:37:23.047275] Process 5. Episode 35300, average_reward -0.069547
Episode 35300: Total Loss of tensor([[16.8709]], grad_fn=<SubBackward0>)
[2022-11-06 09:37:24.701048] Process 4. Episode 37200, average_reward -0.071640
Episode 37200: Total Loss of tensor([[8.5964]], grad_fn=<SubBackward0>)
[2022-11-06 09:37:28.364439] Process 2. Episode 35650, average_reward -0.074390
Episode 35650: Total Loss of tensor([[10.9739]], grad_fn=<SubBackward0>)
[2022-11-06 09:37:37.368620] Process 1. Episode 34200, average_reward -0.068480
Episode 34200: Total Loss of tensor([[-9.6720]], grad_fn=<SubBackward0>)
[2022-11-06 09:38:51.474856] Process 0. Episode 34850, average_reward -0.070100
Episode 34850: Total Loss of tensor([[-8.0877]], grad_fn=<SubBackward0>)
[2022-11-06 09:39:47.740176] Process 4. Episode 37250, average_reward -0.071651
Episode 37250: Total Loss of tensor([[10.6689]], grad_fn=<SubBackward0>)
[2022-11-06 09:39:49.224298] Process 5. Episode 35350, average_reward -0.069590
Episode 35350: Total Loss of tensor([[17.1857]], grad_fn=<SubBackward0>)
[2022-11-06 09:39:49.753036] Process 3. Episode 35000, average_reward -0.071657
Episode 35000: Total Loss of tensor([[9.4409]], grad_fn=<SubBackward0>)
[2022-11-06 09:39:57.134822] Process 2. Episode 35700, average_reward -0.074426
Episode 35700: Total Loss of tensor([[16.4228]], grad_fn=<SubBackward0>)
[2022-11-06 09:40:13.171884] Process 1. Episode 34250, average_reward -0.068672
Episode 34250: Total Loss of tensor([[-122.9928]], grad_fn=<SubBackward0>)
[2022-11-06 09:41:30.409659] Process 0. Episode 34900, average_reward -0.070086
Episode 34900: Total Loss of tensor([[8.5417]], grad_fn=<SubBackward0>)
[2022-11-06 09:42:11.078564] Process 4. Episode 37300, average_reward -0.071689
Episode 37300: Total Loss of tensor([[2.0183]], grad_fn=<SubBackward0>)
[2022-11-06 09:42:11.875965] Process 5. Episode 35400, average_reward -0.069576
Episode 35400: Total Loss of tensor([[7.7031]], grad_fn=<SubBackward0>)
[2022-11-06 09:42:24.964520] Process 3. Episode 35050, average_reward -0.071812
Episode 35050: Total Loss of tensor([[-1.4541]], grad_fn=<SubBackward0>)
[2022-11-06 09:42:27.374276] Process 2. Episode 35750, average_reward -0.074490
Episode 35750: Total Loss of tensor([[9.3170]], grad_fn=<SubBackward0>)
[2022-11-06 09:42:45.207408] Process 1. Episode 34300, average_reward -0.068630
Episode 34300: Total Loss of tensor([[8.8026]], grad_fn=<SubBackward0>)
[2022-11-06 09:44:15.675548] Process 0. Episode 34950, average_reward -0.070072
Episode 34950: Total Loss of tensor([[2.2537]], grad_fn=<SubBackward0>)
[2022-11-06 09:44:29.999015] Process 4. Episode 37350, average_reward -0.071700
Episode 37350: Total Loss of tensor([[14.4756]], grad_fn=<SubBackward0>)
[2022-11-06 09:44:36.291421] Process 5. Episode 35450, average_reward -0.069619
Episode 35450: Total Loss of tensor([[-1.7356]], grad_fn=<SubBackward0>)
[2022-11-06 09:44:55.885146] Process 3. Episode 35100, average_reward -0.071738
Episode 35100: Total Loss of tensor([[4.1702]], grad_fn=<SubBackward0>)
[2022-11-06 09:45:00.146940] Process 2. Episode 35800, average_reward -0.074441
Episode 35800: Total Loss of tensor([[14.4270]], grad_fn=<SubBackward0>)
[2022-11-06 09:45:12.513984] Process 1. Episode 34350, average_reward -0.068646
Episode 34350: Total Loss of tensor([[-0.1839]], grad_fn=<SubBackward0>)
[2022-11-06 09:46:46.163176] Process 0. Episode 35000, average_reward -0.070029
Episode 35000: Total Loss of tensor([[15.5969]], grad_fn=<SubBackward0>)
[2022-11-06 09:46:51.253062] Process 4. Episode 37400, average_reward -0.071711
Episode 37400: Total Loss of tensor([[-0.3715]], grad_fn=<SubBackward0>)
[2022-11-06 09:47:06.015210] Process 5. Episode 35500, average_reward -0.069634
Episode 35500: Total Loss of tensor([[4.6549]], grad_fn=<SubBackward0>)
[2022-11-06 09:47:25.341216] Process 3. Episode 35150, average_reward -0.071664
Episode 35150: Total Loss of tensor([[-10.2583]], grad_fn=<SubBackward0>)
[2022-11-06 09:47:29.013911] Process 2. Episode 35850, average_reward -0.074449
Episode 35850: Total Loss of tensor([[10.4222]], grad_fn=<SubBackward0>)
[2022-11-06 09:47:50.493045] Process 1. Episode 34400, average_reward -0.068605
Episode 34400: Total Loss of tensor([[-4.3207]], grad_fn=<SubBackward0>)
[2022-11-06 09:49:11.761228] Process 4. Episode 37450, average_reward -0.071722
Episode 37450: Total Loss of tensor([[-77.2642]], grad_fn=<SubBackward0>)
[2022-11-06 09:49:17.384234] Process 0. Episode 35050, average_reward -0.070043
Episode 35050: Total Loss of tensor([[-2.5985]], grad_fn=<SubBackward0>)
[2022-11-06 09:49:32.926166] Process 5. Episode 35550, average_reward -0.069620
Episode 35550: Total Loss of tensor([[6.1994]], grad_fn=<SubBackward0>)
[2022-11-06 09:49:56.232315] Process 2. Episode 35900, average_reward -0.074457
Episode 35900: Total Loss of tensor([[-23.8067]], grad_fn=<SubBackward0>)
[2022-11-06 09:49:57.871111] Process 3. Episode 35200, average_reward -0.071648
Episode 35200: Total Loss of tensor([[15.0327]], grad_fn=<SubBackward0>)
[2022-11-06 09:50:34.204536] Process 1. Episode 34450, average_reward -0.068679
Episode 34450: Total Loss of tensor([[1.7975]], grad_fn=<SubBackward0>)
[2022-11-06 09:51:36.241889] Process 4. Episode 37500, average_reward -0.071760
Episode 37500: Total Loss of tensor([[-186.4001]], grad_fn=<SubBackward0>)
[2022-11-06 09:51:50.134708] Process 0. Episode 35100, average_reward -0.070114
Episode 35100: Total Loss of tensor([[13.0277]], grad_fn=<SubBackward0>)
[2022-11-06 09:51:54.594808] Process 5. Episode 35600, average_reward -0.069579
Episode 35600: Total Loss of tensor([[19.4848]], grad_fn=<SubBackward0>)
[2022-11-06 09:52:16.652153] Process 2. Episode 35950, average_reward -0.074520
Episode 35950: Total Loss of tensor([[21.8239]], grad_fn=<SubBackward0>)
[2022-11-06 09:52:26.467044] Process 3. Episode 35250, average_reward -0.071716
Episode 35250: Total Loss of tensor([[-79.5059]], grad_fn=<SubBackward0>)
[2022-11-06 09:53:15.003915] Process 1. Episode 34500, average_reward -0.068696
Episode 34500: Total Loss of tensor([[8.4634]], grad_fn=<SubBackward0>)
[2022-11-06 09:54:02.489779] Process 4. Episode 37550, average_reward -0.071744
Episode 37550: Total Loss of tensor([[1.8500]], grad_fn=<SubBackward0>)
[2022-11-06 09:54:17.893028] Process 5. Episode 35650, average_reward -0.069621
Episode 35650: Total Loss of tensor([[5.2335]], grad_fn=<SubBackward0>)
[2022-11-06 09:54:26.221109] Process 0. Episode 35150, average_reward -0.070014
Episode 35150: Total Loss of tensor([[-0.1478]], grad_fn=<SubBackward0>)
[2022-11-06 09:54:39.707730] Process 2. Episode 36000, average_reward -0.074528
Episode 36000: Total Loss of tensor([[6.3218]], grad_fn=<SubBackward0>)
[2022-11-06 09:54:57.785835] Process 3. Episode 35300, average_reward -0.071785
Episode 35300: Total Loss of tensor([[14.1372]], grad_fn=<SubBackward0>)
[2022-11-06 09:55:48.997186] Process 1. Episode 34550, average_reward -0.068712
Episode 34550: Total Loss of tensor([[-9.0253]], grad_fn=<SubBackward0>)
[2022-11-06 09:56:25.118557] Process 4. Episode 37600, average_reward -0.071835
Episode 37600: Total Loss of tensor([[1.8640]], grad_fn=<SubBackward0>)
[2022-11-06 09:56:40.747897] Process 5. Episode 35700, average_reward -0.069608
Episode 35700: Total Loss of tensor([[1.0745]], grad_fn=<SubBackward0>)
[2022-11-06 09:57:03.013057] Process 2. Episode 36050, average_reward -0.074424
Episode 36050: Total Loss of tensor([[-0.5894]], grad_fn=<SubBackward0>)
[2022-11-06 09:57:05.167335] Process 0. Episode 35200, average_reward -0.069972
Episode 35200: Total Loss of tensor([[7.0723]], grad_fn=<SubBackward0>)
[2022-11-06 09:57:32.183725] Process 3. Episode 35350, average_reward -0.071768
Episode 35350: Total Loss of tensor([[8.7046]], grad_fn=<SubBackward0>)
[2022-11-06 09:58:26.105153] Process 1. Episode 34600, average_reward -0.068728
Episode 34600: Total Loss of tensor([[-12.9082]], grad_fn=<SubBackward0>)
[2022-11-06 09:58:46.727657] Process 4. Episode 37650, average_reward -0.071846
Episode 37650: Total Loss of tensor([[19.8211]], grad_fn=<SubBackward0>)
[2022-11-06 09:59:05.631993] Process 5. Episode 35750, average_reward -0.069594
Episode 35750: Total Loss of tensor([[-62.9479]], grad_fn=<SubBackward0>)
[2022-11-06 09:59:29.979486] Process 2. Episode 36100, average_reward -0.074488
Episode 36100: Total Loss of tensor([[2.4613]], grad_fn=<SubBackward0>)
[2022-11-06 09:59:39.787758] Process 0. Episode 35250, average_reward -0.070043
Episode 35250: Total Loss of tensor([[6.9914]], grad_fn=<SubBackward0>)
[2022-11-06 10:00:13.514453] Process 3. Episode 35400, average_reward -0.071780
Episode 35400: Total Loss of tensor([[5.3991]], grad_fn=<SubBackward0>)
[2022-11-06 10:00:56.535179] Process 1. Episode 34650, average_reward -0.068745
Episode 34650: Total Loss of tensor([[6.0452]], grad_fn=<SubBackward0>)
[2022-11-06 10:01:07.313446] Process 4. Episode 37700, average_reward -0.071804
Episode 37700: Total Loss of tensor([[5.7387]], grad_fn=<SubBackward0>)
[2022-11-06 10:01:29.375908] Process 5. Episode 35800, average_reward -0.069609
Episode 35800: Total Loss of tensor([[18.2722]], grad_fn=<SubBackward0>)
[2022-11-06 10:01:53.738570] Process 2. Episode 36150, average_reward -0.074440
Episode 36150: Total Loss of tensor([[5.3963]], grad_fn=<SubBackward0>)
[2022-11-06 10:02:07.386743] Process 0. Episode 35300, average_reward -0.070000
Episode 35300: Total Loss of tensor([[5.6678]], grad_fn=<SubBackward0>)
[2022-11-06 10:02:52.837696] Process 3. Episode 35450, average_reward -0.071707
Episode 35450: Total Loss of tensor([[-6.2543]], grad_fn=<SubBackward0>)
[2022-11-06 10:03:27.626859] Process 4. Episode 37750, average_reward -0.071815
Episode 37750: Total Loss of tensor([[18.1039]], grad_fn=<SubBackward0>)
[2022-11-06 10:03:33.538671] Process 1. Episode 34700, average_reward -0.068790
Episode 34700: Total Loss of tensor([[4.0974]], grad_fn=<SubBackward0>)
[2022-11-06 10:03:50.365355] Process 5. Episode 35850, average_reward -0.069707
Episode 35850: Total Loss of tensor([[17.8793]], grad_fn=<SubBackward0>)
[2022-11-06 10:04:25.818693] Process 2. Episode 36200, average_reward -0.074475
Episode 36200: Total Loss of tensor([[4.3316]], grad_fn=<SubBackward0>)
[2022-11-06 10:04:35.716631] Process 0. Episode 35350, average_reward -0.070014
Episode 35350: Total Loss of tensor([[6.7341]], grad_fn=<SubBackward0>)
[2022-11-06 10:05:22.445470] Process 3. Episode 35500, average_reward -0.071690
Episode 35500: Total Loss of tensor([[12.5166]], grad_fn=<SubBackward0>)
[2022-11-06 10:05:47.430117] Process 4. Episode 37800, average_reward -0.071799
Episode 37800: Total Loss of tensor([[9.0821]], grad_fn=<SubBackward0>)
[2022-11-06 10:06:12.413468] Process 1. Episode 34750, average_reward -0.068806
Episode 34750: Total Loss of tensor([[7.2237]], grad_fn=<SubBackward0>)
[2022-11-06 10:06:14.940503] Process 5. Episode 35900, average_reward -0.069610
Episode 35900: Total Loss of tensor([[10.5031]], grad_fn=<SubBackward0>)
[2022-11-06 10:06:51.210515] Process 2. Episode 36250, average_reward -0.074510
Episode 36250: Total Loss of tensor([[-40.3685]], grad_fn=<SubBackward0>)
[2022-11-06 10:07:00.182482] Process 0. Episode 35400, average_reward -0.069944
Episode 35400: Total Loss of tensor([[6.1789]], grad_fn=<SubBackward0>)
[2022-11-06 10:07:50.664031] Process 3. Episode 35550, average_reward -0.071617
Episode 35550: Total Loss of tensor([[2.2277]], grad_fn=<SubBackward0>)
[2022-11-06 10:08:11.464013] Process 4. Episode 37850, average_reward -0.071836
Episode 37850: Total Loss of tensor([[0.2630]], grad_fn=<SubBackward0>)
[2022-11-06 10:08:39.168131] Process 5. Episode 35950, average_reward -0.069597
Episode 35950: Total Loss of tensor([[1.6791]], grad_fn=<SubBackward0>)
[2022-11-06 10:08:43.448043] Process 1. Episode 34800, average_reward -0.068736
Episode 34800: Total Loss of tensor([[8.3084]], grad_fn=<SubBackward0>)
[2022-11-06 10:09:12.114957] Process 2. Episode 36300, average_reward -0.074463
Episode 36300: Total Loss of tensor([[8.1080]], grad_fn=<SubBackward0>)
[2022-11-06 10:09:27.327304] Process 0. Episode 35450, average_reward -0.069901
Episode 35450: Total Loss of tensor([[4.5666]], grad_fn=<SubBackward0>)
[2022-11-06 10:10:18.264526] Process 3. Episode 35600, average_reward -0.071657
Episode 35600: Total Loss of tensor([[-90.0502]], grad_fn=<SubBackward0>)
[2022-11-06 10:10:42.853143] Process 4. Episode 37900, average_reward -0.071768
Episode 37900: Total Loss of tensor([[4.4490]], grad_fn=<SubBackward0>)
[2022-11-06 10:11:11.878436] Process 5. Episode 36000, average_reward -0.069556
Episode 36000: Total Loss of tensor([[-3.6793]], grad_fn=<SubBackward0>)
[2022-11-06 10:11:22.521174] Process 1. Episode 34850, average_reward -0.068723
Episode 34850: Total Loss of tensor([[6.7438]], grad_fn=<SubBackward0>)
[2022-11-06 10:11:40.148573] Process 2. Episode 36350, average_reward -0.074443
Episode 36350: Total Loss of tensor([[7.3638]], grad_fn=<SubBackward0>)
[2022-11-06 10:11:54.590331] Process 0. Episode 35500, average_reward -0.069887
Episode 35500: Total Loss of tensor([[-24.7151]], grad_fn=<SubBackward0>)
[2022-11-06 10:12:49.291619] Process 3. Episode 35650, average_reward -0.071641
Episode 35650: Total Loss of tensor([[-5.0863]], grad_fn=<SubBackward0>)
[2022-11-06 10:13:06.021909] Process 4. Episode 37950, average_reward -0.071752
Episode 37950: Total Loss of tensor([[5.5216]], grad_fn=<SubBackward0>)
[2022-11-06 10:13:38.362959] Process 5. Episode 36050, average_reward -0.069487
Episode 36050: Total Loss of tensor([[0.3284]], grad_fn=<SubBackward0>)
[2022-11-06 10:14:03.251980] Process 1. Episode 34900, average_reward -0.068682
Episode 34900: Total Loss of tensor([[7.0129]], grad_fn=<SubBackward0>)
[2022-11-06 10:14:11.286570] Process 2. Episode 36400, average_reward -0.074341
Episode 36400: Total Loss of tensor([[14.3333]], grad_fn=<SubBackward0>)
[2022-11-06 10:14:21.771373] Process 0. Episode 35550, average_reward -0.069873
Episode 35550: Total Loss of tensor([[-102.5463]], grad_fn=<SubBackward0>)
[2022-11-06 10:15:18.264055] Process 3. Episode 35700, average_reward -0.071709
Episode 35700: Total Loss of tensor([[8.1768]], grad_fn=<SubBackward0>)
[2022-11-06 10:15:27.921905] Process 4. Episode 38000, average_reward -0.071763
Episode 38000: Total Loss of tensor([[8.4522]], grad_fn=<SubBackward0>)
[2022-11-06 10:16:12.744433] Process 5. Episode 36100, average_reward -0.069668
Episode 36100: Total Loss of tensor([[-0.7101]], grad_fn=<SubBackward0>)
[2022-11-06 10:16:40.484990] Process 1. Episode 34950, average_reward -0.068698
Episode 34950: Total Loss of tensor([[-80.6304]], grad_fn=<SubBackward0>)
[2022-11-06 10:16:43.189380] Process 2. Episode 36450, average_reward -0.074376
Episode 36450: Total Loss of tensor([[-37.5766]], grad_fn=<SubBackward0>)
[2022-11-06 10:16:51.254512] Process 0. Episode 35600, average_reward -0.069803
Episode 35600: Total Loss of tensor([[-0.6938]], grad_fn=<SubBackward0>)
[2022-11-06 10:17:48.639847] Process 4. Episode 38050, average_reward -0.071800
Episode 38050: Total Loss of tensor([[5.7035]], grad_fn=<SubBackward0>)
[2022-11-06 10:17:50.291797] Process 3. Episode 35750, average_reward -0.071748
Episode 35750: Total Loss of tensor([[5.8735]], grad_fn=<SubBackward0>)
[2022-11-06 10:18:37.301833] Process 5. Episode 36150, average_reward -0.069654
Episode 36150: Total Loss of tensor([[11.9711]], grad_fn=<SubBackward0>)
[2022-11-06 10:19:12.359164] Process 1. Episode 35000, average_reward -0.068686
Episode 35000: Total Loss of tensor([[-103.7813]], grad_fn=<SubBackward0>)
[2022-11-06 10:19:19.202986] Process 2. Episode 36500, average_reward -0.074356
Episode 36500: Total Loss of tensor([[2.5478]], grad_fn=<SubBackward0>)
[2022-11-06 10:19:19.745967] Process 0. Episode 35650, average_reward -0.069874
Episode 35650: Total Loss of tensor([[24.8433]], grad_fn=<SubBackward0>)
[2022-11-06 10:20:07.531306] Process 4. Episode 38100, average_reward -0.071785
Episode 38100: Total Loss of tensor([[2.5316]], grad_fn=<SubBackward0>)
[2022-11-06 10:20:19.956548] Process 3. Episode 35800, average_reward -0.071732
Episode 35800: Total Loss of tensor([[-1.2931]], grad_fn=<SubBackward0>)
[2022-11-06 10:21:00.791275] Process 5. Episode 36200, average_reward -0.069669
Episode 36200: Total Loss of tensor([[-19.2438]], grad_fn=<SubBackward0>)
[2022-11-06 10:21:44.237661] Process 1. Episode 35050, average_reward -0.068730
Episode 35050: Total Loss of tensor([[0.3768]], grad_fn=<SubBackward0>)
[2022-11-06 10:21:50.698118] Process 2. Episode 36550, average_reward -0.074337
Episode 36550: Total Loss of tensor([[15.0365]], grad_fn=<SubBackward0>)
[2022-11-06 10:21:55.279998] Process 0. Episode 35700, average_reward -0.069832
Episode 35700: Total Loss of tensor([[-0.2451]], grad_fn=<SubBackward0>)
[2022-11-06 10:22:38.201388] Process 4. Episode 38150, average_reward -0.071743
Episode 38150: Total Loss of tensor([[9.3337]], grad_fn=<SubBackward0>)
[2022-11-06 10:22:45.476614] Process 3. Episode 35850, average_reward -0.071743
Episode 35850: Total Loss of tensor([[-22.6903]], grad_fn=<SubBackward0>)
[2022-11-06 10:23:36.803317] Process 5. Episode 36250, average_reward -0.069655
Episode 36250: Total Loss of tensor([[0.0830]], grad_fn=<SubBackward0>)
[2022-11-06 10:24:12.028229] Process 2. Episode 36600, average_reward -0.074262
Episode 36600: Total Loss of tensor([[4.2537]], grad_fn=<SubBackward0>)
[2022-11-06 10:24:15.489341] Process 1. Episode 35100, average_reward -0.068832
Episode 35100: Total Loss of tensor([[-8.0562]], grad_fn=<SubBackward0>)
[2022-11-06 10:24:25.801558] Process 0. Episode 35750, average_reward -0.069790
Episode 35750: Total Loss of tensor([[1.2771]], grad_fn=<SubBackward0>)
[2022-11-06 10:25:02.587851] Process 4. Episode 38200, average_reward -0.071728
Episode 38200: Total Loss of tensor([[1.8490]], grad_fn=<SubBackward0>)
[2022-11-06 10:25:11.703820] Process 3. Episode 35900, average_reward -0.071727
Episode 35900: Total Loss of tensor([[0.6391]], grad_fn=<SubBackward0>)
[2022-11-06 10:26:09.287914] Process 5. Episode 36300, average_reward -0.069669
Episode 36300: Total Loss of tensor([[-116.5029]], grad_fn=<SubBackward0>)
[2022-11-06 10:26:35.308150] Process 2. Episode 36650, average_reward -0.074325
Episode 36650: Total Loss of tensor([[0.1364]], grad_fn=<SubBackward0>)
[2022-11-06 10:26:52.725292] Process 1. Episode 35150, average_reward -0.068819
Episode 35150: Total Loss of tensor([[8.5890]], grad_fn=<SubBackward0>)
[2022-11-06 10:26:55.441979] Process 0. Episode 35800, average_reward -0.069832
Episode 35800: Total Loss of tensor([[3.3248]], grad_fn=<SubBackward0>)
[2022-11-06 10:27:26.832397] Process 4. Episode 38250, average_reward -0.071712
Episode 38250: Total Loss of tensor([[7.7897]], grad_fn=<SubBackward0>)
[2022-11-06 10:27:34.335879] Process 3. Episode 35950, average_reward -0.071711
Episode 35950: Total Loss of tensor([[-6.4826]], grad_fn=<SubBackward0>)
[2022-11-06 10:28:52.822080] Process 5. Episode 36350, average_reward -0.069601
Episode 36350: Total Loss of tensor([[17.3912]], grad_fn=<SubBackward0>)
[2022-11-06 10:28:57.675055] Process 2. Episode 36700, average_reward -0.074360
Episode 36700: Total Loss of tensor([[15.0719]], grad_fn=<SubBackward0>)
[2022-11-06 10:29:32.170495] Process 0. Episode 35850, average_reward -0.069819
Episode 35850: Total Loss of tensor([[-0.2017]], grad_fn=<SubBackward0>)
[2022-11-06 10:29:39.689780] Process 1. Episode 35200, average_reward -0.068807
Episode 35200: Total Loss of tensor([[26.8723]], grad_fn=<SubBackward0>)
[2022-11-06 10:29:49.398437] Process 4. Episode 38300, average_reward -0.071723
Episode 38300: Total Loss of tensor([[6.4351]], grad_fn=<SubBackward0>)
[2022-11-06 10:29:59.570919] Process 3. Episode 36000, average_reward -0.071694
Episode 36000: Total Loss of tensor([[0.6769]], grad_fn=<SubBackward0>)
[2022-11-06 10:31:24.881512] Process 2. Episode 36750, average_reward -0.074367
Episode 36750: Total Loss of tensor([[9.6208]], grad_fn=<SubBackward0>)
[2022-11-06 10:31:26.420600] Process 5. Episode 36400, average_reward -0.069505
Episode 36400: Total Loss of tensor([[4.5279]], grad_fn=<SubBackward0>)
[2022-11-06 10:32:14.613449] Process 4. Episode 38350, average_reward -0.071656
Episode 38350: Total Loss of tensor([[3.7073]], grad_fn=<SubBackward0>)
[2022-11-06 10:32:14.976506] Process 0. Episode 35900, average_reward -0.069777
Episode 35900: Total Loss of tensor([[-108.9526]], grad_fn=<SubBackward0>)
[2022-11-06 10:32:15.152256] Process 1. Episode 35250, average_reward -0.068766
Episode 35250: Total Loss of tensor([[-3.3359]], grad_fn=<SubBackward0>)
[2022-11-06 10:32:36.326387] Process 3. Episode 36050, average_reward -0.071734
Episode 36050: Total Loss of tensor([[3.3829]], grad_fn=<SubBackward0>)
[2022-11-06 10:33:53.986614] Process 5. Episode 36450, average_reward -0.069492
Episode 36450: Total Loss of tensor([[-0.5551]], grad_fn=<SubBackward0>)
[2022-11-06 10:33:55.769362] Process 2. Episode 36800, average_reward -0.074375
Episode 36800: Total Loss of tensor([[2.8463]], grad_fn=<SubBackward0>)
[2022-11-06 10:34:38.258522] Process 4. Episode 38400, average_reward -0.071589
Episode 38400: Total Loss of tensor([[9.2522]], grad_fn=<SubBackward0>)
[2022-11-06 10:34:44.557835] Process 1. Episode 35300, average_reward -0.068782
Episode 35300: Total Loss of tensor([[-1.2635]], grad_fn=<SubBackward0>)
[2022-11-06 10:34:56.986287] Process 0. Episode 35950, average_reward -0.069708
Episode 35950: Total Loss of tensor([[6.7556]], grad_fn=<SubBackward0>)
[2022-11-06 10:35:02.344407] Process 3. Episode 36100, average_reward -0.071745
Episode 36100: Total Loss of tensor([[11.4587]], grad_fn=<SubBackward0>)
[2022-11-06 10:36:21.482585] Process 2. Episode 36850, average_reward -0.074383
Episode 36850: Total Loss of tensor([[8.1230]], grad_fn=<SubBackward0>)
[2022-11-06 10:36:25.471118] Process 5. Episode 36500, average_reward -0.069562
Episode 36500: Total Loss of tensor([[16.9457]], grad_fn=<SubBackward0>)
[2022-11-06 10:37:09.617740] Process 4. Episode 38450, average_reward -0.071573
Episode 38450: Total Loss of tensor([[5.8452]], grad_fn=<SubBackward0>)
[2022-11-06 10:37:16.006952] Process 1. Episode 35350, average_reward -0.068769
Episode 35350: Total Loss of tensor([[8.9155]], grad_fn=<SubBackward0>)
[2022-11-06 10:37:30.543310] Process 0. Episode 36000, average_reward -0.069722
Episode 36000: Total Loss of tensor([[9.8937]], grad_fn=<SubBackward0>)
[2022-11-06 10:37:38.219160] Process 3. Episode 36150, average_reward -0.071646
Episode 36150: Total Loss of tensor([[8.2547]], grad_fn=<SubBackward0>)
[2022-11-06 10:38:52.982499] Process 2. Episode 36900, average_reward -0.074417
Episode 36900: Total Loss of tensor([[-1.2799]], grad_fn=<SubBackward0>)
[2022-11-06 10:38:58.429693] Process 5. Episode 36550, average_reward -0.069603
Episode 36550: Total Loss of tensor([[-4.9855]], grad_fn=<SubBackward0>)
[2022-11-06 10:39:39.953759] Process 4. Episode 38500, average_reward -0.071558
Episode 38500: Total Loss of tensor([[5.2827]], grad_fn=<SubBackward0>)
[2022-11-06 10:39:57.431435] Process 1. Episode 35400, average_reward -0.068757
Episode 35400: Total Loss of tensor([[4.9746]], grad_fn=<SubBackward0>)
[2022-11-06 10:40:07.455084] Process 3. Episode 36200, average_reward -0.071657
Episode 36200: Total Loss of tensor([[3.7052]], grad_fn=<SubBackward0>)
[2022-11-06 10:40:15.954654] Process 0. Episode 36050, average_reward -0.069792
Episode 36050: Total Loss of tensor([[15.3168]], grad_fn=<SubBackward0>)
[2022-11-06 10:41:19.999641] Process 2. Episode 36950, average_reward -0.074371
Episode 36950: Total Loss of tensor([[2.8667]], grad_fn=<SubBackward0>)
[2022-11-06 10:41:22.887566] Process 5. Episode 36600, average_reward -0.069645
Episode 36600: Total Loss of tensor([[13.5483]], grad_fn=<SubBackward0>)
[2022-11-06 10:42:06.641623] Process 4. Episode 38550, average_reward -0.071543
Episode 38550: Total Loss of tensor([[-72.0019]], grad_fn=<SubBackward0>)
[2022-11-06 10:42:28.984499] Process 1. Episode 35450, average_reward -0.068773
Episode 35450: Total Loss of tensor([[0.7203]], grad_fn=<SubBackward0>)
[2022-11-06 10:42:39.951313] Process 3. Episode 36250, average_reward -0.071697
Episode 36250: Total Loss of tensor([[3.4429]], grad_fn=<SubBackward0>)
[2022-11-06 10:42:47.703532] Process 0. Episode 36100, average_reward -0.069778
Episode 36100: Total Loss of tensor([[15.6959]], grad_fn=<SubBackward0>)
[2022-11-06 10:43:47.856070] Process 2. Episode 37000, average_reward -0.074405
Episode 37000: Total Loss of tensor([[4.2610]], grad_fn=<SubBackward0>)
[2022-11-06 10:43:53.806549] Process 5. Episode 36650, average_reward -0.069659
Episode 36650: Total Loss of tensor([[5.4324]], grad_fn=<SubBackward0>)
[2022-11-06 10:44:31.496949] Process 4. Episode 38600, average_reward -0.071580
Episode 38600: Total Loss of tensor([[5.3169]], grad_fn=<SubBackward0>)
[2022-11-06 10:44:58.774684] Process 1. Episode 35500, average_reward -0.068732
Episode 35500: Total Loss of tensor([[1.4433]], grad_fn=<SubBackward0>)
[2022-11-06 10:45:16.000557] Process 3. Episode 36300, average_reward -0.071736
Episode 36300: Total Loss of tensor([[7.3324]], grad_fn=<SubBackward0>)
[2022-11-06 10:45:20.238025] Process 0. Episode 36150, average_reward -0.069876
Episode 36150: Total Loss of tensor([[11.6146]], grad_fn=<SubBackward0>)
[2022-11-06 10:46:13.446320] Process 2. Episode 37050, average_reward -0.074332
Episode 37050: Total Loss of tensor([[9.5120]], grad_fn=<SubBackward0>)
[2022-11-06 10:46:21.337481] Process 5. Episode 36700, average_reward -0.069619
Episode 36700: Total Loss of tensor([[5.3863]], grad_fn=<SubBackward0>)
[2022-11-06 10:46:53.761118] Process 4. Episode 38650, average_reward -0.071565
Episode 38650: Total Loss of tensor([[15.0679]], grad_fn=<SubBackward0>)
[2022-11-06 10:47:26.743824] Process 1. Episode 35550, average_reward -0.068664
Episode 35550: Total Loss of tensor([[-38.1575]], grad_fn=<SubBackward0>)
[2022-11-06 10:47:52.420625] Process 3. Episode 36350, average_reward -0.071774
Episode 36350: Total Loss of tensor([[6.5241]], grad_fn=<SubBackward0>)
[2022-11-06 10:48:01.526802] Process 0. Episode 36200, average_reward -0.069834
Episode 36200: Total Loss of tensor([[-1.2646]], grad_fn=<SubBackward0>)
[2022-11-06 10:48:39.485475] Process 2. Episode 37100, average_reward -0.074286
Episode 37100: Total Loss of tensor([[-23.7207]], grad_fn=<SubBackward0>)
[2022-11-06 10:48:55.769869] Process 5. Episode 36750, average_reward -0.069605
Episode 36750: Total Loss of tensor([[-27.7437]], grad_fn=<SubBackward0>)
[2022-11-06 10:49:18.289562] Process 4. Episode 38700, average_reward -0.071628
Episode 38700: Total Loss of tensor([[-30.1226]], grad_fn=<SubBackward0>)
[2022-11-06 10:50:01.756953] Process 1. Episode 35600, average_reward -0.068624
Episode 35600: Total Loss of tensor([[11.8373]], grad_fn=<SubBackward0>)
[2022-11-06 10:50:28.470544] Process 3. Episode 36400, average_reward -0.071703
Episode 36400: Total Loss of tensor([[12.9204]], grad_fn=<SubBackward0>)
[2022-11-06 10:50:33.244088] Process 0. Episode 36250, average_reward -0.069821
Episode 36250: Total Loss of tensor([[-1.1766]], grad_fn=<SubBackward0>)
[2022-11-06 10:51:14.424541] Process 2. Episode 37150, average_reward -0.074266
Episode 37150: Total Loss of tensor([[13.0949]], grad_fn=<SubBackward0>)
[2022-11-06 10:51:20.931489] Process 5. Episode 36800, average_reward -0.069592
Episode 36800: Total Loss of tensor([[-81.3440]], grad_fn=<SubBackward0>)
[2022-11-06 10:51:39.148695] Process 4. Episode 38750, average_reward -0.071613
Episode 38750: Total Loss of tensor([[-1.8876]], grad_fn=<SubBackward0>)
[2022-11-06 10:52:40.520508] Process 1. Episode 35650, average_reward -0.068724
Episode 35650: Total Loss of tensor([[7.4640]], grad_fn=<SubBackward0>)
[2022-11-06 10:53:02.147126] Process 3. Episode 36450, average_reward -0.071687
Episode 36450: Total Loss of tensor([[12.6921]], grad_fn=<SubBackward0>)
[2022-11-06 10:53:17.564233] Process 0. Episode 36300, average_reward -0.069917
Episode 36300: Total Loss of tensor([[-88.4083]], grad_fn=<SubBackward0>)
[2022-11-06 10:53:40.839441] Process 2. Episode 37200, average_reward -0.074355
Episode 37200: Total Loss of tensor([[11.8757]], grad_fn=<SubBackward0>)
[2022-11-06 10:53:48.832748] Process 5. Episode 36850, average_reward -0.069607
Episode 36850: Total Loss of tensor([[0.5700]], grad_fn=<SubBackward0>)
[2022-11-06 10:53:58.970984] Process 4. Episode 38800, average_reward -0.071624
Episode 38800: Total Loss of tensor([[11.8922]], grad_fn=<SubBackward0>)
[2022-11-06 10:55:17.945191] Process 1. Episode 35700, average_reward -0.068739
Episode 35700: Total Loss of tensor([[5.5342]], grad_fn=<SubBackward0>)
[2022-11-06 10:55:31.003012] Process 3. Episode 36500, average_reward -0.071616
Episode 36500: Total Loss of tensor([[10.8520]], grad_fn=<SubBackward0>)
[2022-11-06 10:55:55.105089] Process 0. Episode 36350, average_reward -0.069904
Episode 36350: Total Loss of tensor([[4.9856]], grad_fn=<SubBackward0>)
[2022-11-06 10:56:11.622193] Process 2. Episode 37250, average_reward -0.074336
Episode 37250: Total Loss of tensor([[16.1125]], grad_fn=<SubBackward0>)
[2022-11-06 10:56:13.000843] Process 5. Episode 36900, average_reward -0.069566
Episode 36900: Total Loss of tensor([[14.5508]], grad_fn=<SubBackward0>)
[2022-11-06 10:56:20.406177] Process 4. Episode 38850, average_reward -0.071609
Episode 38850: Total Loss of tensor([[14.7334]], grad_fn=<SubBackward0>)
[2022-11-06 10:57:50.974077] Process 1. Episode 35750, average_reward -0.068755
Episode 35750: Total Loss of tensor([[15.6760]], grad_fn=<SubBackward0>)
[2022-11-06 10:57:59.294298] Process 3. Episode 36550, average_reward -0.071683
Episode 36550: Total Loss of tensor([[14.1963]], grad_fn=<SubBackward0>)
[2022-11-06 10:58:28.365254] Process 0. Episode 36400, average_reward -0.069945
Episode 36400: Total Loss of tensor([[16.6563]], grad_fn=<SubBackward0>)
[2022-11-06 10:58:36.306067] Process 2. Episode 37300, average_reward -0.074236
Episode 37300: Total Loss of tensor([[11.5978]], grad_fn=<SubBackward0>)
[2022-11-06 10:58:41.013150] Process 5. Episode 36950, average_reward -0.069499
Episode 36950: Total Loss of tensor([[13.0496]], grad_fn=<SubBackward0>)
[2022-11-06 10:58:41.580829] Process 4. Episode 38900, average_reward -0.071671
Episode 38900: Total Loss of tensor([[14.7369]], grad_fn=<SubBackward0>)
[2022-11-06 11:00:22.215147] Process 1. Episode 35800, average_reward -0.068715
Episode 35800: Total Loss of tensor([[9.1643]], grad_fn=<SubBackward0>)
[2022-11-06 11:00:22.241038] Process 3. Episode 36600, average_reward -0.071667
Episode 36600: Total Loss of tensor([[1.7249]], grad_fn=<SubBackward0>)
[2022-11-06 11:01:04.324918] Process 4. Episode 38950, average_reward -0.071759
Episode 38950: Total Loss of tensor([[17.0181]], grad_fn=<SubBackward0>)
[2022-11-06 11:01:05.945623] Process 2. Episode 37350, average_reward -0.074244
Episode 37350: Total Loss of tensor([[7.4458]], grad_fn=<SubBackward0>)
[2022-11-06 11:01:15.192333] Process 0. Episode 36450, average_reward -0.069877
Episode 36450: Total Loss of tensor([[2.0256]], grad_fn=<SubBackward0>)
[2022-11-06 11:01:26.726945] Process 5. Episode 37000, average_reward -0.069568
Episode 37000: Total Loss of tensor([[8.8050]], grad_fn=<SubBackward0>)
[2022-11-06 11:02:43.018454] Process 3. Episode 36650, average_reward -0.071623
Episode 36650: Total Loss of tensor([[-2.3753]], grad_fn=<SubBackward0>)
[2022-11-06 11:02:49.351361] Process 1. Episode 35850, average_reward -0.068731
Episode 35850: Total Loss of tensor([[11.2077]], grad_fn=<SubBackward0>)
[2022-11-06 11:03:22.673059] Process 4. Episode 39000, average_reward -0.071692
Episode 39000: Total Loss of tensor([[8.0781]], grad_fn=<SubBackward0>)
[2022-11-06 11:03:39.155977] Process 2. Episode 37400, average_reward -0.074251
Episode 37400: Total Loss of tensor([[14.1274]], grad_fn=<SubBackward0>)
[2022-11-06 11:03:52.120013] Process 0. Episode 36500, average_reward -0.069918
Episode 36500: Total Loss of tensor([[10.8502]], grad_fn=<SubBackward0>)
[2022-11-06 11:04:06.777844] Process 5. Episode 37050, average_reward -0.069690
Episode 37050: Total Loss of tensor([[-126.8775]], grad_fn=<SubBackward0>)
[2022-11-06 11:05:05.685054] Process 3. Episode 36700, average_reward -0.071717
Episode 36700: Total Loss of tensor([[-5.2663]], grad_fn=<SubBackward0>)
[2022-11-06 11:05:28.737257] Process 1. Episode 35900, average_reward -0.068774
Episode 35900: Total Loss of tensor([[6.6381]], grad_fn=<SubBackward0>)
[2022-11-06 11:05:44.257912] Process 4. Episode 39050, average_reward -0.071729
Episode 39050: Total Loss of tensor([[14.0265]], grad_fn=<SubBackward0>)
[2022-11-06 11:06:06.800711] Process 2. Episode 37450, average_reward -0.074179
Episode 37450: Total Loss of tensor([[13.5518]], grad_fn=<SubBackward0>)
Training process 4 terminated
[2022-11-06 11:06:28.838611] Process 0. Episode 36550, average_reward -0.069932
Episode 36550: Total Loss of tensor([[10.3291]], grad_fn=<SubBackward0>)
[2022-11-06 11:06:30.297782] Process 5. Episode 37100, average_reward -0.069704
Episode 37100: Total Loss of tensor([[5.9574]], grad_fn=<SubBackward0>)
[2022-11-06 11:07:32.750843] Process 3. Episode 36750, average_reward -0.071728
Episode 36750: Total Loss of tensor([[11.2616]], grad_fn=<SubBackward0>)
[2022-11-06 11:07:51.657049] Process 1. Episode 35950, average_reward -0.068790
Episode 35950: Total Loss of tensor([[10.4409]], grad_fn=<SubBackward0>)
[2022-11-06 11:08:22.415690] Process 2. Episode 37500, average_reward -0.074187
Episode 37500: Total Loss of tensor([[5.6789]], grad_fn=<SubBackward0>)
[2022-11-06 11:08:44.997981] Process 0. Episode 36600, average_reward -0.069973
Episode 36600: Total Loss of tensor([[3.8830]], grad_fn=<SubBackward0>)
[2022-11-06 11:08:47.207112] Process 5. Episode 37150, average_reward -0.069664
Episode 37150: Total Loss of tensor([[16.7425]], grad_fn=<SubBackward0>)
[2022-11-06 11:09:53.147457] Process 3. Episode 36800, average_reward -0.071739
Episode 36800: Total Loss of tensor([[10.1594]], grad_fn=<SubBackward0>)
[2022-11-06 11:10:02.522074] Process 1. Episode 36000, average_reward -0.068806
Episode 36000: Total Loss of tensor([[8.2784]], grad_fn=<SubBackward0>)
[2022-11-06 11:10:44.048300] Process 2. Episode 37550, average_reward -0.074194
Episode 37550: Total Loss of tensor([[15.5642]], grad_fn=<SubBackward0>)
[2022-11-06 11:10:54.767912] Process 5. Episode 37200, average_reward -0.069677
Episode 37200: Total Loss of tensor([[4.0450]], grad_fn=<SubBackward0>)
[2022-11-06 11:11:11.769313] Process 0. Episode 36650, average_reward -0.069905
Episode 36650: Total Loss of tensor([[-2.7012]], grad_fn=<SubBackward0>)
[2022-11-06 11:12:13.330770] Process 3. Episode 36850, average_reward -0.071669
Episode 36850: Total Loss of tensor([[13.5769]], grad_fn=<SubBackward0>)
[2022-11-06 11:12:23.867181] Process 1. Episode 36050, average_reward -0.068738
Episode 36050: Total Loss of tensor([[2.2316]], grad_fn=<SubBackward0>)
[2022-11-06 11:13:07.806833] Process 2. Episode 37600, average_reward -0.074202
Episode 37600: Total Loss of tensor([[-33.8006]], grad_fn=<SubBackward0>)
[2022-11-06 11:13:09.444744] Process 5. Episode 37250, average_reward -0.069638
Episode 37250: Total Loss of tensor([[11.9606]], grad_fn=<SubBackward0>)
[2022-11-06 11:13:38.145023] Process 0. Episode 36700, average_reward -0.069891
Episode 36700: Total Loss of tensor([[11.3650]], grad_fn=<SubBackward0>)
[2022-11-06 11:14:27.985523] Process 3. Episode 36900, average_reward -0.071653
Episode 36900: Total Loss of tensor([[13.6155]], grad_fn=<SubBackward0>)
[2022-11-06 11:14:44.010722] Process 1. Episode 36100, average_reward -0.068809
Episode 36100: Total Loss of tensor([[0.9701]], grad_fn=<SubBackward0>)
[2022-11-06 11:15:18.547255] Process 5. Episode 37300, average_reward -0.069625
Episode 37300: Total Loss of tensor([[-113.4071]], grad_fn=<SubBackward0>)
[2022-11-06 11:15:30.916015] Process 2. Episode 37650, average_reward -0.074157
Episode 37650: Total Loss of tensor([[6.8275]], grad_fn=<SubBackward0>)
[2022-11-06 11:15:56.214302] Process 0. Episode 36750, average_reward -0.069878
Episode 36750: Total Loss of tensor([[14.1424]], grad_fn=<SubBackward0>)
[2022-11-06 11:16:48.500971] Process 3. Episode 36950, average_reward -0.071691
Episode 36950: Total Loss of tensor([[16.7025]], grad_fn=<SubBackward0>)
[2022-11-06 11:17:04.573014] Process 1. Episode 36150, average_reward -0.068797
Episode 36150: Total Loss of tensor([[-98.7117]], grad_fn=<SubBackward0>)
[2022-11-06 11:17:31.535490] Process 5. Episode 37350, average_reward -0.069639
Episode 37350: Total Loss of tensor([[13.9849]], grad_fn=<SubBackward0>)
[2022-11-06 11:17:45.162426] Process 2. Episode 37700, average_reward -0.074111
Episode 37700: Total Loss of tensor([[10.1407]], grad_fn=<SubBackward0>)
[2022-11-06 11:18:16.254369] Process 0. Episode 36800, average_reward -0.069837
Episode 36800: Total Loss of tensor([[2.3617]], grad_fn=<SubBackward0>)
[2022-11-06 11:19:03.054066] Process 3. Episode 37000, average_reward -0.071730
Episode 37000: Total Loss of tensor([[-3.5833]], grad_fn=<SubBackward0>)
[2022-11-06 11:19:30.371375] Process 1. Episode 36200, average_reward -0.068757
Episode 36200: Total Loss of tensor([[15.4903]], grad_fn=<SubBackward0>)
[2022-11-06 11:19:51.252742] Process 5. Episode 37400, average_reward -0.069652
Episode 37400: Total Loss of tensor([[1.7700]], grad_fn=<SubBackward0>)
[2022-11-06 11:19:58.124709] Process 2. Episode 37750, average_reward -0.074119
Episode 37750: Total Loss of tensor([[8.3186]], grad_fn=<SubBackward0>)
[2022-11-06 11:20:33.164579] Process 0. Episode 36850, average_reward -0.069796
Episode 36850: Total Loss of tensor([[-2.9153]], grad_fn=<SubBackward0>)
[2022-11-06 11:21:28.573293] Process 3. Episode 37050, average_reward -0.071714
Episode 37050: Total Loss of tensor([[5.2603]], grad_fn=<SubBackward0>)
[2022-11-06 11:21:46.720055] Process 1. Episode 36250, average_reward -0.068745
Episode 36250: Total Loss of tensor([[24.8729]], grad_fn=<SubBackward0>)
[2022-11-06 11:22:02.071967] Process 5. Episode 37450, average_reward -0.069640
Episode 37450: Total Loss of tensor([[6.9984]], grad_fn=<SubBackward0>)
[2022-11-06 11:22:23.161988] Process 2. Episode 37800, average_reward -0.074101
Episode 37800: Total Loss of tensor([[13.0542]], grad_fn=<SubBackward0>)
[2022-11-06 11:22:45.027511] Process 0. Episode 36900, average_reward -0.069783
Episode 36900: Total Loss of tensor([[3.2532]], grad_fn=<SubBackward0>)
[2022-11-06 11:23:46.638474] Process 3. Episode 37100, average_reward -0.071671
Episode 37100: Total Loss of tensor([[8.7630]], grad_fn=<SubBackward0>)
[2022-11-06 11:24:16.817472] Process 1. Episode 36300, average_reward -0.068705
Episode 36300: Total Loss of tensor([[5.2240]], grad_fn=<SubBackward0>)
[2022-11-06 11:24:18.598982] Process 5. Episode 37500, average_reward -0.069680
Episode 37500: Total Loss of tensor([[5.2137]], grad_fn=<SubBackward0>)
[2022-11-06 11:24:40.857343] Process 2. Episode 37850, average_reward -0.074108
Episode 37850: Total Loss of tensor([[1.3522]], grad_fn=<SubBackward0>)
[2022-11-06 11:24:57.296414] Process 0. Episode 36950, average_reward -0.069824
Episode 36950: Total Loss of tensor([[17.1932]], grad_fn=<SubBackward0>)
[2022-11-06 11:26:03.173846] Process 3. Episode 37150, average_reward -0.071682
Episode 37150: Total Loss of tensor([[-117.8149]], grad_fn=<SubBackward0>)
[2022-11-06 11:26:33.891804] Process 5. Episode 37550, average_reward -0.069667
Episode 37550: Total Loss of tensor([[-62.3759]], grad_fn=<SubBackward0>)
[2022-11-06 11:26:38.065176] Process 1. Episode 36350, average_reward -0.068748
Episode 36350: Total Loss of tensor([[3.4045]], grad_fn=<SubBackward0>)
[2022-11-06 11:27:01.942774] Process 2. Episode 37900, average_reward -0.074169
Episode 37900: Total Loss of tensor([[-94.3300]], grad_fn=<SubBackward0>)
[2022-11-06 11:27:09.579717] Process 0. Episode 37000, average_reward -0.069811
Episode 37000: Total Loss of tensor([[6.0254]], grad_fn=<SubBackward0>)
[2022-11-06 11:28:16.851299] Process 3. Episode 37200, average_reward -0.071694
Episode 37200: Total Loss of tensor([[-6.7388]], grad_fn=<SubBackward0>)
[2022-11-06 11:28:47.291659] Process 5. Episode 37600, average_reward -0.069707
Episode 37600: Total Loss of tensor([[0.3872]], grad_fn=<SubBackward0>)
[2022-11-06 11:29:01.758248] Process 1. Episode 36400, average_reward -0.068791
Episode 36400: Total Loss of tensor([[0.2006]], grad_fn=<SubBackward0>)
[2022-11-06 11:29:17.052480] Process 2. Episode 37950, average_reward -0.074097
Episode 37950: Total Loss of tensor([[6.5111]], grad_fn=<SubBackward0>)
[2022-11-06 11:29:28.728866] Process 0. Episode 37050, average_reward -0.069825
Episode 37050: Total Loss of tensor([[2.7632]], grad_fn=<SubBackward0>)
[2022-11-06 11:30:29.084999] Process 3. Episode 37250, average_reward -0.071678
Episode 37250: Total Loss of tensor([[0.7666]], grad_fn=<SubBackward0>)
[2022-11-06 11:31:00.670180] Process 5. Episode 37650, average_reward -0.069721
Episode 37650: Total Loss of tensor([[-1.4510]], grad_fn=<SubBackward0>)
[2022-11-06 11:31:27.668668] Process 1. Episode 36450, average_reward -0.068752
Episode 36450: Total Loss of tensor([[6.1091]], grad_fn=<SubBackward0>)
[2022-11-06 11:31:35.243756] Process 2. Episode 38000, average_reward -0.074026
Episode 38000: Total Loss of tensor([[-2.3787]], grad_fn=<SubBackward0>)
[2022-11-06 11:31:43.023244] Process 0. Episode 37100, average_reward -0.069784
Episode 37100: Total Loss of tensor([[-0.1456]], grad_fn=<SubBackward0>)
[2022-11-06 11:32:45.550390] Process 3. Episode 37300, average_reward -0.071609
Episode 37300: Total Loss of tensor([[8.6029]], grad_fn=<SubBackward0>)
[2022-11-06 11:33:11.681021] Process 5. Episode 37700, average_reward -0.069788
Episode 37700: Total Loss of tensor([[3.9093]], grad_fn=<SubBackward0>)
[2022-11-06 11:33:54.080344] Process 1. Episode 36500, average_reward -0.068795
Episode 36500: Total Loss of tensor([[4.3444]], grad_fn=<SubBackward0>)
[2022-11-06 11:33:54.213150] Process 2. Episode 38050, average_reward -0.074034
Episode 38050: Total Loss of tensor([[-5.0721]], grad_fn=<SubBackward0>)
[2022-11-06 11:34:09.191358] Process 0. Episode 37150, average_reward -0.069798
Episode 37150: Total Loss of tensor([[15.1812]], grad_fn=<SubBackward0>)
[2022-11-06 11:34:58.346804] Process 3. Episode 37350, average_reward -0.071539
Episode 37350: Total Loss of tensor([[3.6529]], grad_fn=<SubBackward0>)
[2022-11-06 11:35:23.191448] Process 5. Episode 37750, average_reward -0.069801
Episode 37750: Total Loss of tensor([[9.6196]], grad_fn=<SubBackward0>)
[2022-11-06 11:36:15.975624] Process 2. Episode 38100, average_reward -0.074094
Episode 38100: Total Loss of tensor([[-37.3666]], grad_fn=<SubBackward0>)
[2022-11-06 11:36:16.169240] Process 1. Episode 36550, average_reward -0.068755
Episode 36550: Total Loss of tensor([[0.3064]], grad_fn=<SubBackward0>)
[2022-11-06 11:36:25.078426] Process 0. Episode 37200, average_reward -0.069785
Episode 37200: Total Loss of tensor([[6.9565]], grad_fn=<SubBackward0>)
[2022-11-06 11:37:14.100424] Process 3. Episode 37400, average_reward -0.071578
Episode 37400: Total Loss of tensor([[-52.2151]], grad_fn=<SubBackward0>)
[2022-11-06 11:37:32.318100] Process 5. Episode 37800, average_reward -0.069788
Episode 37800: Total Loss of tensor([[11.6800]], grad_fn=<SubBackward0>)
[2022-11-06 11:38:29.629645] Process 2. Episode 38150, average_reward -0.073997
Episode 38150: Total Loss of tensor([[3.4578]], grad_fn=<SubBackward0>)
[2022-11-06 11:38:35.731379] Process 1. Episode 36600, average_reward -0.068770
Episode 36600: Total Loss of tensor([[10.3058]], grad_fn=<SubBackward0>)
[2022-11-06 11:38:48.191370] Process 0. Episode 37250, average_reward -0.069879
Episode 37250: Total Loss of tensor([[9.6374]], grad_fn=<SubBackward0>)
[2022-11-06 11:39:27.716720] Process 3. Episode 37450, average_reward -0.071562
Episode 37450: Total Loss of tensor([[5.6555]], grad_fn=<SubBackward0>)
[2022-11-06 11:39:46.246529] Process 5. Episode 37850, average_reward -0.069828
Episode 37850: Total Loss of tensor([[-1.1161]], grad_fn=<SubBackward0>)
[2022-11-06 11:40:40.405809] Process 2. Episode 38200, average_reward -0.073901
Episode 38200: Total Loss of tensor([[11.6503]], grad_fn=<SubBackward0>)
[2022-11-06 11:41:01.346347] Process 1. Episode 36650, average_reward -0.068759
Episode 36650: Total Loss of tensor([[20.0710]], grad_fn=<SubBackward0>)
[2022-11-06 11:41:08.716039] Process 0. Episode 37300, average_reward -0.069893
Episode 37300: Total Loss of tensor([[-1.6282]], grad_fn=<SubBackward0>)
[2022-11-06 11:41:43.607748] Process 3. Episode 37500, average_reward -0.071600
Episode 37500: Total Loss of tensor([[11.3058]], grad_fn=<SubBackward0>)
[2022-11-06 11:41:57.759413] Process 5. Episode 37900, average_reward -0.069868
Episode 37900: Total Loss of tensor([[8.1846]], grad_fn=<SubBackward0>)
[2022-11-06 11:43:03.070787] Process 2. Episode 38250, average_reward -0.073830
Episode 38250: Total Loss of tensor([[10.9280]], grad_fn=<SubBackward0>)
[2022-11-06 11:43:23.347373] Process 1. Episode 36700, average_reward -0.068801
Episode 36700: Total Loss of tensor([[-6.2167]], grad_fn=<SubBackward0>)
[2022-11-06 11:43:33.550667] Process 0. Episode 37350, average_reward -0.069960
Episode 37350: Total Loss of tensor([[20.9724]], grad_fn=<SubBackward0>)
[2022-11-06 11:43:54.886271] Process 3. Episode 37550, average_reward -0.071611
Episode 37550: Total Loss of tensor([[5.7281]], grad_fn=<SubBackward0>)
[2022-11-06 11:44:09.123727] Process 5. Episode 37950, average_reward -0.069855
Episode 37950: Total Loss of tensor([[9.8520]], grad_fn=<SubBackward0>)
[2022-11-06 11:45:25.764132] Process 2. Episode 38300, average_reward -0.073838
Episode 38300: Total Loss of tensor([[12.2344]], grad_fn=<SubBackward0>)
[2022-11-06 11:45:42.972790] Process 1. Episode 36750, average_reward -0.068707
Episode 36750: Total Loss of tensor([[1.9167]], grad_fn=<SubBackward0>)
[2022-11-06 11:45:59.402925] Process 0. Episode 37400, average_reward -0.069973
Episode 37400: Total Loss of tensor([[4.5760]], grad_fn=<SubBackward0>)
[2022-11-06 11:46:05.001920] Process 3. Episode 37600, average_reward -0.071622
Episode 37600: Total Loss of tensor([[6.3602]], grad_fn=<SubBackward0>)
[2022-11-06 11:46:19.791022] Process 5. Episode 38000, average_reward -0.069947
Episode 38000: Total Loss of tensor([[3.8566]], grad_fn=<SubBackward0>)
[2022-11-06 11:47:46.941515] Process 2. Episode 38350, average_reward -0.073872
Episode 38350: Total Loss of tensor([[-17.0123]], grad_fn=<SubBackward0>)
[2022-11-06 11:48:04.822670] Process 1. Episode 36800, average_reward -0.068696
Episode 36800: Total Loss of tensor([[9.0673]], grad_fn=<SubBackward0>)
[2022-11-06 11:48:14.525664] Process 3. Episode 37650, average_reward -0.071660
Episode 37650: Total Loss of tensor([[-4.3094]], grad_fn=<SubBackward0>)
[2022-11-06 11:48:23.783479] Process 0. Episode 37450, average_reward -0.069880
Episode 37450: Total Loss of tensor([[5.8145]], grad_fn=<SubBackward0>)
[2022-11-06 11:48:30.703099] Process 5. Episode 38050, average_reward -0.069987
Episode 38050: Total Loss of tensor([[-7.1577]], grad_fn=<SubBackward0>)
[2022-11-06 11:50:09.605699] Process 2. Episode 38400, average_reward -0.073854
Episode 38400: Total Loss of tensor([[7.6055]], grad_fn=<SubBackward0>)
[2022-11-06 11:50:24.601377] Process 1. Episode 36850, average_reward -0.068711
Episode 36850: Total Loss of tensor([[2.9372]], grad_fn=<SubBackward0>)
[2022-11-06 11:50:28.206948] Process 3. Episode 37700, average_reward -0.071645
Episode 37700: Total Loss of tensor([[3.8066]], grad_fn=<SubBackward0>)
[2022-11-06 11:50:42.645515] Process 5. Episode 38100, average_reward -0.070079
Episode 38100: Total Loss of tensor([[7.2623]], grad_fn=<SubBackward0>)
[2022-11-06 11:50:42.787434] Process 0. Episode 37500, average_reward -0.069867
Episode 37500: Total Loss of tensor([[9.5528]], grad_fn=<SubBackward0>)
[2022-11-06 11:52:35.004253] Process 2. Episode 38450, average_reward -0.073810
Episode 38450: Total Loss of tensor([[1.7107]], grad_fn=<SubBackward0>)
[2022-11-06 11:52:44.211342] Process 1. Episode 36900, average_reward -0.068753
Episode 36900: Total Loss of tensor([[-22.4129]], grad_fn=<SubBackward0>)
[2022-11-06 11:52:49.803665] Process 3. Episode 37750, average_reward -0.071762
Episode 37750: Total Loss of tensor([[-6.3465]], grad_fn=<SubBackward0>)
[2022-11-06 11:52:53.494276] Process 5. Episode 38150, average_reward -0.070118
Episode 38150: Total Loss of tensor([[7.9966]], grad_fn=<SubBackward0>)
[2022-11-06 11:53:04.608775] Process 0. Episode 37550, average_reward -0.069800
Episode 37550: Total Loss of tensor([[15.8267]], grad_fn=<SubBackward0>)
[2022-11-06 11:54:50.975551] Process 2. Episode 38500, average_reward -0.073792
Episode 38500: Total Loss of tensor([[-10.2888]], grad_fn=<SubBackward0>)
[2022-11-06 11:55:03.290131] Process 1. Episode 36950, average_reward -0.068850
Episode 36950: Total Loss of tensor([[25.3037]], grad_fn=<SubBackward0>)
[2022-11-06 11:55:08.209262] Process 5. Episode 38200, average_reward -0.070131
Episode 38200: Total Loss of tensor([[17.1764]], grad_fn=<SubBackward0>)
[2022-11-06 11:55:12.230681] Process 3. Episode 37800, average_reward -0.071720
Episode 37800: Total Loss of tensor([[-2.2687]], grad_fn=<SubBackward0>)
[2022-11-06 11:55:23.686515] Process 0. Episode 37600, average_reward -0.069761
Episode 37600: Total Loss of tensor([[15.4486]], grad_fn=<SubBackward0>)
[2022-11-06 11:57:13.163667] Process 1. Episode 37000, average_reward -0.068892
Episode 37000: Total Loss of tensor([[2.0217]], grad_fn=<SubBackward0>)
[2022-11-06 11:57:14.547928] Process 2. Episode 38550, average_reward -0.073696
Episode 38550: Total Loss of tensor([[16.4201]], grad_fn=<SubBackward0>)
[2022-11-06 11:57:22.440385] Process 5. Episode 38250, average_reward -0.070092
Episode 38250: Total Loss of tensor([[9.0434]], grad_fn=<SubBackward0>)
[2022-11-06 11:57:32.162906] Process 3. Episode 37850, average_reward -0.071625
Episode 37850: Total Loss of tensor([[-2.6095]], grad_fn=<SubBackward0>)
[2022-11-06 11:57:46.585822] Process 0. Episode 37650, average_reward -0.069854
Episode 37650: Total Loss of tensor([[16.7856]], grad_fn=<SubBackward0>)
[2022-11-06 11:59:28.439377] Process 1. Episode 37050, average_reward -0.068826
Episode 37050: Total Loss of tensor([[-24.5797]], grad_fn=<SubBackward0>)
[2022-11-06 11:59:40.465701] Process 5. Episode 38300, average_reward -0.070026
Episode 38300: Total Loss of tensor([[3.7618]], grad_fn=<SubBackward0>)
[2022-11-06 11:59:41.398300] Process 3. Episode 37900, average_reward -0.071689
Episode 37900: Total Loss of tensor([[0.5378]], grad_fn=<SubBackward0>)
[2022-11-06 11:59:42.070114] Process 2. Episode 38600, average_reward -0.073731
Episode 38600: Total Loss of tensor([[10.3399]], grad_fn=<SubBackward0>)
[2022-11-06 12:00:04.591291] Process 0. Episode 37700, average_reward -0.069867
Episode 37700: Total Loss of tensor([[-0.8002]], grad_fn=<SubBackward0>)
[2022-11-06 12:01:44.283639] Process 1. Episode 37100, average_reward -0.068895
Episode 37100: Total Loss of tensor([[-15.1799]], grad_fn=<SubBackward0>)
[2022-11-06 12:01:50.828520] Process 3. Episode 37950, average_reward -0.071700
Episode 37950: Total Loss of tensor([[-109.9041]], grad_fn=<SubBackward0>)
[2022-11-06 12:01:58.177422] Process 5. Episode 38350, average_reward -0.070091
Episode 38350: Total Loss of tensor([[-106.8115]], grad_fn=<SubBackward0>)
[2022-11-06 12:02:00.052423] Process 2. Episode 38650, average_reward -0.073739
Episode 38650: Total Loss of tensor([[-119.3164]], grad_fn=<SubBackward0>)
[2022-11-06 12:02:26.787594] Process 0. Episode 37750, average_reward -0.069775
Episode 37750: Total Loss of tensor([[11.3437]], grad_fn=<SubBackward0>)
[2022-11-06 12:04:02.770178] Process 3. Episode 38000, average_reward -0.071605
Episode 38000: Total Loss of tensor([[2.6525]], grad_fn=<SubBackward0>)
[2022-11-06 12:04:11.007767] Process 1. Episode 37150, average_reward -0.068883
Episode 37150: Total Loss of tensor([[-28.7662]], grad_fn=<SubBackward0>)
[2022-11-06 12:04:14.093636] Process 2. Episode 38700, average_reward -0.073695
Episode 38700: Total Loss of tensor([[-120.5795]], grad_fn=<SubBackward0>)
[2022-11-06 12:04:14.928093] Process 5. Episode 38400, average_reward -0.070078
Episode 38400: Total Loss of tensor([[5.2041]], grad_fn=<SubBackward0>)
[2022-11-06 12:04:44.812531] Process 0. Episode 37800, average_reward -0.069762
Episode 37800: Total Loss of tensor([[1.1143]], grad_fn=<SubBackward0>)
[2022-11-06 12:06:12.188465] Process 3. Episode 38050, average_reward -0.071643
Episode 38050: Total Loss of tensor([[8.1564]], grad_fn=<SubBackward0>)
[2022-11-06 12:06:24.307746] Process 1. Episode 37200, average_reward -0.068925
Episode 37200: Total Loss of tensor([[1.7464]], grad_fn=<SubBackward0>)
[2022-11-06 12:06:26.373089] Process 2. Episode 38750, average_reward -0.073703
Episode 38750: Total Loss of tensor([[5.9247]], grad_fn=<SubBackward0>)
[2022-11-06 12:06:35.254496] Process 5. Episode 38450, average_reward -0.070039
Episode 38450: Total Loss of tensor([[12.2360]], grad_fn=<SubBackward0>)
[2022-11-06 12:07:02.985372] Process 0. Episode 37850, average_reward -0.069828
Episode 37850: Total Loss of tensor([[4.6138]], grad_fn=<SubBackward0>)
[2022-11-06 12:08:28.690094] Process 3. Episode 38100, average_reward -0.071575
Episode 38100: Total Loss of tensor([[-39.8899]], grad_fn=<SubBackward0>)
[2022-11-06 12:08:37.838054] Process 2. Episode 38800, average_reward -0.073737
Episode 38800: Total Loss of tensor([[5.2985]], grad_fn=<SubBackward0>)
[2022-11-06 12:08:51.895931] Process 1. Episode 37250, average_reward -0.068913
Episode 37250: Total Loss of tensor([[-19.7225]], grad_fn=<SubBackward0>)
[2022-11-06 12:08:55.039121] Process 5. Episode 38500, average_reward -0.070104
Episode 38500: Total Loss of tensor([[-7.5544]], grad_fn=<SubBackward0>)
[2022-11-06 12:09:17.144530] Process 0. Episode 37900, average_reward -0.069842
Episode 37900: Total Loss of tensor([[6.7379]], grad_fn=<SubBackward0>)
[2022-11-06 12:10:45.188725] Process 2. Episode 38850, average_reward -0.073719
Episode 38850: Total Loss of tensor([[13.3951]], grad_fn=<SubBackward0>)
[2022-11-06 12:10:51.704901] Process 3. Episode 38150, average_reward -0.071638
Episode 38150: Total Loss of tensor([[95.6614]], grad_fn=<SubBackward0>)
[2022-11-06 12:11:05.929426] Process 5. Episode 38550, average_reward -0.070117
Episode 38550: Total Loss of tensor([[25.5158]], grad_fn=<SubBackward0>)
[2022-11-06 12:11:17.160618] Process 1. Episode 37300, average_reward -0.068874
Episode 37300: Total Loss of tensor([[21.8581]], grad_fn=<SubBackward0>)
[2022-11-06 12:11:30.487994] Process 0. Episode 37950, average_reward -0.069881
Episode 37950: Total Loss of tensor([[11.8448]], grad_fn=<SubBackward0>)
[2022-11-06 12:12:56.339260] Process 2. Episode 38900, average_reward -0.073702
Episode 38900: Total Loss of tensor([[12.8155]], grad_fn=<SubBackward0>)
[2022-11-06 12:13:13.704240] Process 3. Episode 38200, average_reward -0.071728
Episode 38200: Total Loss of tensor([[12.5373]], grad_fn=<SubBackward0>)
[2022-11-06 12:13:15.692993] Process 5. Episode 38600, average_reward -0.070078
Episode 38600: Total Loss of tensor([[10.1287]], grad_fn=<SubBackward0>)
[2022-11-06 12:13:40.554843] Process 1. Episode 37350, average_reward -0.068782
Episode 37350: Total Loss of tensor([[11.1874]], grad_fn=<SubBackward0>)
[2022-11-06 12:13:43.840713] Process 0. Episode 38000, average_reward -0.069842
Episode 38000: Total Loss of tensor([[12.1938]], grad_fn=<SubBackward0>)
[2022-11-06 12:15:09.144101] Process 2. Episode 38950, average_reward -0.073813
Episode 38950: Total Loss of tensor([[-6.0047]], grad_fn=<SubBackward0>)
[2022-11-06 12:15:32.321557] Process 3. Episode 38250, average_reward -0.071765
Episode 38250: Total Loss of tensor([[15.5396]], grad_fn=<SubBackward0>)
[2022-11-06 12:15:40.268439] Process 5. Episode 38650, average_reward -0.070142
Episode 38650: Total Loss of tensor([[-9.1258]], grad_fn=<SubBackward0>)
[2022-11-06 12:15:57.795736] Process 0. Episode 38050, average_reward -0.069777
Episode 38050: Total Loss of tensor([[10.0521]], grad_fn=<SubBackward0>)
[2022-11-06 12:16:02.773021] Process 1. Episode 37400, average_reward -0.068850
Episode 37400: Total Loss of tensor([[13.2901]], grad_fn=<SubBackward0>)
[2022-11-06 12:17:23.338811] Process 2. Episode 39000, average_reward -0.073821
Episode 39000: Total Loss of tensor([[6.4322]], grad_fn=<SubBackward0>)
[2022-11-06 12:17:43.002763] Process 3. Episode 38300, average_reward -0.071775
Episode 38300: Total Loss of tensor([[5.1062]], grad_fn=<SubBackward0>)
[2022-11-06 12:17:53.115602] Process 5. Episode 38700, average_reward -0.070103
Episode 38700: Total Loss of tensor([[13.8326]], grad_fn=<SubBackward0>)
[2022-11-06 12:18:24.497980] Process 0. Episode 38100, average_reward -0.069790
Episode 38100: Total Loss of tensor([[-45.5378]], grad_fn=<SubBackward0>)
[2022-11-06 12:18:29.807096] Process 1. Episode 37450, average_reward -0.068838
Episode 37450: Total Loss of tensor([[16.6960]], grad_fn=<SubBackward0>)
[2022-11-06 12:19:39.567541] Process 2. Episode 39050, average_reward -0.073828
Episode 39050: Total Loss of tensor([[3.7474]], grad_fn=<SubBackward0>)
[2022-11-06 12:19:52.771512] Process 3. Episode 38350, average_reward -0.071682
Episode 38350: Total Loss of tensor([[9.7710]], grad_fn=<SubBackward0>)
[2022-11-06 12:20:07.517528] Process 5. Episode 38750, average_reward -0.070116
Episode 38750: Total Loss of tensor([[-3.8051]], grad_fn=<SubBackward0>)
Training process 2 terminated
[2022-11-06 12:20:41.615684] Process 0. Episode 38150, average_reward -0.069777
Episode 38150: Total Loss of tensor([[17.6219]], grad_fn=<SubBackward0>)
[2022-11-06 12:20:48.348709] Process 1. Episode 37500, average_reward -0.068827
Episode 37500: Total Loss of tensor([[-121.6195]], grad_fn=<SubBackward0>)
[2022-11-06 12:21:58.231396] Process 3. Episode 38400, average_reward -0.071745
Episode 38400: Total Loss of tensor([[3.6263]], grad_fn=<SubBackward0>)
[2022-11-06 12:22:15.093695] Process 5. Episode 38800, average_reward -0.070026
Episode 38800: Total Loss of tensor([[-1.5078]], grad_fn=<SubBackward0>)
[2022-11-06 12:22:51.499977] Process 0. Episode 38200, average_reward -0.069791
Episode 38200: Total Loss of tensor([[6.7355]], grad_fn=<SubBackward0>)
[2022-11-06 12:22:52.153540] Process 1. Episode 37550, average_reward -0.068868
Episode 37550: Total Loss of tensor([[1.1136]], grad_fn=<SubBackward0>)
[2022-11-06 12:24:02.538331] Process 3. Episode 38450, average_reward -0.071730
Episode 38450: Total Loss of tensor([[4.5120]], grad_fn=<SubBackward0>)
[2022-11-06 12:24:20.212771] Process 5. Episode 38850, average_reward -0.070064
Episode 38850: Total Loss of tensor([[5.0102]], grad_fn=<SubBackward0>)
[2022-11-06 12:24:55.134450] Process 0. Episode 38250, average_reward -0.069778
Episode 38250: Total Loss of tensor([[6.3912]], grad_fn=<SubBackward0>)
[2022-11-06 12:25:02.491739] Process 1. Episode 37600, average_reward -0.068883
Episode 37600: Total Loss of tensor([[-46.2498]], grad_fn=<SubBackward0>)
[2022-11-06 12:26:05.226023] Process 3. Episode 38500, average_reward -0.071714
Episode 38500: Total Loss of tensor([[17.1780]], grad_fn=<SubBackward0>)
[2022-11-06 12:26:22.962662] Process 5. Episode 38900, average_reward -0.070077
Episode 38900: Total Loss of tensor([[-110.1931]], grad_fn=<SubBackward0>)
[2022-11-06 12:27:00.088800] Process 0. Episode 38300, average_reward -0.069765
Episode 38300: Total Loss of tensor([[13.2174]], grad_fn=<SubBackward0>)
[2022-11-06 12:27:13.656778] Process 1. Episode 37650, average_reward -0.068898
Episode 37650: Total Loss of tensor([[-65.6870]], grad_fn=<SubBackward0>)
[2022-11-06 12:28:08.516228] Process 3. Episode 38550, average_reward -0.071673
Episode 38550: Total Loss of tensor([[9.4084]], grad_fn=<SubBackward0>)
[2022-11-06 12:28:26.346881] Process 5. Episode 38950, average_reward -0.070064
Episode 38950: Total Loss of tensor([[13.7163]], grad_fn=<SubBackward0>)
[2022-11-06 12:29:04.259092] Process 0. Episode 38350, average_reward -0.069700
Episode 38350: Total Loss of tensor([[6.7417]], grad_fn=<SubBackward0>)
[2022-11-06 12:29:21.113032] Process 1. Episode 37700, average_reward -0.068939
Episode 37700: Total Loss of tensor([[6.2988]], grad_fn=<SubBackward0>)
[2022-11-06 12:30:11.110585] Process 3. Episode 38600, average_reward -0.071632
Episode 38600: Total Loss of tensor([[-1.5939]], grad_fn=<SubBackward0>)
[2022-11-06 12:30:25.925635] Process 5. Episode 39000, average_reward -0.069974
Episode 39000: Total Loss of tensor([[4.0881]], grad_fn=<SubBackward0>)
[2022-11-06 12:31:09.700133] Process 0. Episode 38400, average_reward -0.069740
Episode 38400: Total Loss of tensor([[4.7122]], grad_fn=<SubBackward0>)
[2022-11-06 12:31:31.784976] Process 1. Episode 37750, average_reward -0.068927
Episode 37750: Total Loss of tensor([[5.0417]], grad_fn=<SubBackward0>)
[2022-11-06 12:32:17.054878] Process 3. Episode 38650, average_reward -0.071669
Episode 38650: Total Loss of tensor([[12.4851]], grad_fn=<SubBackward0>)
[2022-11-06 12:32:28.389553] Process 5. Episode 39050, average_reward -0.069936
Episode 39050: Total Loss of tensor([[10.1646]], grad_fn=<SubBackward0>)
Training process 5 terminated
[2022-11-06 12:33:15.971935] Process 0. Episode 38450, average_reward -0.069753
Episode 38450: Total Loss of tensor([[0.1459]], grad_fn=<SubBackward0>)
[2022-11-06 12:33:34.874715] Process 1. Episode 37800, average_reward -0.068995
Episode 37800: Total Loss of tensor([[7.5958]], grad_fn=<SubBackward0>)
[2022-11-06 12:34:07.785311] Process 3. Episode 38700, average_reward -0.071654
Episode 38700: Total Loss of tensor([[18.3255]], grad_fn=<SubBackward0>)
[2022-11-06 12:35:09.713066] Process 0. Episode 38500, average_reward -0.069740
Episode 38500: Total Loss of tensor([[4.9381]], grad_fn=<SubBackward0>)
[2022-11-06 12:35:29.722522] Process 1. Episode 37850, average_reward -0.068983
Episode 37850: Total Loss of tensor([[11.9748]], grad_fn=<SubBackward0>)
[2022-11-06 12:35:55.758968] Process 3. Episode 38750, average_reward -0.071665
Episode 38750: Total Loss of tensor([[-6.2144]], grad_fn=<SubBackward0>)
[2022-11-06 12:36:59.619832] Process 0. Episode 38550, average_reward -0.069780
Episode 38550: Total Loss of tensor([[3.2376]], grad_fn=<SubBackward0>)
[2022-11-06 12:37:24.536127] Process 1. Episode 37900, average_reward -0.069024
Episode 37900: Total Loss of tensor([[4.3649]], grad_fn=<SubBackward0>)
[2022-11-06 12:37:44.296610] Process 3. Episode 38800, average_reward -0.071598
Episode 38800: Total Loss of tensor([[3.0532]], grad_fn=<SubBackward0>)
[2022-11-06 12:38:49.903096] Process 0. Episode 38600, average_reward -0.069689
Episode 38600: Total Loss of tensor([[1.1630]], grad_fn=<SubBackward0>)
[2022-11-06 12:39:21.492079] Process 1. Episode 37950, average_reward -0.068933
Episode 37950: Total Loss of tensor([[-3.2148]], grad_fn=<SubBackward0>)
[2022-11-06 12:39:34.172166] Process 3. Episode 38850, average_reward -0.071532
Episode 38850: Total Loss of tensor([[-4.8633]], grad_fn=<SubBackward0>)
[2022-11-06 12:40:40.960705] Process 0. Episode 38650, average_reward -0.069677
Episode 38650: Total Loss of tensor([[1.0698]], grad_fn=<SubBackward0>)
[2022-11-06 12:41:14.557047] Process 1. Episode 38000, average_reward -0.069026
Episode 38000: Total Loss of tensor([[9.6032]], grad_fn=<SubBackward0>)
[2022-11-06 12:41:24.895313] Process 3. Episode 38900, average_reward -0.071491
Episode 38900: Total Loss of tensor([[14.0995]], grad_fn=<SubBackward0>)
[2022-11-06 12:42:34.065122] Process 0. Episode 38700, average_reward -0.069664
Episode 38700: Total Loss of tensor([[7.9626]], grad_fn=<SubBackward0>)
[2022-11-06 12:43:08.324376] Process 1. Episode 38050, average_reward -0.069067
Episode 38050: Total Loss of tensor([[-106.8351]], grad_fn=<SubBackward0>)
[2022-11-06 12:43:14.071546] Process 3. Episode 38950, average_reward -0.071502
Episode 38950: Total Loss of tensor([[4.8100]], grad_fn=<SubBackward0>)
[2022-11-06 12:44:27.087175] Process 0. Episode 38750, average_reward -0.069652
Episode 38750: Total Loss of tensor([[7.6921]], grad_fn=<SubBackward0>)
[2022-11-06 12:45:03.563171] Process 1. Episode 38100, average_reward -0.069055
Episode 38100: Total Loss of tensor([[8.1739]], grad_fn=<SubBackward0>)
[2022-11-06 12:45:05.462838] Process 3. Episode 39000, average_reward -0.071538
Episode 39000: Total Loss of tensor([[14.8436]], grad_fn=<SubBackward0>)
[2022-11-06 12:46:20.542134] Process 0. Episode 38800, average_reward -0.069665
Episode 38800: Total Loss of tensor([[5.9700]], grad_fn=<SubBackward0>)
[2022-11-06 12:46:56.527619] Process 3. Episode 39050, average_reward -0.071575
Episode 39050: Total Loss of tensor([[16.9640]], grad_fn=<SubBackward0>)
[2022-11-06 12:46:56.945265] Process 1. Episode 38150, average_reward -0.069069
Episode 38150: Total Loss of tensor([[21.6597]], grad_fn=<SubBackward0>)
Training process 3 terminated
[2022-11-06 12:48:06.376204] Process 0. Episode 38850, average_reward -0.069627
Episode 38850: Total Loss of tensor([[12.8713]], grad_fn=<SubBackward0>)
[2022-11-06 12:48:37.573004] Process 1. Episode 38200, average_reward -0.069084
Episode 38200: Total Loss of tensor([[21.1327]], grad_fn=<SubBackward0>)
[2022-11-06 12:49:44.935137] Process 0. Episode 38900, average_reward -0.069614
Episode 38900: Total Loss of tensor([[3.1239]], grad_fn=<SubBackward0>)
[2022-11-06 12:50:16.200965] Process 1. Episode 38250, average_reward -0.069020
Episode 38250: Total Loss of tensor([[12.7294]], grad_fn=<SubBackward0>)
[2022-11-06 12:51:23.630369] Process 0. Episode 38950, average_reward -0.069628
Episode 38950: Total Loss of tensor([[-117.2470]], grad_fn=<SubBackward0>)
[2022-11-06 12:51:54.825172] Process 1. Episode 38300, average_reward -0.068982
Episode 38300: Total Loss of tensor([[-2.3790]], grad_fn=<SubBackward0>)
[2022-11-06 12:53:01.515237] Process 0. Episode 39000, average_reward -0.069667
Episode 39000: Total Loss of tensor([[-27.9129]], grad_fn=<SubBackward0>)
[2022-11-06 12:53:33.466244] Process 1. Episode 38350, average_reward -0.068970
Episode 38350: Total Loss of tensor([[10.4063]], grad_fn=<SubBackward0>)
[2022-11-06 12:54:39.346109] Process 0. Episode 39050, average_reward -0.069603
Episode 39050: Total Loss of tensor([[17.3034]], grad_fn=<SubBackward0>)
Training process 0 terminated
The code runs for 118124.82 s 
[2022-11-06 12:55:08.344632] Process 1. Episode 38400, average_reward -0.068958
Episode 38400: Total Loss of tensor([[-24.6897]], grad_fn=<SubBackward0>)
[2022-11-06 12:56:32.463297] Process 1. Episode 38450, average_reward -0.068947
Episode 38450: Total Loss of tensor([[14.6050]], grad_fn=<SubBackward0>)
[2022-11-06 12:57:55.966062] Process 1. Episode 38500, average_reward -0.068883
Episode 38500: Total Loss of tensor([[5.9610]], grad_fn=<SubBackward0>)
[2022-11-06 12:59:21.138628] Process 1. Episode 38550, average_reward -0.068872
Episode 38550: Total Loss of tensor([[2.2089]], grad_fn=<SubBackward0>)
[2022-11-06 13:00:45.428180] Process 1. Episode 38600, average_reward -0.068938
Episode 38600: Total Loss of tensor([[11.9073]], grad_fn=<SubBackward0>)
[2022-11-06 13:02:09.693179] Process 1. Episode 38650, average_reward -0.068900
Episode 38650: Total Loss of tensor([[13.9618]], grad_fn=<SubBackward0>)
[2022-11-06 13:03:34.918134] Process 1. Episode 38700, average_reward -0.068837
Episode 38700: Total Loss of tensor([[5.1679]], grad_fn=<SubBackward0>)
[2022-11-06 13:04:59.244935] Process 1. Episode 38750, average_reward -0.068903
Episode 38750: Total Loss of tensor([[-3.6111]], grad_fn=<SubBackward0>)
[2022-11-06 13:06:22.799622] Process 1. Episode 38800, average_reward -0.068969
Episode 38800: Total Loss of tensor([[13.5208]], grad_fn=<SubBackward0>)
[2022-11-06 13:07:46.817090] Process 1. Episode 38850, average_reward -0.068958
Episode 38850: Total Loss of tensor([[20.1278]], grad_fn=<SubBackward0>)
[2022-11-06 13:09:10.033939] Process 1. Episode 38900, average_reward -0.068946
Episode 38900: Total Loss of tensor([[1.1922]], grad_fn=<SubBackward0>)
[2022-11-06 13:10:33.963153] Process 1. Episode 38950, average_reward -0.068909
Episode 38950: Total Loss of tensor([[14.2453]], grad_fn=<SubBackward0>)
[2022-11-06 13:11:56.511750] Process 1. Episode 39000, average_reward -0.068872
Episode 39000: Total Loss of tensor([[7.2206]], grad_fn=<SubBackward0>)
[2022-11-06 13:13:21.623207] Process 1. Episode 39050, average_reward -0.068886
Episode 39050: Total Loss of tensor([[0.6873]], grad_fn=<SubBackward0>)
Training process 1 terminated
